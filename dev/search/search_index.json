{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Viralgenie","text":""},{"location":"#_2","title":"Viralgenie","text":"<p>A metagenomic analysis pipeline for eukaryotic viruses written in nextflow.</p> <p> </p> <p> </p>"},{"location":"#introduction","title":"Introduction","text":"<p>Viralgenie is a bioinformatics best-practice analysis pipeline for reconstructing consensus genomes and identifying intra-host variants from metagenomic sequencing data or enriched based sequencing data like hybrid capture.</p>"},{"location":"#pipeline-summary","title":"Pipeline summary","text":"<ol> <li>Read QC (<code>FastQC</code>)</li> <li>Performs optional read pre-processing<ul> <li>Adapter trimming(<code>fastp</code>, <code>Trimmomatic</code>)</li> <li>Read UMI deduplication (<code>HUMID</code>)</li> <li>Low complexity and quality filtering (<code>bbduk</code>, <code>prinseq++</code>)</li> <li>Host-read removal (<code>BowTie2</code>)</li> </ul> </li> <li>Metagenomic diversity mapping<ul> <li>Performs taxonomic classification and/or profiling using one or more of:<ul> <li><code>Kraken2</code></li> <li><code>Bracken</code>(optional)</li> <li><code>Kaiju</code></li> </ul> </li> <li>Plotting Kraken2 and Kaiju (<code>Krona</code>)</li> </ul> </li> <li>Denovo assembly (<code>SPAdes</code>, <code>TRINITY</code>, <code>megahit</code>), combine contigs.</li> <li>[Optional] extend the contigs with sspace_basic and filter with <code>prinseq++</code></li> <li>[Optional] Map reads to contigs for coverage estimation (<code>BowTie2</code>,<code>BWAmem2</code> and <code>BWA</code>)</li> <li>Contig reference idententification (<code>blastn</code>)<ul> <li>Identify top 5 blast hits</li> <li>Merge blast hit and all contigs of a sample</li> </ul> </li> <li>[Optional] Precluster contigs based on taxonomy<ul> <li>Identify taxonomy <code>Kraken2</code> and\\or <code>Kaiju</code></li> <li>Resolve potential inconsistencies in taxonomy &amp; taxon filtering | simplification <code>bin/extract_precluster.py</code></li> </ul> </li> <li>Cluster contigs (or every taxonomic bin) of samples, options are:<ul> <li><code>cdhitest</code></li> <li><code>vsearch</code></li> <li><code>mmseqs-linclust</code></li> <li><code>mmseqs-cluster</code></li> <li><code>vRhyme</code></li> <li><code>Mash</code></li> </ul> </li> <li>[Optional] Remove clusters with low read coverage. <code>bin/extract_clusters.py</code></li> <li>Scaffolding of contigs to centroid (<code>Minimap2</code>, <code>iVar-consensus</code>)</li> <li>[Optional] Annotate 0-depth regions with external reference <code>bin/lowcov_to_reference.py</code>.</li> <li>[Optional] Select best reference from <code>--mapping_constraints</code>:<ul> <li><code>Mash sketch</code></li> <li><code>Mash screen</code></li> </ul> </li> <li>Mapping filtered reads to supercontig and mapping constraints(<code>BowTie2</code>,<code>BWAmem2</code> and <code>BWA</code>)</li> <li>[Optional] Deduplicate reads (<code>Picard</code> or if UMI's are used <code>UMI-tools</code>)</li> <li>Variant calling and filtering (<code>BCFTools</code>,<code>iVar</code>)</li> <li>Create consensus genome (<code>BCFTools</code>,<code>iVar</code>)</li> <li>Repeat step 12-15 multiple times for the denovo contig route</li> <li>Consensus evaluation and annotation (<code>QUAST</code>,<code>CheckV</code>,<code>blastn</code>, <code>mmseqs-search</code>, <code>MAFFT</code> - alignment of contigs vs iterations &amp; consensus)</li> <li>Result summary visualisation for raw read, alignment, assembly, variant calling and consensus calling results (<code>MultiQC</code>)</li> </ol>"},{"location":"#usage","title":"Usage","text":"<p>[!NOTE] If you are new to Nextflow and nf-core, please refer to this page on how to set-up Nextflow. Make sure to test your setup with <code>-profile test</code> before running the workflow on actual data.</p> <p>First, prepare a samplesheet with your input data that looks as follows:</p> <p><code>samplesheet.csv</code>:</p> <pre><code>sample,fastq_1,fastq_2\nsample1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\nsample2,AEG588A5_S5_L003_R1_001.fastq.gz,\nsample3,AEG588A3_S3_L002_R1_001.fastq.gz,AEG588A3_S3_L002_R2_001.fastq.gz\n</code></pre> <p>Each row represents a fastq file (single-end) or a pair of fastq files (paired end).</p> <p>Now, you can run the pipeline using:</p> <pre><code>nextflow run Joon-Klaps/viralgenie \\\n   -profile &lt;docker/singularity/.../institute&gt; \\\n   --input samplesheet.csv \\\n   --outdir &lt;OUTDIR&gt;\n</code></pre> <p>[!WARNING] Please provide pipeline parameters via the CLI or Nextflow <code>-params-file</code> option. Custom config files including those provided by the <code>-c</code> Nextflow option can be used to provide any configuration except for parameters; see docs.</p> <p>For more details and further functionality, please refer to the usage documentation and the parameter documentation.</p>"},{"location":"#pipeline-output","title":"Pipeline output","text":"<p>To see the results of an example test run with a full size dataset refer to the results tab on the nf-core website pipeline page. For more details about the output files and reports, please refer to the output documentation.</p>"},{"location":"#credits","title":"Credits","text":"<p>Viralgenie was originally written by <code>Joon-Klaps</code>.</p> <p>We thank the following people for their extensive assistance in the development of this pipeline:</p> <ul> <li><code>Philippe Lemey</code></li> <li><code>Liana Kafetzopoulou</code></li> <li><code>nf-core community</code></li> </ul>"},{"location":"#contributions-and-support","title":"Contributions and Support","text":"<p>If you would like to contribute to this pipeline, please see the contributing guidelines.</p>"},{"location":"#citations","title":"Citations","text":"<p>[!WARNING] Viralgenie is currently not Published. Please cite as: Klaps J, Lemey P, Kafetzopoulou L. Viralgenie: A metagenomics analysis pipeline for eukaryotic viruses. Github https://github.com/Joon-Klaps/viralgenie</p> <p>An extensive list of references for the tools used by the pipeline can be found in the <code>CITATIONS.md</code> file.</p>"},{"location":"CITATIONS/","title":"Citations","text":""},{"location":"CITATIONS/#viralgenie","title":"Viralgenie","text":"<p>Warning</p> <p>Viralgenie is currently not Published. Please cite as:</p> <ul> <li>Klaps J, Lemey P, Kafetzopoulou L. Viralgenie: A metagenomics analysis pipeline for eukaryotic viruses. Github https://github.com/Joon-Klaps/viralgenie</li> </ul>"},{"location":"CITATIONS/#nf-core","title":"nf-core","text":"<p>Ewels PA, Peltzer A, Fillinger S, Patel H, Alneberg J, Wilm A, Garcia MU, Di Tommaso P, Nahnsen S. The nf-core framework for community-curated bioinformatics pipelines. Nat Biotechnol. 2020 Mar;38(3):276-278. doi: 10.1038/s41587-020-0439-x. PubMed PMID: 32055031.</p>"},{"location":"CITATIONS/#nextflow","title":"Nextflow","text":"<p>Di Tommaso P, Chatzou M, Floden EW, Barja PP, Palumbo E, Notredame C. Nextflow enables reproducible computational workflows. Nat Biotechnol. 2017 Apr 11;35(4):316-319. doi: 10.1038/nbt.3820. PubMed PMID: 28398311.</p>"},{"location":"CITATIONS/#pipeline-tools","title":"Pipeline tools","text":"<ul> <li> <p>Bbduk</p> <p>Bushnell B. (2022) BBMap, URL: http://sourceforge.net/projects/bbmap/</p> </li> <li> <p>BCFtools</p> <p>Danecek, Petr et al. \u201cTwelve years of SAMtools and BCFtools.\u201d GigaScience vol. 10,2 (2021): giab008. doi:10.1093/gigascience/giab008</p> </li> <li> <p>blast</p> <p>Camacho, Christiam et al. \u201cBLAST+: architecture and applications.\u201d BMC bioinformatics vol. 10 421. 15 Dec. 2009, doi:10.1186/1471-2105-10-421</p> </li> <li> <p>Bowtie2</p> <p>Langmead, Ben, and Steven L Salzberg. \u201cFast gapped-read alignment with Bowtie 2.\u201d Nature methods vol. 9,4 357-9. 4 Mar. 2012, doi:10.1038/nmeth.1923</p> </li> <li> <p>BWA-MEM</p> <p>Li H. (2013) Aligning sequence reads, clone sequences and assembly contigs with BWA-MEM. arXiv:1303.3997v2.</p> </li> <li> <p>BWA-MEM2</p> <p>M. Vasimuddin, S. Misra, H. Li and S. Aluru, \"Efficient Architecture-Aware Acceleration of BWA-MEM for Multicore Systems,\" 2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS), Rio de Janeiro, Brazil, 2019, pp. 314-324, doi: 10.1109/IPDPS.2019.00041.</p> </li> <li> <p>cdhit</p> <p>Fu, Limin et al. \u201cCD-HIT: accelerated for clustering the next-generation sequencing data.\u201d Bioinformatics (Oxford, England) vol. 28,23 (2012): 3150-2. doi:10.1093/bioinformatics/bts565</p> </li> <li> <p>checkv</p> <p>Nayfach, Stephen et al. \u201cCheckV assesses the quality and completeness of metagenome-assembled viral genomes.\u201d Nature biotechnology vol. 39,5 (2021): 578-585. doi:10.1038/s41587-020-00774-7</p> </li> <li> <p>FastQC</p> <p>Andrews, S. (2010). FastQC: A Quality Control Tool for High Throughput Sequence Data [Online].</p> </li> <li> <p>fastp</p> <p>Chen, Shifu et al. \u201cfastp: an ultra-fast all-in-one FASTQ preprocessor.\u201d Bioinformatics (Oxford, England) vol. 34,17 (2018): i884-i890. doi:10.1093/bioinformatics/bty560</p> </li> <li> <p>HUMID</p> <p>Laros J, van den Berg R, Github https://github.com/jfjlaros/HUMID</p> </li> <li> <p>iVar</p> <p>Grubaugh, Nathan D et al. \u201cAn amplicon-based sequencing framework for accurately measuring intrahost virus diversity using PrimalSeq and iVar.\u201d Genome biology vol. 20,1 8. 8 Jan. 2019, doi:10.1186/s13059-018-1618-7</p> </li> <li> <p>Kaiju</p> <p>Menzel, Peter et al. \u201cFast and sensitive taxonomic classification for metagenomics with Kaiju.\u201d Nature communications vol. 7 11257. 13 Apr. 2016, doi:10.1038/ncomms11257</p> </li> <li> <p>Kraken2</p> <p>Wood, Derrick E., Jennifer Lu, and Ben Langmead. 2019. Improved Metagenomic Analysis with Kraken 2. Genome Biology 20 (1): 257. doi: 10.1186/s13059-019-1891-0.</p> </li> <li> <p>leiden-algorithm</p> <p>Traag, V A et al. \u201cFrom Louvain to Leiden: guaranteeing well-connected communities.\u201d Scientific reports vol. 9,1 5233. 26 Mar. 2019, doi:10.1038/s41598-019-41695-z</p> </li> <li> <p>Mash</p> <p>Ondov, Brian D et al. \u201cMash: fast genome and metagenome distance estimation using MinHash.\u201d Genome biology vol. 17,1 132. 20 Jun. 2016, doi:10.1186/s13059-016-0997-x</p> </li> <li> <p>Megahit</p> <p>Li, Dinghua et al. \u201cMEGAHIT v1.0: A fast and scalable metagenome assembler driven by advanced methodologies and community practices.\u201d Methods (San Diego, Calif.) vol. 102 (2016): 3-11. doi:10.1016/j.ymeth.2016.02.020</p> </li> <li> <p>Minimap2</p> <p>Li, Heng. \u201cMinimap2: pairwise alignment for nucleotide sequences.\u201d Bioinformatics (Oxford, England) vol. 34,18 (2018): 3094-3100. doi:10.1093/bioinformatics/bty191</p> </li> <li> <p>MMseqs2</p> <p>Steinegger, Martin, and Johannes S\u00f6ding. \u201cMMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets.\u201d Nature biotechnology vol. 35,11 (2017): 1026-1028. doi:10.1038/nbt.3988</p> </li> <li> <p>Mosdepth</p> <p>Pedersen, Brent S, and Aaron R Quinlan. \u201cMosdepth: quick coverage calculation for genomes and exomes.\u201d Bioinformatics (Oxford, England) vol. 34,5 (2018): 867-868. doi:10.1093/bioinformatics/btx699</p> </li> <li> <p>MultiQC</p> <p>Ewels, Philip et al. \u201cMultiQC: summarize analysis results for multiple tools and samples in a single report.\u201d Bioinformatics (Oxford, England) vol. 32,19 (2016): 3047-8. doi:10.1093/bioinformatics/btw354</p> </li> <li> <p>picard-tools</p> </li> <li> <p>prokka</p> <p>Seemann, Torsten. \u201cProkka: rapid prokaryotic genome annotation.\u201d Bioinformatics (Oxford, England) vol. 30,14 (2014): 2068-9. doi:10.1093/bioinformatics/btu153</p> </li> <li> <p>QUAST</p> <p>Gurevich, Alexey et al. \u201cQUAST: quality assessment tool for genome assemblies.\u201d Bioinformatics (Oxford, England) vol. 29,8 (2013): 1072-5. doi:10.1093/bioinformatics/btt086</p> </li> <li> <p>SAMtools</p> <p>Li H. A statistical framework for SNP calling, mutation discovery, association mapping and population genetical parameter estimation from sequencing data. Bioinformatics. 2011 Nov 1;27(21):2987-93. doi: 10.1093/bioinformatics/btr509. Epub 2011 Sep 8. PMID: 21903627; PMCID: PMC3198575.</p> </li> <li> <p>SPAdes</p> <p>Bankevich, Anton et al. \u201cSPAdes: a new genome assembly algorithm and its applications to single-cell sequencing.\u201d Journal of computational biology : a journal of computational molecular cell biology vol. 19,5 (2012): 455-77. doi:10.1089/cmb.2012.0021</p> </li> <li> <p>SSPACE Basic</p> <p>Boetzer, Marten et al. \u201cScaffolding pre-assembled contigs using SSPACE.\u201d Bioinformatics (Oxford, England) vol. 27,4 (2011): 578-9. doi:10.1093/bioinformatics/btq683</p> </li> <li> <p>Trimmomatic</p> <p>Bolger, Anthony M et al. \u201cTrimmomatic: a flexible trimmer for Illumina sequence data.\u201d Bioinformatics (Oxford, England) vol. 30,15 (2014): 2114-20. doi:10.1093/bioinformatics/btu170</p> </li> <li> <p>Trinity</p> <p>Haas, Brian J et al. \u201cDe novo transcript sequence reconstruction from RNA-seq using the Trinity platform for reference generation and analysis.\u201d Nature protocols vol. 8,8 (2013): 1494-512. doi:10.1038/nprot.2013.084</p> </li> <li> <p>UMI-tools</p> <p>Smith, Tom et al. \u201cUMI-tools: modeling sequencing errors in Unique Molecular Identifiers to improve quantification accuracy.\u201d Genome research vol. 27,3 (2017): 491-499. doi:10.1101/gr.209601.116</p> </li> <li> <p>vRhyme</p> <p>Kieft, Kristopher et al. \u201cvRhyme enables binning of viral genomes from metagenomes.\u201d Nucleic acids research vol. 50,14 (2022): e83. doi:10.1093/nar/gkac341</p> </li> <li> <p>VSEARCH</p> <p>Rognes, Torbj\u00f8rn et al. \u201cVSEARCH: a versatile open source tool for metagenomics.\u201d PeerJ vol. 4 e2584. 18 Oct. 2016, doi:10.7717/peerj.2584</p> </li> </ul>"},{"location":"CITATIONS/#software-packagingcontainerisation-tools","title":"Software packaging/containerisation tools","text":"<ul> <li> <p>Anaconda</p> <p>Anaconda Software Distribution. Computer software. Vers. 2-2.4.0. Anaconda, Nov. 2016. Web.</p> </li> <li> <p>Bioconda</p> <p>Gr\u00fcning B, Dale R, Sj\u00f6din A, Chapman BA, Rowe J, Tomkins-Tinch CH, Valieris R, K\u00f6ster J; Bioconda Team. Bioconda: sustainable and comprehensive software distribution for the life sciences. Nat Methods. 2018 Jul;15(7):475-476. doi: 10.1038/s41592-018-0046-7. PubMed PMID: 29967506.</p> </li> <li> <p>BioContainers</p> <p>da Veiga Leprevost F, Gr\u00fcning B, Aflitos SA, R\u00f6st HL, Uszkoreit J, Barsnes H, Vaudel M, Moreno P, Gatto L, Weber J, Bai M, Jimenez RC, Sachsenberg T, Pfeuffer J, Alvarez RV, Griss J, Nesvizhskii AI, Perez-Riverol Y. BioContainers: an open-source and community-driven framework for software standardization. Bioinformatics. 2017 Aug 15;33(16):2580-2582. doi: 10.1093/bioinformatics/btx192. PubMed PMID: 28379341; PubMed Central PMCID: PMC5870671.</p> </li> <li> <p>Docker</p> <p>Merkel, D. (2014). Docker: lightweight linux containers for consistent development and deployment. Linux Journal, 2014(239), 2. doi: 10.5555/2600239.2600241.</p> </li> <li> <p>Singularity</p> <p>Kurtzer GM, Sochat V, Bauer MW. Singularity: Scientific containers for mobility of compute. PLoS One. 2017 May 11;12(5):e0177459. doi: 10.1371/journal.pone.0177459. eCollection 2017. PubMed PMID: 28494014; PubMed Central PMCID: PMC5426675.</p> </li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing Guidelines","text":"<p>Hi there!</p> <p>\ud83e\udd29 Many thanks for taking an interest in improving Viralgenie. \ud83e\udd29</p> <p>We try to manage the required tasks for Viralgenie using GitHub issues, you probably came to this page when creating one. Please use the pre-filled template to save time.</p> <p>However, don't be put off by this template - other more general issues and suggestions are welcome! Contributions to the code are even more welcome ;</p> <p>Info</p> <p>If you need help using or modifying viralgenie then the best place to ask is on the nf-core Slack Joon-Klaps.</p>"},{"location":"CONTRIBUTING/#contribution-workflow","title":"Contribution workflow","text":"<p>If you'd like to write some code for Joon-Klaps/viralgenie, the standard workflow is as follows:</p> <ol> <li>Check that there isn't already an issue about your idea in the Joon-Klaps/viralgenie issues to avoid duplicating work. If there isn't one already, please create one so that others know you're working on this</li> <li>Fork the Joon-Klaps/viralgenie repository to your GitHub account</li> <li>Make the necessary changes / additions within your forked repository following Pipeline conventions</li> <li>Use <code>nf-core pipelines schema build</code> and add any new parameters to the pipeline JSON schema (requires nf-core tools &gt;= 1.10).</li> <li>Submit a Pull Request against the <code>dev</code> branch and wait for the code to be reviewed and merged</li> </ol> <p>If you're not used to this workflow with git, you can start with some docs from GitHub or even their excellent <code>git</code> resources.</p>"},{"location":"CONTRIBUTING/#tests","title":"Tests","text":"<p>You can optionally test your changes by running the pipeline locally. Then it is recommended to use the <code>debug</code> profile to receive warnings about process selectors and other debug info. Example: <code>nextflow run . -profile debug,test,docker --outdir &lt;OUTDIR&gt;</code>.</p> <p>When you create a pull request with changes, GitHub Actions will run automatic tests. Typically, pull-requests are only fully reviewed when these tests are passing, though of course we can help out before then.</p> <p>There are typically two types of tests that run:</p>"},{"location":"CONTRIBUTING/#lint-tests","title":"Lint tests","text":"<p><code>nf-core</code> has a set of guidelines which viralgenie adheres to. To enforce these and ensure that viralgenie stays in sync, we have developed a helper tool which runs checks on the pipeline code. This is in the nf-core/tools repository and once installed can be run locally with the <code>nf-core lint &lt;pipeline-directory&gt;</code> command.</p> <p>If any failures or warnings are encountered, please follow the listed URL for more documentation.</p>"},{"location":"CONTRIBUTING/#pipeline-tests","title":"Pipeline tests","text":"<p>Viralgenie is set up with a minimal set of test-data. <code>GitHub Actions</code> then runs the pipeline on this data to ensure that it exits successfully. If there are any failures then the automated tests fail. These tests are run both with the latest available version of <code>Nextflow</code> and also the minimum required version that is stated in the pipeline code.</p>"},{"location":"CONTRIBUTING/#getting-help","title":"Getting help","text":"<p>For further information/help, please consult the Joon-Klaps/viralgenie documentation and don't hesitate to get in touch on Slack Joon-Klaps channel (join the nf-core Slack here).</p>"},{"location":"CONTRIBUTING/#pipeline-contribution-conventions","title":"Pipeline contribution conventions","text":"<p>To make the Joon-Klaps/viralgenie code and processing logic more understandable for new contributors and to ensure quality, we semi-standardise the way the code and other contributions are written.</p>"},{"location":"CONTRIBUTING/#adding-a-new-step","title":"Adding a new step","text":"<p>If you wish to contribute a new step, please use the following coding standards:</p> <ol> <li>Define the corresponding input channel into your new process from the expected previous process channel</li> <li>Write the process block (see below).</li> <li>Define the output channel if needed (see below).</li> <li>Add any new parameters to <code>nextflow.config</code> with a default (see below).</li> <li>Add any new parameters to <code>nextflow_schema.json</code> with help text (via the <code>nf-core pipelines schema build</code> tool).</li> <li>Add sanity checks and validation for all relevant parameters.</li> <li>Perform local tests to validate that the new code works as expected.</li> <li>If applicable, add a new test command in <code>.github/workflow/ci.yml</code>.</li> <li>Update MultiQC config <code>assets/multiqc_config.yml</code> so relevant suffixes, file name clean up and module plots are in the appropriate order. If applicable, add a MultiQC module.</li> <li>Add a description of the output files and if relevant any appropriate images from the MultiQC report to <code>docs/output.md</code>.</li> </ol>"},{"location":"CONTRIBUTING/#default-values","title":"Default values","text":"<p>Parameters should be initialised / defined with default values in <code>nextflow.config</code> under the <code>params</code> scope.</p> <p>Once there, use <code>nf-core pipelines schema build</code> to add to <code>nextflow_schema.json</code>.</p>"},{"location":"CONTRIBUTING/#default-processes-resource-requirements","title":"Default processes resource requirements","text":"<p>Sensible defaults for process resource requirements (CPUs / memory / time) for a process should be defined in <code>conf/base.config</code>. These should generally be specified generic with <code>withLabel:</code> selectors so they can be shared across multiple processes/steps of the pipeline. A nf-core standard set of labels that should be followed where possible can be seen in the nf-core pipeline template, which has the default process as a single core-process, and then different levels of multi-core configurations for increasingly large memory requirements defined with standardised labels.</p> <p>The process resources can be passed on to the tool dynamically within the process with the <code>${task.cpus}</code> and <code>${task.memory}</code> variables in the <code>script:</code> block.</p>"},{"location":"CONTRIBUTING/#naming-schemes","title":"Naming schemes","text":"<p>Please use the following naming schemes, to make it easy to understand what is going where.</p> <ul> <li>initial process channel: <code>ch_output_from_&lt;process&gt;</code></li> <li>intermediate and terminal channels: <code>ch_&lt;previousprocess&gt;_for_&lt;nextprocess&gt;</code></li> </ul>"},{"location":"CONTRIBUTING/#nextflow-version-bumping","title":"Nextflow version bumping","text":"<p>If you are using a new feature from core Nextflow, you may bump the minimum required version of nextflow in the pipeline with: <code>nf-core pipelines bump-version --nextflow . [min-nf-version]</code></p>"},{"location":"CONTRIBUTING/#images-and-figures","title":"Images and figures","text":"<p>For overview images and other documents we follow the nf-core style guidelines and examples.</p>"},{"location":"CONTRIBUTING/#github-codespaces","title":"GitHub Codespaces","text":"<p>This repo includes a devcontainer configuration which will create a GitHub Codespaces for Nextflow development! This is an online developer environment that runs in your browser, complete with VSCode and a terminal.</p> <p>To get started:</p> <ul> <li>Open the repo in Codespaces</li> <li>Tools installed<ul> <li>nf-core</li> <li>Nextflow</li> </ul> </li> </ul> <p>Devcontainer specs:</p> <ul> <li>DevContainer config</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>Viralgenie uses Nextflow, and a package/container management system (Docker, Singularity or Conda) so both need to be installed on the system where you launch your analysis.</p> <p>New to bioinformatics?</p> <p>If the word \"terminal\" brings to mind an airport boarding area, you can become a little lost. This blog post (up until Configuring an Xserver ...) will help people with little bioinformatic experience set up Nextflow and Docker on a Windows computer.</p>"},{"location":"installation/#software-managers-docker-singularity-and-conda","title":"Software managers: Docker, Singularity, and Conda","text":"<p>Viralgenie can be run using either Docker, Singularity or Conda. The choice of container system is up to the user, but it is important to note that Docker and Singularity are the most reproducible. Nextflow supports more containers in addition to Docker and Singularity, such as Podman, Shifter, and Charliecloud. You can read the full list of supported containers and how to set them up here.</p> <p>When using these containers, Nextflow will use the manager for each process that is executed. In other words, Nextflow will be using <code>docker run</code> or <code>singularity exec</code> without the need for you to do anything else.</p> DockerSingularityConda | Mamba <p>Docker is a containerization system that allows you to package your code, tools, and data into a single image that can be run on most operating systems. It is the most widely used containerization system in bioinformatics.</p> <p>To install Docker, follow the instructions on the Docker website.</p> <p>Warning</p> <p>Docker requires root access to run. If you do not have root access like, i.e. a user on an HPC or on a cloud - use Singularity instead.</p> <p>Singularity is a containerization system that allows you to package your code, tools, and data into a single image that can be run on most operating systems. It is the most widely used containerization system in bioinformatics.</p> <p>To install Singularity, follow the instructions on the Singularity website.</p> <p>Warning</p> <p>Singularity is a great alternative to Docker but can be challenging to set up on an Apple silicon chip or any other ARM device. If you are using an ARM device, consider using Docker instead.</p> <p>Conda is a package manager that allows you to install software packages and dependencies in isolated environments. It is a good choice if you are facing issues while installing Docker or Singularity.</p> <ul> <li>To install Conda, follow the instructions on the Conda website.</li> <li>To install Mamba, a faster alternative to Conda, follow the instructions on the Mamba miniforge website.</li> </ul> <p>Warning</p> <p>Conda environments are great! However, Conda tools can easily become broken or incompatible due to dependency issues. For this reason, Conda is not as reproducible as Docker or Singularity containers. If you encounter issues with Conda, please try running the pipeline with Docker or Singularity first to see if the issue persists. In other words, if you have a container system, use it over Conda!</p>"},{"location":"installation/#nextflow","title":"Nextflow","text":"<p>Nextflow runs on most POSIX systems (Linux, macOS, etc) and requires Java 11 or later. It can be installed in several ways, including using the Nextflow installer or Bioconda.</p> Nextflow installerBioconda <p>Tip</p> <p>Unsure how to install Nextflow with these commands? Check out the Nextflow installation documentation for more information.</p> <pre><code># Make sure that Java v11+ is installed:\njava -version\n\n# Install Nextflow\ncurl -fsSL get.nextflow.io | bash\n\n# Try a simple demo\n./nextflow run hello\n</code></pre> <p>Tip</p> <p>Add Nextflow binary to your user's <code>PATH</code>: <pre><code>mv nextflow ~/bin/\n</code></pre> Or to install it system-wide: <pre><code>sudo mv nextflow /usr/local/bin/\n</code></pre></p> <p>First, set up Bioconda according to the Bioconda documentation, notably setting up channels: <pre><code>conda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\n</code></pre></p> <p>A best practice with Conda is to create an environment and install the tools in it. Therefore you will prevent version conflicts and keep everything clean. To do so use the following command:</p> <pre><code>conda create --name nextflow-env nextflow\nconda activate nextflow-env\n</code></pre> <p>To deactivate the Conda environment, run the following command:</p> <pre><code>conda deactivate\n</code></pre> <p>If you're already in the Conda environment you want to use, you can just install Nextflow directly:</p> <pre><code>conda install nextflow\n</code></pre>"},{"location":"installation/#viralgenie","title":"Viralgenie","text":"<p>If you have both Nextflow and a software manager installed, you are all set! You can test the pipeline using the following command:</p> <pre><code>nextflow run Joon-Klaps/viralgenie \\\n    -profile test,&lt;docker/singularity/.../institute&gt;\n</code></pre> <p>Note</p> <p>With the argument <code>-profile &lt;docker/singularity/.../institute&gt;</code>, you can specify the container system you want to use. The <code>test</code> profile is used to run the pipeline with a small dataset to verify if everything is working correctly.</p> <p>Running Nextflow on a High performance computing (HPC) system?</p> <p>You might not be the first person to run a Nextflow pipeline on your infrastructure! Check out the nf-core configuration website as it might already contain a specific configuration for your infrastructure.</p> <p>Apple silicon (ARM)</p> <p>If you are using an Apple silicon (ARM) machine, you may encounter issues. Most tools are not yet compatible with ARM architecture, therefore Conda will most likely fail. In this case, use Docker in combination with the profile <code>arm</code>. <pre><code>nextflow run Joon-Klaps/viralgenie \\\n    -profile test,docker,arm\n</code></pre> If you still encounter issues, you can set up a Nextflow Tower account and run the pipeline with wave containers. In this config file, supply the following: <pre><code>wave {\n    enabled = true\n    wave.strategy = ['dockerfile']\n}\ntower {\n    accessToken = '&lt;your access token&gt;'\n}\n</code></pre></p>"},{"location":"output/","title":"Output","text":""},{"location":"output/#introduction","title":"Introduction","text":"<p>This document describes the output produced by the pipeline. Most of the plots are taken from the MultiQC report, which summarizes results at the end of the pipeline.</p> <p>The directories listed below will be created in the results directory after the pipeline has finished. All paths are relative to the top-level results directory.</p> <p>Tip</p> <p>A global, partly random prefix can be created using the argument <code>--prefix &lt;string&gt;</code>. The following string will then be used as a prefix to all output files: <pre><code>\"&lt;prefix_string&gt;_&lt;date&gt;_&lt;pipeline_version&gt;_&lt;workflow_runName&gt;\"\n</code></pre></p>"},{"location":"output/#preprocessing","title":"Preprocessing","text":"<p>All output files of the preprocessing steps can be found in the directory <code>preprocessing/</code>.</p>"},{"location":"output/#fastqc","title":"FastQC","text":"Output files <ul> <li><code>fastqc/{raw,trim,host}</code><ul> <li><code>*_fastqc.html</code>: FastQC report containing quality metrics.</li> <li><code>*_fastqc.zip</code>: Zip archive containing the FastQC report, tab-delimited data file, and plot images.</li> </ul> </li> </ul> <p>FastQC gives general quality metrics about your sequenced reads. It provides information about the quality score distribution across your reads, per base sequence content (%A/T/G/C), adapter contamination, and overrepresented sequences. For further reading and documentation see the FastQC help pages.</p> <p></p> <p></p> <p></p> <p>Tip</p> <p>The FastQC plots displayed in the MultiQC report show untrimmed, trimmed, and host filtered reads. Make sure to check the section titles for the correct set of reads.</p>"},{"location":"output/#fastp","title":"fastp","text":"<p>fastp is a FASTQ pre-processing tool for quality control, trimming of adapters, quality filtering, and other features.</p> Output files <ul> <li><code>fastp/</code><ul> <li><code>report/&lt;sample-id&gt;.*{html,json}</code>: report files in different formats.</li> <li><code>log/&lt;sample-id&gt;.*{html,json}</code>: log files.</li> <li><code>&lt;sample-id&gt;.fastp.fastq.gz</code>: file with the trimmed fastq reads.</li> <li><code>fail/&lt;sample-id&gt;.fail.fastq.gz</code>: file with reads that didn't suffice quality controls.</li> </ul> </li> </ul> <p>By default, viralgenie will only provide the report and log files if fastp is selected. The trimmed reads can be saved by specifying <code>--save_intermediate_reads</code> or <code>--save_final_reads 'trimming'</code>. Similarly, the saving of the output reads can be enabled with <code>--save_trimmed_fail</code>.</p>"},{"location":"output/#trimmomatic","title":"Trimmomatic","text":"<p>Trimmomatic is a FASTQ pre-processing tool for quality control, trimming of adapters, quality filtering, and other features.</p> Output files <ul> <li><code>trimmomatic/</code><ul> <li><code>&lt;sample-id&gt;.fastq.gz</code>: file with the trimmed fastq reads.</li> <li><code>log/&lt;sample-id&gt;.*{html,txt,zip}</code>: log files generated by Trimmomatic.</li> </ul> </li> </ul> <p>By default, viralgenie will only provide the report and log files if Trimmomatic is selected. The trimmed reads can be saved by specifying <code>--save_intermediate_reads</code> or <code>--save_final_reads 'trimming'</code>.</p>"},{"location":"output/#umi-deduplication","title":"UMI-deduplication","text":"<p>UMI-deduplication can be done at the read level using <code>HUMID</code>. Viralgenie also provides the opportunity to extract the UMI from the read using <code>UMI-tools extract</code> if the UMI is not in the header. Results will be stored in the <code>preprocessing/umi</code> directory.</p> Output files <ul> <li><code>umi/</code><ul> <li><code>humid/</code><ul> <li><code>log/&lt;sample-id&gt;.log</code>: log file of HUMID.</li> <li><code>annotated/&lt;sample-id&gt;_annotated_*.fastq.gz</code>: annotated FastQ files, reads will have their assigned cluster in the read header.</li> <li><code>deduplicated/&lt;sample-id&gt;_deduplicated_*.fastq.gz</code>: deduplicated FastQ files.</li> </ul> </li> <li><code>umitools/</code><ul> <li><code>log/&lt;sample-id&gt;.log</code>: log file of UMI-tools.</li> <li><code>extracts/&lt;sample-id&gt;.umi_extract*.fastq.gz</code>: fastq file where UMIs have been removed from the read and moved to the read header.</li> </ul> </li> </ul> </li> </ul> <p>By default, viralgenie will not assume reads have UMIs. To enable this, use the parameter <code>--with_umi</code>. Specify where UMI deduplication should occur with <code>--umi_deduplicate</code> if at a <code>read</code> level, on a <code>mapping</code> level, or <code>both</code> at a read and mapping level. The deduplicated reads can be saved by specifying <code>--save_intermediate_reads</code> or <code>--save_final_reads 'deduplication'</code>.</p>"},{"location":"output/#bbduk","title":"BBDuk","text":"<p>BBDuk stands for Decontamination Using Kmers. BBDuk was developed to combine most common data-quality-related trimming, filtering, and masking operations into a single high-performance tool.</p> <p>It is used in viralgenie for complexity filtering using different algorithms. This means that it will remove reads with low sequence diversity (e.g., mono- or dinucleotide repeats).</p> Output files <ul> <li><code>bbduk/</code><ul> <li><code>log/&lt;sample-id&gt;.bbduk.log</code>: log file containing filtering statistics.</li> <li><code>&lt;sample-id&gt;.fastq.gz</code>: resulting FASTQ file without low-complexity reads.</li> </ul> </li> </ul> <p>By default, viralgenie will only provide the log files of BBDuk. The filtered reads can be saved by specifying <code>--save_intermediate_reads</code> or <code>--save_final_reads 'complexity'</code>.</p>"},{"location":"output/#prinseq","title":"prinseq++","text":"<p><code>prinseq++</code> is used in viralgenie for complexity filtering using different algorithms. This means that it will remove reads with low sequence diversity (e.g., mono- or dinucleotide repeats).</p> Output files <ul> <li><code>prinseq/</code><ul> <li><code>log/&lt;sample-id&gt;.log</code>: log file containing filtering statistics.</li> <li><code>&lt;sample-id&gt;.fastq.gz</code>: resulting FASTQ file without low-complexity reads.</li> </ul> </li> </ul> <p>By default, viralgenie will only provide the log files of prinseq. The filtered reads can be saved by specifying <code>--save_intermediate_reads</code> or <code>--save_final_reads 'complexity'</code>.</p>"},{"location":"output/#hostremoval-kraken2","title":"Hostremoval-Kraken2","text":"<p>Kraken2 is a taxonomic sequence classifier that assigns taxonomic labels to DNA sequences. Kraken examines the k-mers within a query sequence and uses the information within those k-mers to query a database. That database maps k-mers to the lowest common ancestor (LCA) of all genomes known to contain a given k-mer.</p> Output files <ul> <li><code>hostremoval-kraken2/</code><ul> <li><code>&lt;sample-id&gt;_kraken2_host.report.txt</code>: A profile of the aligned reads to a given host contamination database.</li> <li><code>&lt;sample-id&gt;_kraken2_host.unclassified*.fastq.gz</code>: resulting FASTQ file with reads that don't have any matches to the given host contamination database.</li> </ul> </li> </ul> <p>By default, viralgenie will only provide the log files of Kraken2 which are visualized in MultiQC. The filtered reads can be saved by specifying <code>--save_intermediate_reads</code> or <code>--save_final_reads 'host'</code>.</p>"},{"location":"output/#metagenomic-diversity","title":"Metagenomic Diversity","text":"<p>The results of the metagenomic diversity analysis are stored in the directory <code>metagenomic_diversity/</code>. Results are also visualized in the MultiQC report. </p>"},{"location":"output/#kraken2","title":"Kraken2","text":"<p>Kraken2 is a taxonomic sequence classifier that assigns taxonomic labels to DNA sequences. Kraken examines the k-mers within a query sequence and uses the information within those k-mers to query a database. That database maps k-mers to the lowest common ancestor (LCA) of all genomes known to contain a given k-mer.</p> Output files <ul> <li><code>metagenomic_diversity/kraken2/</code><ul> <li><code>&lt;sample-id&gt;.report.txt</code>: A Kraken2 report that summarizes the fraction abundance, taxonomic ID, number of k-mers, taxonomic path of all the hits in the Kraken2 run for a given sample. Will be 6 columns rather than 8 if <code>--save_minimizers</code> specified.</li> <li><code>&lt;sample-id&gt;_kraken2_host.unclassified*.fastq.gz</code>: resulting FASTQ file with reads that don't have any matches to the given host contamination database.</li> <li><code>&lt;sample-id&gt;.classified.fastq.gz</code>: FASTQ file containing all reads that had a hit against a reference in the database for a given sample.</li> <li><code>&lt;sample-id&gt;.unclassified.fastq.gz</code>: FASTQ file containing all reads that did not have a hit in the database for a given sample.</li> <li><code>&lt;sample-id&gt;.classifiedreads.txt</code>: A list of read IDs and the hits each read had against the database for a given sample.</li> </ul> </li> </ul> <p>By default, viralgenie will provide any classified or unclassified fastq files, specify this with <code>--kraken2_save_reads</code>. Similarly, for the classified reads table, specify this with <code>--kraken2_save_readclassification</code>.</p>"},{"location":"output/#kaiju","title":"Kaiju","text":"<p>Kaiju is a program for sensitive taxonomic classification of high-throughput sequencing reads from metagenomic data. It is based on the Burrows-Wheeler transform and the lowest common ancestor algorithm.</p> Output files <ul> <li><code>metagenomic_diversity/kaiju/</code><ul> <li><code>&lt;sample-id&gt;.tsv</code>: Raw output from Kaiju with taxonomic rank, read ID, and taxonomic ID.</li> <li><code>&lt;sample-id&gt;.txt</code>: A summary of the taxonomic classification of the reads in the sample.</li> </ul> </li> </ul>"},{"location":"output/#krona","title":"Krona","text":"<p>Krona is a hierarchical data visualization tool that can be used to visualize the taxonomic classification of metagenomic data.</p> Output files <ul> <li><code>metagenomic_diversity/krona/</code><ul> <li><code>&lt;kaiju|kraken2&gt;_.html</code>: A HTML file containing the Krona visualization of the taxonomic classification of the reads in the sample.</li> </ul> </li> </ul> <p></p>"},{"location":"output/#assembly-polishing","title":"Assembly &amp; Polishing","text":"<p>The results of the assembly processes &amp; polishing are stored in the directory <code>assembly/</code>.</p> <p>Multiple intermediate files can be generated during the assembly process, some of them might not always be interesting to have. For this reason, there is an option to save the intermediate files with the <code>--save_intermediate_polishing</code> argument which is by default off.</p>"},{"location":"output/#assemblers","title":"Assemblers","text":"<p>Multiple assemblers [spades, trinity, megahit] can be used which have their results combined. Each assembler has its own directory in the <code>assembly/assemblers</code> directory, where there will be a subfolder for the contigs and the QC results from QUAST.</p> Output files <ul> <li><code>assemblers/</code><ul> <li><code>spades/&lt;spades_mode&gt;/</code><ul> <li><code>contigs/&lt;sample-id&gt;_spades.fa.gz</code>: Contigs generated by SPAdes.</li> <li><code>log/&lt;sample-id&gt;_spades.log</code>: Directory containing the log file of the SPAdes run.</li> <li><code>quast/&lt;sample-id&gt;_spades.tsv</code>: Directory containing the QUAST report.</li> </ul> </li> <li><code>trinity/</code><ul> <li><code>contigs/&lt;sample-id&gt;_trinity.fa.gz</code>: Contigs generated by Trinity.</li> <li><code>quast/&lt;sample-id&gt;_trinity.tsv</code>: Directory containing the QUAST report.</li> </ul> </li> <li><code>megahit/</code><ul> <li><code>contigs/&lt;sample-id&gt;_megahit.fa.gz</code>: Contigs generated by Megahit.</li> <li><code>quast/&lt;sample-id&gt;_megahit.tsv</code>: Directory containing the QUAST report.</li> </ul> </li> </ul> </li> </ul> <p>QUAST results are also summarized and plotted in the MultiQC report.</p> <p></p> <p>Finally, the results of the assemblers are combined and stored in the <code>tools_combined/</code> directory.</p> Output files <ul> <li><code>assemblers</code><ul> <li><code>tools_combined/&lt;sample-id&gt;.combined.fa</code>: Contigs generated by combining the results of the assemblers.</li> </ul> </li> </ul>"},{"location":"output/#sspace-basic","title":"SSPACE Basic","text":"<p>SSPACE Basic is a tool for scaffolding contigs using paired-end reads. It is modified from the SSAKE assembler and has the feature of extending contigs using reads that are unmappable in the contig assembly step.</p> Output files <ul> <li><code>sspace_basic/</code><ul> <li><code>scaffolds/&lt;sample-id&gt;.scaffolds.fasta</code>: Scaffolds generated by SSPACE Basic.</li> <li><code>log/&lt;sample-id&gt;.*.txt</code>: Various txt files containing log and summary information on the SSPACE Basic run.</li> </ul> </li> </ul>"},{"location":"output/#prinseq-contigs","title":"prinseq++ - contigs","text":"<p><code>prinseq++</code> is used for complexity filtering of contigs.</p> Output files <ul> <li><code>prinseq/</code><ul> <li><code>scaffolds/&lt;sample-id&gt;.scaffolds.fasta</code>: Scaffolds generated by SSPACE Basic.</li> <li><code>log/&lt;sample-id&gt;.*.txt</code>: Various txt files containing log and summary information on the SSPACE Basic run.</li> </ul> </li> </ul>"},{"location":"output/#blast","title":"BLAST","text":"<p>BLAST is a sequence comparison tool that can be used to compare a query sequence against a database of sequences. In viralgenie, BLAST is used to compare the contigs generated by the assemblers to a database of viral sequences.</p> <p>By default, viralgenie will only provide the BLAST results in a tabular format. It will have selected only for the top five hits and will also have a filtered version where it will only include hits with an e-value of 0.01 or lower, a bitscore of 50 or higher, and an alignment percentage of 0.80 or higher.</p> Column names <ul> <li>qseqid</li> <li>sseqid</li> <li>stitle</li> <li>pident</li> <li>qlen</li> <li>slen</li> <li>length</li> <li>mismatch</li> <li>gapopen</li> <li>qstart</li> <li>qend</li> <li>sstart</li> <li>send</li> <li>evalue</li> <li>bitscore</li> </ul> Output files <ul> <li><code>polishing/</code><ul> <li><code>blast/&lt;sample-id&gt;_filter.tsv</code>: Filtered BLAST results in tabular format.</li> <li><code>intermediate/blast/filtered-sequences/&lt;sample-id&gt;_withref.fa</code>: Contigs with the blast hit sequence in a fasta file.</li> <li><code>intermediate/blast/hits/&lt;sample-id&gt;.txt</code>: unfiltered BLAST results in tabular format.</li> </ul> </li> </ul> <p>By default, viralgenie will only provide the filtered blast.txt file. The intermediate files can be saved by specifying <code>--save_intermediate_polishing</code>.</p>"},{"location":"output/#preclustering-kaiju-kraken2","title":"Preclustering - Kaiju &amp; Kraken2","text":"<p>Kaiju is a program for sensitive taxonomic classification of high-throughput sequencing reads from metagenomic data. It is based on the Burrows-Wheeler transform and the lowest common ancestor algorithm.</p> <p>Kraken2 is a taxonomic sequence classifier that assigns taxonomic labels to DNA sequences. Kraken examines the k-mers within a query sequence and uses the information within those k-mers to query a database. That database maps k-mers to the lowest common ancestor (LCA) of all genomes known to contain a given k-mer.</p> Output files <ul> <li><code>polishing/intermediate/precluster</code><ul> <li><code>kaiju/&lt;sample-id&gt;_kaiju.tsv</code>: Raw output from Kaiju with taxonomic rank, read ID, and taxonomic ID.</li> <li><code>kraken2/&lt;sample-id&gt;_kraken2_reports.txt</code>: A Kraken2 report that summarizes the fraction abundance, taxonomic ID, number of k-mers, taxonomic path of all the hits in the Kraken2 run for a given sample. Will be 6 columns rather than 8 if <code>--save_minimizers</code> specified.</li> <li><code>kraken2/&lt;sample-id&gt;_kraken2.classifiedreads.txt</code>: A list of read IDs and the hits each read had against the database for a given sample.</li> <li><code>merged_classifications/&lt;sample-id&gt;.txt</code>: Taxonomy merged based on the specified strategy, filtered based on specified filters, and simplified up to a certain taxonomy with the columns being taxonomic rank, read ID, and taxonomic ID.</li> <li><code>sequences/&lt;sample-id&gt;/&lt;sample-id&gt;_taxid&lt;taxonomic ID&gt;.fa</code>: Fasta file with the contigs that were classified to that specific taxonomic ID.</li> </ul> </li> </ul> <p>By default, viralgenie will not provide any preclustering files. The intermediate files can be saved by specifying <code>--save_intermediate_polishing</code>.</p>"},{"location":"output/#clustering","title":"Clustering","text":"<p>The output files of each clustering method are directly put in the <code>assembly/polishing</code> directory, with the exception of a summary file that is generated by the pipeline for each cluster with the size of the cluster, the centroid, etc.</p> Output files <ul> <li><code>polishing/intermediate/cluster/</code><ul> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;.summary_mqc.tsv</code>: A tabular file with comments used for MultiQC with statistics on the number of identified clusters in a sample.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;.clusters.tsv</code>: A tabular file with metadata on all clusters in a sample. It's the JSON file of all clusters in a table format.</li> </ul> </li> </ul> <p>Tip</p> <p>Whenever there is a 'cl#' in the file name, it refers to the cluster number of that sample.</p> <p>By default, viralgenie will not provide any clustering overview files. The intermediate files can be saved by specifying <code>--save_intermediate_polishing</code>.</p>"},{"location":"output/#cd-hit-est","title":"CD-HIT-EST","text":"<p>CD-HIT is a very fast, widely used program for clustering and comparing protein or nucleotide sequences.</p> Output files <ul> <li><code>polishing/cdhit/</code><ul> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;.fa.clstr</code>: A cluster file containing the clustering information. where  \"&gt;\" starts a new cluster, a \"*\" at the end means that this sequence is the representative or centroid of this cluster, and a \"%\" is the identity between this sequence and the representative.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;.fa</code>: A fasta file containing the centroid sequence.</li> </ul> </li> </ul>"},{"location":"output/#vsearch-cluster","title":"vsearch-cluster","text":"<p>vsearch implements a single-pass, greedy centroid-based clustering algorithm, similar to the algorithms implemented in usearch, DNAclust, and sumaclust for example. The output has to be in the <code>--uc</code> format or else the pipeline will not be able to process the output.</p> Output files <ul> <li><code>polishing/vsearch/</code><ul> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;.tsv.gz</code>: A cluster file containing the clustering information.</li> </ul> </li> </ul> vsearch -uc columns <ol> <li>Entry (S, H, or C):</li> <li>Record type: S, H, or C.</li> <li>Cluster number (zero-based).</li> <li>Centroid length (S), query length (H), or cluster size (C).</li> <li>Percentage of similarity with the centroid sequence (H), or set to \u2019*\u2019 (S, C).</li> <li>Match orientation + or - (H), or set to \u2019\u2019 (S, C). Not used, always set to \u2019\u2019 (S, C) or to zero (H).</li> <li>Not used, always set to \u2019*\u2019 (S, C) or to zero (H).</li> <li>Set to \u2019*\u2019 (S, C) or, for H, compact representation of the pairwise alignment using the CIGAR format (Compact Idiosyncratic Gapped Alignment Report): M (match/mismatch), D (deletion), and I (insertion). The equal sign \u2019=\u2019 indicates that the query is identical to the centroid sequence.</li> <li>Label of the query sequence (H), or of the centroid sequence (S, C). 10. Label of the centroid sequence (H), or set to \u2019*\u2019 (S, C).</li> </ol>"},{"location":"output/#mmseqs2","title":"MMseqs2","text":"<p>MMseqs2 is a software suite to search and cluster huge protein and nucleotide sequence sets. The cascaded clustering workflow (<code>mmseqs-cluster</code>) first runs linclust, the linear-time clustering module of mmseqs (<code>mmseqs-linclust</code>), that can produce clustering\u2019s down to 50% sequence identity in very short time.</p> Output files <ul> <li><code>polishing/</code><ul> <li><code>mmseqs2/&lt;sample-id&gt;/&lt;sample-id&gt;.tsv</code>: A cluster file containing the clustering information. Where the first column is the cluster representative and the second column the member.</li> <li><code>intermediate/mmseqs/clustered_db/&lt;sample-id&gt;*</code>: A MMseqs2 database of the clustered sequences.</li> <li><code>intermediate/mmseqs/sequence_db/&lt;sample-id&gt;*</code>: A MMseqs2 database of the input sequences (contigs + blast hits).</li> </ul> </li> </ul>"},{"location":"output/#vrhyme","title":"vRhyme","text":"<p>vRhyme is a multi-functional tool for binning virus genomes from metagenomes. vRhyme functions by utilizing coverage variance comparisons and supervised machine learning classification of sequence features to construct viral metagenome-assembled genomes (vMAGs).</p> Output files <ul> <li><code>polishing/vrhyme</code><ul> <li><code>&lt;sample-id&gt;/vRhyme_best_bins.#.membership.tsv</code>: scaffold membership of best bins.</li> <li><code>&lt;sample-id&gt;/vRhyme_best_bins.#.summary.tsv</code>: summary stats of best bins.</li> </ul> </li> </ul>"},{"location":"output/#mash","title":"Mash","text":"<p>Mash calculates the distance between two sequences based on the jaccard distance. The Mash distance can be quickly computed from the size-reduced sketches alone, yet produces a result that strongly correlates with alignment-based measures such as the Average Nucleotide Identity (ANI).</p> Output files <ul> <li><code>polishing/mash</code><ul> <li><code>&lt;sample-id&gt;/dist/*.tsv</code>: A distance matrix of the genomes with ANI.</li> <li><code>&lt;sample-id&gt;/cluster/*.tsv</code>: A table where the first column represents the contig/genome and the second column it's corresponding cluster.</li> <li><code>&lt;sample-id&gt;/visual/*.png</code>: A visualization of the network.</li> </ul> </li> </ul> <p>The network of a triple segmented Hazara virus looks like this, each node represents a contig colored on cluster. The edge represents that the ANI is higher than the specified <code>--identity_threshold</code>.</p> <p></p> <p>What are those names?</p> <p>Most assemblers tend to give each contig name a specific prefix. For example,</p> <ul> <li>Trinity: <code>'TRINITY_...'</code></li> <li>SPAdes: <code>'NODE_...'</code></li> <li>Megahit: <code>'k\\d{3}_...'</code></li> </ul> <p>Based on these prefixes viralgenie separates external references from denovo contigs. If any assemblers are added, consider specifying a specific regex for <code>--assembler_patterns</code>.</p>"},{"location":"output/#minimap2","title":"Minimap2","text":"<p>Minimap2 is a versatile sequence alignment program that aligns larger DNA or mRNA sequences against a large reference database.</p> Output files <ul> <li><code>polishing/scaffolding/&lt;sample-id&gt;/minimap</code><ul> <li><code>&lt;sample-id&gt;_cl#.bam</code>: A BAM file containing the alignment of contigs to the centroid.</li> <li><code>&lt;sample-id&gt;_cl#.mmi</code>: The centroid index file.</li> </ul> </li> </ul> <p>By default, viralgenie will not provide the minimap output files. The intermediate files can be saved by specifying <code>--save_intermediate_polishing</code>.</p>"},{"location":"output/#ivar-contig-consensus","title":"iVar contig consensus","text":"<p>iVar is a computational method for calling consensus sequences from viral populations.</p> Output files <ul> <li><code>polishing/scaffolding/&lt;sample-id&gt;</code><ul> <li><code>&lt;sample-id&gt;_cl#_consensus.fa</code>: A fasta file containing the consensus sequence of the cluster.</li> <li><code>&lt;sample-id&gt;_cl#_consensus.mpileup</code>: A mpileup file containing depth at each position of the consensus sequence.</li> <li><code>hybrid-&lt;sample-id&gt;_cl#_consensus.fa</code>: A fasta file containing the hybrid consensus sequence of the cluster and the reference.</li> <li><code>/visualised/</code><ul> <li><code>*.png</code>: A visualization of the consensus sequence displaying which regions came from the reference and which from the contigs.</li> <li><code>*.txt</code>: The alignment of the reference to the consensus sequence written as a blast alignment.</li> </ul> </li> </ul> </li> </ul> <p>By default, viralgenie will not provide the iVar output files. The intermediate files can be saved by specifying <code>--save_intermediate_polishing</code>.</p> <p>A visualization is made to show which regions came from the external reference (red) and which from the denovo contigs (green). For example,</p> <p></p> <p>Info</p> <p>The hybrid consensus is generated by mapping the contigs to the reference and then calling the consensus sequence. This is done to fill in the gaps in the contigs with the reference sequence, if there are no positions with 0 coverage there will not be a hybrid consensus and the output from iVar will be used.</p>"},{"location":"output/#variant-calling-iterative-refinement","title":"Variant Calling &amp; Iterative Refinement","text":"<p>The results from variant calling, resulting from the mapping constraints &amp; the final round of polishing are stored in the directory <code>variant_calling/</code>.</p> <p>Info</p> <p>Mapping constraints are combined with the specified samples, here, the identifier of the mapping constraint combined with the sample identifier. All results will have a new prefix which is <code>&lt;sample-id&gt;_&lt;mapping_constraint_id&gt;-CONSTRAINT</code>.</p> <p>The results from the iterations are stored with the same structure as the final round of polishing in the <code>assembly/polishing/iterations/it#</code> directory.</p> <p>Info</p> <p>To be able to make a distinction between the output files of the iterations, viralgenie follows a schema where it starts from <code>singletons</code> or a <code>consensus</code> goes through the iterations and ends with the <code>variant-calling</code>. The output files will have the following structure: <pre><code>graph LR\n    F[singleton] --&gt; B[Iteration 1: 'it1']\n    A[consensus] --&gt; B[Iteration 1: 'it1']\n    B --&gt; C[Iteration 2: 'it2']\n    C --&gt; D[...]\n    D --&gt; E[Variant-calling: 'itvariant-calling']</code></pre></p> <p>The prefix of the sample is combined with the previous state of the sample. For example, in the first iteration (directory <code>iterations/it1</code>), reads will be mapped to the reference-assisted de novo consensus sequence (ie <code>consensus</code>) and so the output file will be <code>assembly/polishing/iterations/it1/bwamem2/bam/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_consensus.bam</code>.</p>"},{"location":"output/#reference-selection","title":"Reference selection","text":"<p>The reference selection is done using <code>mash</code> tool. Here the reference file is sketched (<code>variants/mapping-info/mash/sketch</code>) and compared to the reads (<code>variants/mapping-info/mash/screen</code>) where the reference with the highest estimated average nucleotide identity (ANI) and shared hashes is selected (<code>variants/mapping-info/mash/select-ref</code>).</p> Output files <ul> <li><code>variants/mapping-info/mash</code><ul> <li><code>sketch/&lt;sample-id&gt;_&lt;constraint-id&gt;.msh</code>: The sketch file of the reads.</li> <li><code>screen/&lt;sample-id&gt;_&lt;constraint-id&gt;.screen</code>: The tab results file of the comparisons between references and reads.</li> <li><code>select-ref/&lt;sample-id&gt;_&lt;constraint-id&gt;.json</code>: The reference with the highest estimated ANI and shared hashes.</li> </ul> </li> </ul> Column names: mash-screen <ul> <li>identity</li> <li>shared-hashes</li> <li>median-multiplicity</li> <li>p-value</li> <li>query-ID</li> <li>query-comment</li> </ul>"},{"location":"output/#read-mapping","title":"Read mapping","text":"<p>The mapping results are stored in the directory <code>variants/mapping-info/</code> or in the iterations directory <code>assembly/polishing/iterations/it#</code>.</p> <p>If bowtie is used, the output from the raw mapping results (in addition to the results after deduplication) are included in the multiqc report.</p> <p></p> Output files - variants <ul> <li><code>variants/mapping-info/</code><ul> <li><code>bwamem2/</code><ul> <li><code>index/&lt;sample-id&gt;_&lt;constraint-id&gt;/*</code>: The index files of the consensus.</li> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>unmapped/&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.fastq.gz</code>: A fastq file containing the unmapped reads.</li> </ul> </li> <li><code>bwamem/</code><ul> <li><code>index/&lt;sample-id&gt;_&lt;constraint-id&gt;/*</code>: The index files of the consensus.</li> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>unmapped/&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.fastq.gz</code>: A fastq file containing the unmapped reads.</li> </ul> </li> <li><code>bowtie2/</code><ul> <li><code>build/&lt;sample-id&gt;_&lt;constraint-id&gt;/*</code>: The index files of the consensus.</li> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>unmapped/&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.fastq.gz</code>: A fastq file containing the unmapped reads.</li> <li><code>log/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.log</code>: A log file of the bowtie2 run.</li> </ul> </li> </ul> </li> </ul> Output files - iterations <ul> <li><code>assembly/polishing/iterations/it#/</code><ul> <li><code>bwamem2/</code><ul> <li><code>index/&lt;sample-id&gt;/*</code>: The index files of the consensus.</li> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>unmapped/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.fastq.gz</code>: A fastq file containing the unmapped reads.</li> </ul> </li> <li><code>bwamem/</code><ul> <li><code>index/&lt;sample-id&gt;/*</code>: The index files of the consensus.</li> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>unmapped/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.fastq.gz</code>: A fastq file containing the unmapped reads.</li> </ul> </li> <li><code>bowtie2/</code><ul> <li><code>build/&lt;sample-id&gt;/*</code>: The index files of the consensus.</li> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>unmapped/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.fastq.gz</code>: A fastq file containing the unmapped reads.</li> <li><code>log/&lt;sample-id&gt;_cl#_it#.log</code>: A log file of the bowtie2 run.</li> </ul> </li> </ul> </li> </ul>"},{"location":"output/#deduplication","title":"Deduplication","text":"<p>To accommodate for PCR duplicates, the reads are deduplicated. The deduplication results are stored in the directory <code>variants/mapping-info/deduplicate/</code> or in the iterations directory <code>assembly/polishing/iterations/it#/deduplicate</code>.</p> <p>Deduplication results are also visualized within the MultiQC report.</p>"},{"location":"output/#umi-tools","title":"UMI-tools","text":"<p><code>UMI-tools</code> is a set of tools for handling Unique Molecular Identifiers (UMIs) in NGS data. The deduplication is done by the <code>dedup</code> tool.</p> <p>Number of deduplicated reads: </p> <p>Summary statistics: </p> Output files - variants <ul> <li><code>variants/mapping-info/deduplicate/</code><ul> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.umi_deduplicated.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>log/</code><ul> <li><code>&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.umi_deduplicated.log</code>: A log file of the UMI-tools run.</li> <li><code>&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.umi_deduplicated_edit_distance.tsv</code>: Reports the (binned) average edit distance between the UMIs at each position.</li> <li><code>&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.umi_deduplicated_per_umi.tsv</code>: UMI-level summary statistics.</li> <li><code>&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.umi_deduplicated_per_umi_per_position.tsv</code>: Tabulates the counts for unique combinations of UMI and position.</li> </ul> </li> </ul> </li> </ul> Output files - iterations <ul> <li><code>assembly/polishing/iterations/it#/deduplicate</code><ul> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.umi_deduplicated.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>log/</code><ul> <li><code>&lt;sample-id&gt;_cl#_it#.umi_deduplicated.log</code>: A log file of the UMI-tools run.</li> <li><code>&lt;sample-id&gt;_cl#_it#.umi_deduplicated_edit_distance.tsv</code>: Reports the (binned) average edit distance between the UMIs at each position.</li> <li><code>&lt;sample-id&gt;_cl#_it#.umi_deduplicated_per_umi.tsv</code>: UMI-level summary statistics.</li> <li><code>&lt;sample-id&gt;_cl#_it#.umi_deduplicated_per_umi_per_position.tsv</code>: Tabulates the counts for unique combinations of UMI and position.</li> </ul> </li> </ul> </li> </ul>"},{"location":"output/#picard-mark-duplicates","title":"Picard - Mark Duplicates","text":"<p><code>Picard</code> is a set of command line tools for manipulating high-throughput sequencing data and formats such as SAM/BAM/CRAM and VCF. The deduplication is done by the <code>MarkDuplicates</code> tool.</p> <p></p> Output files - variants <ul> <li><code>variants/mapping-info/deduplicate/</code><ul> <li><code>picard/</code><ul> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>log/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.dedup.MarkDuplicates.metrics.txt</code>: Deduplication metrics from Picard.</li> </ul> </li> </ul> </li> </ul> Output files - iterations <ul> <li><code>assembly/polishing/iterations/it#/deduplicate</code><ul> <li><code>picard/</code><ul> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>log/&lt;sample-id&gt;_cl#_it#.dedup.MarkDuplicates.metrics.txt</code>: Deduplication metrics from Picard.</li> </ul> </li> </ul> </li> </ul>"},{"location":"output/#mapping-statistics","title":"Mapping statistics","text":"<p>Info</p> <p>If <code>--deduplicate</code> is set to <code>true</code> [default], all metrics will be calculated on the deduplicated bam file.</p>"},{"location":"output/#samtools","title":"Samtools","text":"<p>Samtools is a suite of programs for interacting with high-throughput sequencing data. We use samtools in this pipeline to obtain mapping statistics from three tools: <code>flagstat</code>, <code>idxstats</code> and <code>stats</code>.</p> <p> </p> Output files - variants <ul> <li><code>variants/mapping-info/metrics</code><ul> <li><code>flagstat/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.flagstat</code>: A text file containing the flagstat output.</li> <li><code>idxstats/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.idxstats</code>: A text file containing the idxstats output.</li> <li><code>stats/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.stats</code>: A text file containing the stats output.</li> </ul> </li> </ul> Output files - iterations <ul> <li><code>assembly/polishing/iterations/it#/metrics</code><ul> <li><code>flagstat/&lt;sample-id&gt;_cl#_it#.flagstat</code>: A text file containing the flagstat output.</li> <li><code>idxstats/&lt;sample-id&gt;_cl#_it#.idxstats</code>: A text file containing the idxstats output.</li> <li><code>stats/&lt;sample-id&gt;_cl#_it#.stats</code>: A text file containing the stats output.</li> </ul> </li> </ul>"},{"location":"output/#picard-collect-multiple-metrics","title":"Picard - Collect Multiple Metrics","text":"<p><code>Picard</code> is a set of command line tools for manipulating high-throughput sequencing data. We use picard-tools in this pipeline to obtain mapping and coverage metrics.</p> Output files - variants <ul> <li><code>variants/mapping-info/metrics/picard</code><ul> <li><code>*.CollectMultipleMetrics.*</code>: Alignment QC files from picard CollectMultipleMetrics in <code>*_metrics</code> textual format.</li> <li><code>*.pdf</code> plots for metrics obtained from CollectMultipleMetrics.</li> </ul> </li> </ul> Output files - iterations <ul> <li><code>assembly/polishing/iterations/it#/metrics/picard</code><ul> <li><code>*.CollectMultipleMetrics.*</code>: Alignment QC files from picard CollectMultipleMetrics in <code>*_metrics</code> textual format.</li> <li><code>*.pdf</code> plots for metrics obtained from CollectMultipleMetrics.</li> </ul> </li> </ul>"},{"location":"output/#custom-mpileup-like-file","title":"Custom - mpileup like file","text":"<p>To facilitate the intra host analysis, a mpileup like file is generated. This file contains the depth of every nucleotide at each position of the reference as well as the shannon entropy and a weighted shannon entropy based on the following formulae.</p> <ul> <li>Shannon entropy: $$ H = -\\sum_{i=1}^4 p_i \\ln p_i $$</li> <li>Weighted Shannon entropy: $$ w(H) = \\frac{N}{N+k} \\cdot H $$</li> </ul> <p>Where \\(N\\) is the total bases at a position, \\(k\\) is the pseudocount (default 50), and \\(p_i\\) is the frequency of the nucleotide \\(i\\).</p> Output files - variants <ul> <li><code>variants/mapping-info/custom-vcf/&lt;sample-id&gt;</code><ul> <li><code>*.tsv</code>: A custom tsv file containing the depth of every nucleotide at each position of the reference.</li> </ul> </li> </ul> Output files - iterations <ul> <li><code>assembly/polishing/iterations/it#/custom-vcf/&lt;sample-id&gt;</code><ul> <li><code>*.tsv</code>: A custom tsv file containing the depth of every nucleotide at each position of the reference.</li> </ul> </li> </ul>"},{"location":"output/#mosdepth-coverage","title":"Mosdepth - Coverage","text":"<p>mosdepth is a fast BAM/CRAM depth calculation for WGS, exome, or targeted sequencing. mosdepth is used in this pipeline to obtain genome-wide coverage values in 200bp windows. The results are rendered in MultiQC (genome-wide coverage).</p> <p> </p> Output files - variants <ul> <li><code>variants/mapping-info/metrics/mosdepth</code><ul> <li><code>&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.per-base.bed.gz</code>: A bed file containing the coverage values in 200bp windows.</li> <li><code>&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.per-base.bed.gz.csi</code>: Indexed bed file.</li> <li><code>&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.mosdepth.summary.txt</code>: Summary metrics including mean, min and max coverage values.</li> <li><code>&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.mosdepth.global.dist.txt</code>: A cumulative distribution indicating the proportion of total bases that were covered for at least a given coverage value.</li> </ul> </li> </ul> Output files - iterations <ul> <li><code>assembly/polishing/iterations/it#/metrics/mosdepth</code><ul> <li><code>&lt;sample-id&gt;_cl#_it#.per-base.bed.gz</code>: A bed file containing the coverage values in 200bp windows.</li> <li><code>&lt;sample-id&gt;_cl#_it#.per-base.bed.gz.csi</code>: Indexed bed file.</li> <li><code>&lt;sample-id&gt;_cl#_it#.mosdepth.summary.txt</code>: Summary metrics including mean, min and max coverage values.</li> <li><code>&lt;sample-id&gt;_cl#_it#.mosdepth.global.dist.txt</code>: A cumulative distribution indicating the proportion of total bases that were covered for at least a given coverage value.</li> </ul> </li> </ul>"},{"location":"output/#variant-calling-filtering","title":"Variant calling &amp; filtering","text":"<p>Variant calling is done with <code>BCFTools mpileup</code> or <code>iVar</code>, the filtering with <code>BCFtools filter</code>.</p> <p>Variant files are visualized in the MultiQC report.</p> <p></p> Output files - variants <ul> <li><code>variants/variant_calling</code><ul> <li><code>bcftools/</code><ul> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.vcf.gz</code>: A VCF file containing the variant calls.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.norm.vcf.gz</code>: A compressed VCF file where multiallelic sites are split up into biallelic records and SNPs and indels should be merged into a single record.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.filtered.vcf.gz</code>: A compressed VCF file containing the filtered variants.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.vcf.gz.tbi</code>: An index file for the compressed VCF file.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT_stats.txt</code>: A text file stats which is suitable for machine processing and can be plotted using plot-vcfstats.</li> </ul> </li> <li><code>ivar/</code><ul> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.ivar.tsv</code>: A tabular file containing the variant calls.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.ivar.vcf</code>: A VCF file containing the variant calls.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.ivar.variant_counts.log</code>: A summary file containing the number of indels and SNPs.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.filtered.vcf.gz</code>: A compressed VCF file containing the variant calls.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT_stats.txt</code>: A text file stats which is suitable for machine processing and can be plotted using plot-vcfstats.</li> </ul> </li> </ul> </li> </ul> Output files - iterations <pre><code>- `assembly/polishing/iterations/it#/variants/variant_calling`\n    - `bcftools/`\n        - `&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.vcf.gz`: A VCF file containing the variant calls.\n        - `&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.norm.vcf.gz`: A compressed VCF file where multiallelic sites are split up into biallelic records and SNPs and indels should be merged into a single record.\n        - `&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.filtered.vcf.gz`: A compressed VCF file containing the filtered variants.\n        - `&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.vcf.gz.tbi`: An index file for the compressed VCF file.\n        - `&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.stats.txt`: A text file stats which is suitable for machine processing and can be plotted using plot-vcfstats.\n    - `ivar/`\n        - `&lt;sample-id&gt;_cl#_it#.ivar.tsv`: A tabular file containing the variant calls.\n        - `&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.ivar.vcf`: A VCF file containing the variant calls.\n        - `&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.ivar.variant_counts.log`: A summary file containing the number of indels and SNPs.\n        - `&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.filtered.vcf.gz`: A compressed VCF file containing the variant calls.\n        - `&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.stats.txt`: A text file stats which is suitable for machine processing and can be plotted using plot-vcfstats.\n</code></pre>"},{"location":"output/#consensus-generation","title":"Consensus generation","text":"<p>The consensus sequences are generated by <code>BCFTools</code> or <code>iVar</code>. The consensus sequences are stored in the directory <code>consensus/</code> or in the iterations directory <code>assembly/polishing/iterations/it#/consensus</code>.</p> <p><code>BCFtools</code> will use the filtered variants file whereas, <code>iVar</code> will redetermine the variants to collapse in the consensus using their own workflow, read more about their differences in the consensus calling section.</p> Output files - iterations &amp; variants <ul> <li><code>consensus</code><ul> <li><code>seq/&lt;it# | scaffold_consensus | variant-calling | constraint&gt;/</code><ul> <li><code>&lt;sample-id&gt;/*.fasta</code>: A fasta file containing the consensus sequence.</li> </ul> </li> <li><code>mask/&lt;it# | variant-calling | constraint&gt;</code><ul> <li><code>&lt;sample-id&gt;/*.qual.txt</code>: A log file of the consensus run containing statistics. [<code>iVar</code> only]</li> <li><code>&lt;sample-id&gt;/*.bed</code>: A bed file containing the masked regions. [<code>BCFtools</code> only]</li> <li><code>&lt;sample-id&gt;/*.mpileup</code>: A mpileup file containing information on the depth and the quality of each aligned base.</li> </ul> </li> </ul> </li> </ul>"},{"location":"output/#consensus-quality-control","title":"Consensus Quality control","text":"<p>Consensus quality control is done with multiple tools, the results are stored in the directory <code>consensus/quality_control/</code>.</p>"},{"location":"output/#quast","title":"Quast","text":"<p>QUAST is a quality assessment tool for genome assemblies. It calculates various metrics such as N50, L50, number of contigs, number of mismatches, number of indels, etc.</p> Output files <ul> <li><code>consensus/quality_control/quast/</code><ul> <li><code>&lt;sample-id&gt;/&lt;iteration&gt;/&lt;sample-id&gt;_&lt;cl# | constraint-id&gt;.tsv</code>: A tabular file containing the QUAST report.</li> </ul> </li> </ul> <p>If no iterative refinement was run, the output will be in the <code>consensus/quality_control/quast/&lt;sample-id&gt;/constraint</code> directory.</p>"},{"location":"output/#checkv","title":"CheckV","text":"<p><code>CheckV</code> is a tool for assessing the quality of viral genomes recovered from metagenomes. It calculates various metrics such as the number of viral genes, the number of viral contigs, the number of viral genomes, etc.</p> Output files <ul> <li><code>consensus/quality_control/checkv/</code><ul> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;cl# | constraint-id&gt;/quality_summary.tsv</code>: A tabular file that integrates the results from the three main modules of checkv and should be the main output referred to.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;cl# | constraint-id&gt;/completeness.tsv</code>: A detailed overview of how completeness was estimated.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;cl# | constraint-id&gt;/contamination.tsv</code>: A detailed overview of how contamination was estimated.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;cl# | constraint-id&gt;/complete_genomes.tsv</code>: A detailed overview of putative genomes identified.</li> </ul> </li> </ul>"},{"location":"output/#prokka","title":"Prokka","text":"<p><code>Prokka</code> is a whole genome annotation pipeline for identifying features of interest in a set of genomic DNA sequences, and labelling them with useful information. Prokka is a software tool to annotate bacterial, archaeal and viral genomes.)</p> Output files <ul> <li><code>consensus/quality_control/prokka/</code><ul> <li>`//* directories containing the prokka output files."},{"location":"output/#blastn","title":"BLASTn","text":"<p>BLAST is a tool for comparing primary biological sequence information. The output from the BLAST run is stored in the directory <code>consensus/quality_control/blast/</code>. Final consensus genomes are searched against the <code>--reference_pool</code>.</p> Column names <ul> <li>qseqid</li> <li>sseqid</li> <li>stitle</li> <li>pident</li> <li>qlen</li> <li>slen</li> <li>length</li> <li>mismatch</li> <li>gapopen</li> <li>qstart</li> <li>qend</li> <li>sstart</li> <li>send</li> <li>evalue</li> <li>bitscore</li> </ul> <p>Modifying blast columns</p> <p>Modifying these columns can be done through a custom config file and by updating <code>bin/utils/constant_variables.py</code>.</p> Output files <ul> <li><code>consensus/quality_control/blast/</code> A tabular file containing the BLAST report of all intermediate &amp; final results.</li> </ul>"},{"location":"output/#mmseqs-search-annotation","title":"MMseqs-search (annotation)","text":"<p>MMseqs-search is an ultra-fast and sensitive search tool for protein and nucleotide databases. Viralgenie uses MMseqs to search the consensus genomes in an annotated database, like Virousarus (see also defining your own custom annotation database), and uses the annotation data of the best hit to assign the consensus genome a species name, segment name, expected host, and any other metadata that is embedded within the database.</p> Column names <ul> <li>qseqid</li> <li>sseqid</li> <li>stitle</li> <li>pident</li> <li>qlen</li> <li>slen</li> <li>length</li> <li>mismatch</li> <li>gapopen</li> <li>qstart</li> <li>qend</li> <li>sstart</li> <li>send</li> <li>evalue</li> <li>bitscore</li> </ul> <p>Modifying mmseqs columns</p> <p>Modifying these columns can be done through a custom config file and by updating <code>bin/utils/constant_variables.py</code>.</p> Output files <ul> <li><code>consensus/quality_control/mmseqs-search/all_genomes_annotation.hits.tsv</code>: A tabular file containing the MMseqs-search hits, all genomes are combined to reduce the number of jobs.</li> </ul>"},{"location":"output/#mafft","title":"MAFFT","text":"<p>MAFFT is a multiple sequence alignment program for amino acid or nucleotide sequences. The output from the MAFFT run is stored in the directory <code>consensus/quality_control/mafft/</code>.</p> <p>It is used to align the following genomic data: - The final consensus genome - The identified reference genome from <code>--reference_pool</code> - The denovo contigs from each assembler (that constituted the final consensus genome) - Each consensus genome from the iterative refinement steps.</p> Output files <ul> <li><code>consensus/quality_control/mafft/</code><ul> <li><code>&lt;sample-id&gt;/&lt;sample_id&gt;_cl#_iterations.fas</code>: A fasta file containing a multiple sequence alignment of only the iterations.</li> <li><code>&lt;sample-id&gt;/&lt;sample_id&gt;_cl#_aligned.fas</code>: A fasta file containing a multiple sequence alignment of the denovo contigs, the reference from reference_pool and the consensus from iterations.</li> </ul> </li> </ul> <p>Alignment can then be opened with MSA viewer, for example Jalview</p> <p></p>"},{"location":"output/#multiqc","title":"MultiQC","text":"<p>MultiQC is a visualization tool that generates a single HTML report summarizing all samples in your project. Most of the pipeline QC results are visualized in the report and further statistics are available in the report data directory.</p> <p>Results generated by MultiQC collate pipeline QC from supported tools e.g. FastQC. The pipeline has special steps which also allow the software versions to be reported in the MultiQC output for future traceability. For more information about how to use MultiQC reports, see http://multiqc.info.</p> <p>Furthermore, viralgenie runs MultiQC 2 times, as it uses the output from multiqc to create multiple summary tables of the consensus genomes and their iterations.</p> Output files <ul> <li><code>multiqc/</code><ul> <li><code>multiqc_report.html</code>: a standalone HTML file that can be viewed in your web browser.</li> <li><code>multiqc_data/</code>: directory containing parsed statistics from the different tools used in the pipeline.</li> <li><code>multiqc_dataprep/</code>: preparation files for the generated custom tables.</li> <li><code>multiqc_plots/</code>: directory containing static images from the report in various formats.</li> </ul> </li> <li><code>overview-tables/</code>: a directory with a set of summary TSV files.</li> <li><code>contigs_overview_with_iterations.tsv</code>: A tabular file containing the contig information of the final contig consensus genome and their intermediate iterations.</li> <li><code>contigs_overview.tsv</code>: A tabular file containing the contig information of the final contig consensus genome.</li> <li><code>mapping_overview.tsv</code>: A tabular file containing the mapping information of the final mapped consensus genome, from the argument <code>--mapping_constraints</code>.</li> <li><code>samples_overview.tsv</code>: A tabular file containing the sample information combining information from both <code>contigs_overview.tsv</code> &amp; <code>mapping_overview.tsv</code>.</li> </ul>"},{"location":"output/#pipeline-information","title":"Pipeline information","text":"Output files <ul> <li><code>pipeline_info/</code><ul> <li>Reports generated by Nextflow: <code>execution_report.html</code>, <code>execution_timeline.html</code>, <code>execution_trace.txt</code> and <code>pipeline_dag.dot</code>/<code>pipeline_dag.svg</code>.</li> <li>Reports generated by the pipeline: <code>pipeline_report.html</code>, <code>pipeline_report.txt</code> and <code>software_versions.yml</code>. The <code>pipeline_report*</code> files will only be present if the <code>--email</code> / <code>--email_on_fail</code> parameter's are used when running the pipeline.</li> <li>Reformatted samplesheet files used as input to the pipeline: <code>samplesheet.valid.csv</code>.</li> <li>Parameters used by the pipeline run: <code>params.json</code>.</li> </ul> </li> </ul> <p>Nextflow provides excellent functionality for generating various reports relevant to the running and execution of the pipeline. This will allow you to troubleshoot errors with the running of the pipeline, and also provide you with other information such as launch commands, run times and resource usage. ````</p>"},{"location":"parameters/","title":"Parameters","text":"<p>Need Something more interactive?</p> <p>Use <code>nf-core pipelines launch</code> to interactivly set your parameters: <pre><code>nf-core pipelines launch Joon-Klaps/viralgenie\n</code></pre></p>"},{"location":"parameters/#viralgenie-pipeline-parameters","title":"Viralgenie pipeline parameters","text":"<p>A pipeline to reconstruct consensus genomes and identify intrahost variants from metagenomic sequencing data or enriched based sequencing data like hybrid capture.</p>"},{"location":"parameters/#inputoutput-options","title":"Input/output options","text":"<p>Define where the pipeline should find input data and save output data.</p> Parameter Description Default <code>input</code> Path to comma-separated file containing information about the samples in the experiment. HelpYou will need to create a design file with information about the samples in your experiment before running the pipeline. Use this parameter to specify its location. It has to be a comma-separated file with 3 columns, and a header row. See usage docs. <code>outdir</code> The output directory where the results will be saved. You have to use absolute paths to storage on Cloud infrastructure. <code>metadata</code> Sample metadata that is included in the multiqc report <code>email</code> Email address for completion summary. HelpSet this parameter to your e-mail address to get a summary e-mail with details of the run sent to you when the workflow exits. If set in your user config file (<code>~/.nextflow/config</code>) then you don't need to specify this on the command line for every run."},{"location":"parameters/#preprocessing-options","title":"Preprocessing options","text":"<p>Options related to the trimming, low complexity and host removal steps of the reads</p> Parameter Description Default <code>skip_preprocessing</code> Skip read preprocessing and use input reads for downstream analysis <code>skip_fastqc</code> Skip read quality statistics summary tool 'fastqc' <code>save_final_reads</code> Save reads after the final preprocessing step True <code>save_intermediate_reads</code> Save reads after every preprocessing step <code>with_umi</code> With or without UMI detection <code>skip_umi_extract</code> With or without UMI extraction True <code>umi_deduplicate</code> Specify at what level UMI deduplication should occur. read <code>humid_mismatches</code> Specify the maximum number of mismatches between reads for them to still be considered neighbors. 1 <code>humid_strategy</code> Specify the strategy for UMI-deduplication directional vs cluster directional <code>umitools_dedup_strategy</code> Specify the strategy or method for UMI-tools deduplication on mapping level cluster <code>umi_discard_read</code> Discard R1 / R2 if required 0, meaning not to discard 0 <code>trim_tool</code> The used trimming tool fastp <code>skip_trimming</code> Skip read trimming <code>fastp_deduplicate</code> Use Fastp's deduplicate option <code>fastp_dedup_accuracy</code> Define the accuracy used for hashes while deduplicating with fastp <code>adapter_fasta</code> Fasta file of adapters <code>save_trimmed_fail</code> Specify true to save files that failed to pass trimming thresholds ending in <code>*.fail.fastq.gz</code> <code>save_merged</code> Specify true to save all merged reads to a file ending in <code>*.merged.fastq.gz</code> <code>min_trimmed_reads</code> Inputs with fewer than this reads will be filtered out of the \"reads\" output channel 1 <code>skip_complexity_filtering</code> Skip filtering of low complexity regions in reads HelpLow-complexity sequences are defined as having commonly found stretches of nucleotides with limited information content (e.g. the dinucleotide repeat CACACACACA). Such sequences can produce a large number of high-scoring but biologically insignificant results in database searches True <code>decomplexifier</code> Specify the decomplexifier to use, bbduk or prinseq prinseq <code>contaminants</code> Reference files containing adapter and/or contaminant sequences for sequence kmer matching (used by bbduk) <code>skip_hostremoval</code> Skip the removal of host read sequences <code>host_k2_db</code> Kraken2 database used to remove host and contamination s3://ngi-igenomes/test-data/viralrecon/kraken2_human.tar.gz <code>host_k2_library</code> Kraken2 library(s) required to remove host and contamination HelpOnly used when no host kraken2 database is specified. human <code>skip_host_fastqc</code> Skip the fastqc step after host &amp; contaminants were removed"},{"location":"parameters/#metagenomic-diversity","title":"Metagenomic diversity","text":"<p>Parameters used to determine the metagenomic diversity of the sample</p> Parameter Description Default <code>skip_read_classification</code> Skip determining the metagenomic diversity of the sample <code>read_classifiers</code> Specify the taxonomic read classifiers, choices are 'kaiju,kraken2' kraken2,kaiju <code>save_databases</code> Save the used databases <code>kraken2_db</code> Location of the Kraken2 database https://genome-idx.s3.amazonaws.com/kraken/k2_viral_20230314.tar.gz <code>kraken2_save_reads</code> Save classified and unclassified reads as fastq files <code>kraken2_save_readclassification</code> Save summary overview of read classifications in a txt file <code>kraken2_save_minimizers</code> Save kraken2's used minimizers <code>bracken_db</code> Location of bracken database https://genome-idx.s3.amazonaws.com/kraken/k2_viral_20230314.tar.gz <code>kaiju_db</code> Location of Kaiju database https://kaiju-idx.s3.eu-central-1.amazonaws.com/2023/kaiju_db_rvdb_2023-05-26.tgz <code>kaiju_taxon_rank</code> Level of taxa rank that needs to be determined species"},{"location":"parameters/#assembly","title":"Assembly","text":"<p>Parameters relating to the used assembly methods</p> Parameter Description Default <code>skip_assembly</code> Skip de novo assembly of reads <code>assemblers</code> The specified tools for de novo assembly, multiple options are possible spades,megahit <code>spades_mode</code> Specific SPAdes mode to run rnaviral <code>spades_hmm</code> File or directory with amino acid HMMs for Spades HMM-guided mode. <code>spades_yml</code> Path to yml file containing read information. HelpThe raw FASTQ files listed in this YAML file MUST be supplied to the respective illumina/pacbio/nanopore input channel(s) in addition to this YML. File entries in this yml must contain only the file name and no paths. <code>assembler_patterns</code> Regex pattern to identify contigs that have been made by the assemblers <code>skip_contig_prinseq</code> Skip the filtering of low complexity contigs with prinseq <code>skip_sspace_basic</code> Skip the contig extension with sspace_basic <code>read_distance</code> Specify the mean distance between the paired reads 350 <code>read_distance_sd</code> Specify the deviation of the mean distance that is allowed. HelpFor instance, a mean of 200 and a sd of 0.75. This means that any pair having a distance between 150 and 250 is allowed. 0.75 <code>read_orientation</code> Specify the read orientation. FR"},{"location":"parameters/#polishing","title":"Polishing","text":"<p>Parameters relating to the refinement of de novo contigs</p> Parameter Description Default <code>skip_polishing</code> Skip the refinement/polishing of contigs through reference based scaffolding and read mapping <code>save_intermediate_polishing</code> Save intermediate polishing files HelpThere are multiple processes within the polishing subworkflow that might not contain relevant information <code>reference_pool</code> Set of fasta sequences used as potential references for the contigs https://rvdb.dbi.udel.edu/download/C-RVDBvCurrent.fasta.gz <code>skip_precluster</code> Skip the preclustering of assemblies to facilitate downstream processing of assemblies <code>keep_unclassified</code> Keep the contigs that could not be classified with the taxonomic databases (<code>kaiju_db</code> &amp; <code>kraken2_db</code>) HelpWithin the preclustering step, all contigs will get a taxonomic classification using the provided databases for the metagenomic tools. In some cases, the number of unclassified contigs can be very large if the database is restrictive. This will result in large clusters in downstream processing that can take up a lot of resources despite not being a priority in some analyses. So set it to <code>True</code> if you want to keep unclassified contigs and set it to <code>False</code> if you don't want to keep them.  True <code>precluster_classifiers</code> Specify the metagenomic classifiers to use for contig taxonomy classification: 'kraken2,kaiju' kraken2,kaiju <code>precluster_merge_strategy</code> Taxon conflict resolution mode, must be 1 (Kaiju), 2 (Kraken),  lca, or lowest. HelpThe option -c determines the method of resolving conflicts in the taxonomic assignment for a read.Possible values are '1', '2', 'lca', 'lowest':  '1' -&gt; the taxon id from Kaiju is used.  '2' -&gt; the taxon id from Kraken is used.  'lca' -&gt; the least common ancestor of the two taxon ids from both input files is used.  'lowest' -&gt; the lower rank of the two taxa is used if they are within the same lineage. Otherwise the LCA is used. lca <code>precluster_simplification_level</code> Level of taxonomic simplification HelpThe taxonomic classification of the contigs can be simplified to a certain level. This can be done by specifying the taxonomic rank to which the classification should be simplified. <code>precluster_exclude_taxa</code> Hard constraint for taxa to exclude from the preclustering, if multiple given make sure to enclose with '\"' and separate with a space. HelpThe taxonomic classification of the contigs can be filtered. This can be done by specifying a list of taxa that should be excluded from the results. <code>precluster_exclude_children</code> Taxon ids to exclude along with all their children from the preclustering, if multiple given make sure to enclose with '\"' and separate with a space. HelpThe taxonomic classification of the contigs can be filtered. This can be done by specifying a list of taxa that should be excluded along with all their children from the results. <code>precluster_exclude_parents</code> Taxon ids to exclude along with all their parents from the preclustering, if multiple given make sure to enclose with '\"' and separate with a space. HelpThe taxonomic classification of the contigs can be filtered. This can be done by specifying a list of taxa that should be excluded along with all their parents from the results. <code>precluster_include_children</code> Taxon ids to include along with all their children from the preclustering, if multiple given make sure to enclose with '\"' and separate with a space. HelpThe taxonomic classification of the contigs can be filtered. This can be done by specifying a list of taxa that should be included along with all their children from the results. <code>precluster_include_parents</code> Taxon ids to include along with all their parents from the preclustering, if multiple given make sure to enclose with '\"' and separate with a space. HelpThe taxonomic classification of the contigs can be filtered. This can be done by specifying a list of taxa that should be included along with all their parents from the results. <code>cluster_method</code> Cluster algorithm used for contigs cdhitest <code>mmseqs_cluster_mode</code> Specify the algorithm to partition the network graph from mmseqs HelpThe Greedy Set cover (0) algorithm is an approximation for the NP-complete optimization problem called set cover.Connected component (1) uses transitive connection to cover more remote homologs.Greedy incremental (2) works analogous to CD-HIT clustering algorithm. 0 <code>network_clustering</code> (only with mash) Algorithm to partition the network. HelpMash creates a distance matrix that gets translated into a network of connectected nodes where the edges represent the similarity. This network is then split up using the specified method. - leiden algorithm: a hierarchical clustering algorithm, that recursively merges communities into single nodes by greedily optimizing the modularity - [connected_components] algorithm: a clustering algorithm that defines the largest possible communities where each node within a subset is reachable from every other node in the same subset via any edge . connected_components <code>skip_hybrid_consensus</code> Skip creation of the hybrid consensus, instead keep the scaffold with ambiguous bases if the depth of scaffolds is not high enough. <code>identity_threshold</code> Identity threshold value used in clustering algorithms 0.85 <code>min_contig_size</code> Minimum allowed contig size HelpSetting this to a low value will result in a large number of questionable contigs and an increase in computation time  500 <code>perc_reads_contig</code> Minimum cumulated sum of mapped read percentages of each member from a cluster group, set to 0 to disable HelpSetting this variable will remove clusters that have a low cumulated sum of mapped read percentages. This can be used to remove clusters that have a low coverage and are likely to be false positives. 5 <code>max_contig_size</code> Maximum allowed contig size 10000000 <code>max_n_perc</code> Define the maximum percentage of ambiguous bases in a contig 50 <code>skip_singleton_filtering</code> Skip the filtering of contigs that did not cluster together with other contigs HelpSetting this to true will cause the pipeline not to remove contigs that don't have similar contigs. Filtering settings can be further specified with <code>min_contig_size</code> and <code>max_n_100kbp</code>."},{"location":"parameters/#iterative-consensus-refinement","title":"Iterative consensus refinement","text":"<p>Define parameters for iterations to update de novo consensus using  reference based improvements</p> Parameter Description Default <code>skip_iterative_refinement</code> Don't realign reads to consensus sequences and redefine the consensus through (multiple) iterations <code>iterative_refinement_cycles</code> Number of iterations 2 <code>intermediate_mapper</code> Mapping tool used during iterations bwamem2 <code>intermediate_variant_caller</code> Variant caller used during iterations ivar <code>call_intermediate_variants</code> Call variants during the iterations HelpWill always be done when iterative consensus caller is bcftools <code>intermediate_consensus_caller</code> Consensus tool used for calling new consensus during iterations bcftools <code>intermediate_mapping_stats</code> Calculate summary statistics during iterations True"},{"location":"parameters/#variant-analysis","title":"Variant analysis","text":"<p>Parameters relating to the analysis of variants associated to contigs and scaffolds</p> Parameter Description Default <code>skip_variant_calling</code> Skip the analysis of variants for the external reference or contigs <code>mapper</code> Define which mapping tool needs to be used when mapping reads to reference bwamem2 <code>mapping_constraints</code> Sequence to use as a mapping reference instead of the de novo contigs or scaffolds <code>deduplicate</code> Deduplicate the reads HelpIf used with UMI's, <code>umi tools</code> will be used to group and call consensus of each individual read group. If not used with UMI's use <code>PicardsMarkDuplicates</code>.  True <code>variant_caller</code> Define the variant caller to use: 'ivar' or 'bcftools' ivar <code>consensus_caller</code> Consensus tool used for calling new consensus in final iteration ivar <code>umi_separator</code> UMI separator in fastq header. HelpIf you have used an alternative method which does not separate the read id and UMI with a \u201c_\u201d, such as bcl2fastq which uses \u201c:\u201d, you can specify the separator with the option --umi_separator=, replacing  with e.g \u201c:\u201d. : <code>mash_sketch_size</code> Specify the sketch size, the number of (non-redundant) min-hashes that are kept. HelpLarger sketches will better represent the sequence, but at the cost of larger sketch files and longer comparison times. 4000 <code>mash_sketch_kmer_size</code> Specify the kmer size for mash to create their hashes HelpLarger k-mers will provide more specificity, while smaller k-mers will provide more sensitivity. Larger genomes will also require larger k-mers to avoid k-mers that are shared by chance 15 <code>min_mapped_reads</code> Define the minimum number of mapped reads in order to continue the variant and consensus calling 200 <code>mapping_stats</code> Calculate summary statistics in final iteration True <code>ivar_header</code>"},{"location":"parameters/#consensus-qc","title":"Consensus QC","text":"<p>Apply different quality control techniques on the generated consensus genomes</p> Parameter Description Default <code>skip_consensus_qc</code> Skip the quality measurements on consensus genomes <code>skip_checkv</code> Skip the use of checkv for quality check <code>checkv_db</code> Reference database used by checkv for consensus quality control HelpIf not given, the most recent one is downloaded. <code>skip_annotation</code> Skip the annotation of the consensus constructs <code>annotation_db</code> Database used for annotation of the consensus constructs HelpThe metadata fields are stored in the fasta comment as <code>key1:\"value1\"|key2:\"value2\"|...</code> see docs/databases.md for more information. ftp://ftp.expasy.org/databases/viralzone/2020_4/virosaurus90_vertebrate-20200330.fas.gz <code>skip_prokka</code> Skip gene estimation &amp; annotation with prokka <code>prokka_db</code> Define a prokka <code>--protein</code> database for protein annotation <code>skip_quast</code> Skip the use of QUAST for quality check <code>skip_blast_qc</code> Skip the blast search of contigs to the provided reference DB <code>skip_alignment_qc</code> Skip creating an alignment of each the collapsed clusters and each iterative step True <code>mmseqs_searchtype</code> Specify the search algorithm to use for mmseqs. 0: auto 1: amino acid, 2: translated, 3: nucleotide, 4: translated nucleotide alignment HelpOnly search-type 3 supports both forward and reverse search1 - BLASTP;2 - TBLASTN;3 - BLASTN;4 - TBLASTX 4"},{"location":"parameters/#institutional-config-options","title":"Institutional config options","text":"<p>Parameters used to describe centralised config profiles. These should not be edited.</p> Parameter Description Default <code>custom_config_version</code> Git commit id for Institutional configs. master <code>custom_config_base</code> Base directory for Institutional configs. HelpIf you're running offline, Nextflow will not be able to fetch the institutional config files from the internet. If you don't need them, then this is not a problem. If you do need them, you should download the files from the repo and tell Nextflow where to find them with this parameter. https://raw.githubusercontent.com/nf-core/configs/master <code>config_profile_name</code> Institutional config name. <code>config_profile_description</code> Institutional config description. <code>config_profile_contact</code> Institutional config contact information. <code>config_profile_url</code> Institutional config URL link."},{"location":"parameters/#generic-options","title":"Generic options","text":"<p>Less common options for the pipeline, typically set in a config file.</p> Parameter Description Default <code>version</code> Display version and exit. <code>publish_dir_mode</code> Method used to save pipeline results to output directory. HelpThe Nextflow <code>publishDir</code> option specifies which intermediate files should be saved to the output directory. This option tells the pipeline what method should be used to move these files. See Nextflow docs for details. copy <code>email_on_fail</code> Email address for completion summary, only when pipeline fails. HelpAn email address to send a summary email to when the pipeline is completed - ONLY sent if the pipeline does not exit successfully. <code>plaintext_email</code> Send plain-text email instead of HTML. <code>max_multiqc_email_size</code> File size limit when attaching MultiQC reports to summary emails. 25.MB <code>monochrome_logs</code> Do not use coloured log outputs. <code>hook_url</code> Incoming hook URL for messaging service HelpIncoming hook URL for messaging service. Currently, MS Teams and Slack are supported. <code>multiqc_title</code> MultiQC report title. Printed as page header, used for filename if not otherwise specified. <code>multiqc_config</code> Custom config file to supply to MultiQC. <code>multiqc_logo</code> Custom logo file to supply to MultiQC. File name must also be set in the MultiQC config file https://github.com/Joon-Klaps/viralgenie/blob/dev/docs/images/ViralGenie-nf-core-theme.png?raw=true <code>multiqc_methods_description</code> Custom MultiQC yaml file containing HTML including a methods description. <code>clean_output_on_error</code> Delete the output directory if the pipeline fails <code>custom_table_headers</code> Custom yaml file containing the table column names selection and new names. ${projectDir}/assets/custom_table_headers.yml <code>validate_params</code> Boolean whether to validate parameters against the schema at runtime True <code>prefix</code> Prefix of all output files followed by [date][pipelineversion][runName] HelpUse '--global_prefix' to not have metadata embedded. <code>global_prefix</code> Global prefix set if you don't want metadata embedded in the prefix <code>pipelines_testdata_base_path</code> Base URL or local path to location of pipeline test dataset files https://raw.githubusercontent.com/nf-core/test-datasets/"},{"location":"quickstart/","title":"Quick Start","text":"<p>Viralgenie needs two things:</p> <ol> <li>Nextflow</li> <li>Docker, Singularity, or Conda</li> </ol> <pre><code># Install nextflow with conda\nconda install nextflow\n</code></pre> <p>Run the pipeline with a small test dataset using Docker containers: <pre><code>nextflow run Joon-Klaps/viralgenie \\\n    -profile test,docker\n</code></pre></p> <p>For a more complete guide on how to set up Nextflow, Docker, Singularity, and Conda, see the installation guide.</p>"},{"location":"quickstart/#running-viralgenie-with-your-own-samples","title":"Running viralgenie with your own samples","text":"<pre><code>nextflow run Joon-Klaps/viralgenie \\\n    -profile docker \\\n    --input my_samplesheet.csv\n</code></pre> <p>An input file contains the paths to the fastq files and the sample names. The input file can be in TSV, CSV, YAML, or JSON format.</p> TSVCSVYAMLJSON input-samplesheet.tsv<pre><code>sample  fastq_1 fastq_2\nsample1 AEG588A1_S1_L002_R1_001.fastq.gz    AEG588A1_S1_L002_R2_001.fastq.gz\nsample2 AEG588A5_S5_L003_R1_001.fastq.gz\nsample3 AEG588A3_S3_L002_R1_001.fastq.gz    AEG588A3_S3_L002_R2_001.fastq.gz\n</code></pre> input-samplesheet.csv<pre><code>sample,fastq_1,fastq_2\nsample1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\nsample2,AEG588A5_S5_L003_R1_001.fastq.gz,\nsample3,AEG588A3_S3_L002_R1_001.fastq.gz,AEG588A3_S3_L002_R2_001.fastq.gz\n</code></pre> input-samplesheet.yaml<pre><code>- sample: sample1\n  fastq1: AEG588A1_S1_L002_R1_001.fastq.gz\n  fastq2: AEG588A1_S1_L002_R2_001.fastq.gz\n- sample: sample2\n  fastq1: AEG588A5_S5_L003_R1_001.fastq.gz\n- sample: sample3\n  fastq1: AEG588A3_S3_L002_R1_001.fastq.gz\n  fastq2: AEG588A3_S3_L002_R2_001.fastq.gz\n</code></pre> samplesheet.json<pre><code>[\n    {\n        \"sample\": \"sample1\",\n        \"fastq1\": \"AEG588A1_S1_L002_R1_001.fastq.gz\",\n        \"fastq2\": \"AEG588A1_S1_L002_R2_001.fastq.gz\"\n    },\n    {\n        \"sample\": \"sample2\",\n        \"fastq1\": \"AEG588A5_S5_L003_R1_001.fastq.gz\"\n    },\n    {\n        \"sample\": \"sample3\",\n        \"fastq1\": \"AEG588A3_S3_L002_R1_001.fastq.gz\",\n        \"fastq2\": \"AEG588A3_S3_L002_R2_001.fastq.gz\"\n    }\n]\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>Try out the pipeline right now!</p> <pre><code>nextflow run Joon-Klaps/viralgenie -profile test,docker\n</code></pre> <p>Make sure you have Nextflow and a container manager (for example, Docker) installed. See the installation instructions for more info.</p> <p>Tip</p> <p>Did your analysis fail? After fixing the issue add <code>-resume</code> to the command to continue from where it left off.</p>"},{"location":"usage/#input","title":"Input","text":""},{"location":"usage/#samples","title":"Samples","text":"<p>The pipeline requires a samplesheet as input. This samplesheet should contain the name and the absolute locations of reads.</p> <pre><code>--input '[path to samplesheet file]'\n</code></pre> <p>The pipeline will auto-detect whether a sample is single- or paired-end using the information provided in the samplesheet (i.e. if the <code>fastq_2</code> column is empty, the sample is assumed to be single-end).</p> <p>An example samplesheet file consisting of both single- and paired-end data may look something like the one below.</p> TSVCSVYAMLJSON input-samplesheet.tsv<pre><code>sample  fastq_1 fastq_2\nsample1 AEG588A1_S1_L002_R1_001.fastq.gz    AEG588A1_S1_L002_R2_001.fastq.gz\nsample2 AEG588A5_S5_L003_R1_001.fastq.gz\nsample3 AEG588A3_S3_L002_R1_001.fastq.gz    AEG588A3_S3_L002_R2_001.fastq.gz\n</code></pre> input-samplesheet.csv<pre><code>sample,fastq_1,fastq_2\nsample1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\nsample2,AEG588A5_S5_L003_R1_001.fastq.gz,\nsample3,AEG588A3_S3_L002_R1_001.fastq.gz,AEG588A3_S3_L002_R2_001.fastq.gz\n</code></pre> input-samplesheet.yaml<pre><code>- sample: sample1\n  fastq1: AEG588A1_S1_L002_R1_001.fastq.gz\n  fastq2: AEG588A1_S1_L002_R2_001.fastq.gz\n- sample: sample2\n  fastq1: AEG588A5_S5_L003_R1_001.fastq.gz\n- sample: sample3\n  fastq1: AEG588A3_S3_L002_R1_001.fastq.gz\n  fastq2: AEG588A3_S3_L002_R2_001.fastq.gz\n</code></pre> samplesheet.json<pre><code>[\n    {\n        \"sample\": \"sample1\",\n        \"fastq1\": \"AEG588A1_S1_L002_R1_001.fastq.gz\",\n        \"fastq2\": \"AEG588A1_S1_L002_R2_001.fastq.gz\"\n    },\n    {\n        \"sample\": \"sample2\",\n        \"fastq1\": \"AEG588A5_S5_L003_R1_001.fastq.gz\"\n    },\n    {\n        \"sample\": \"sample3\",\n        \"fastq1\": \"AEG588A3_S3_L002_R1_001.fastq.gz\",\n        \"fastq2\": \"AEG588A3_S3_L002_R2_001.fastq.gz\"\n    }\n]\n</code></pre> Value Description <code>sample</code> Custom sample name, needs to be unique <code>fastq_1</code> Full path (not relative paths) to FastQ file for Illumina short reads 1. File has to be gzipped and have the extension \".fastq.gz\" or \".fq.gz\". <code>fastq_2</code> Full path (not relative paths) to FastQ file for Illumina short reads 2. File has to be gzipped and have the extension \".fastq.gz\" or \".fq.gz\"."},{"location":"usage/#mapping-constraints","title":"Mapping constraints","text":"<p>Viralgenie can in addition to constructing de novo consensus genomes map the sample reads to a series of references. These references are provided through the parameter <code>--mapping_constraints</code>.</p> <p>An example mapping constraint samplesheet file consisting of 5 references, may look something like the one below.</p> <p>This is for 5 references, 2 of them being a multi-fasta file, only one of the multi-fasta needs to undergo reference selection.</p> TSVCSVYAMLJSON constraints-samplesheet.tsv<pre><code>id  species segment selection   samples sequence    definition\nLassa-L-dataset LASV    L   true        LASV_L.multi.fasta  Collection of LASV sequences used for hybrid capture bait design, all publicly available sequences of the L segment clustered at 99.5% similarity\nLassa-S-dataset LASV    S   false   sample1;sample3 LASV_S.multi.fasta  Collection of LASV sequences used for hybrid capture bait design, all publicly available sequences of the S segment clustered at 99.5% similarity\nNC038709.1  HAZV    L   false   sample1;sample2 L-NC_038709.1.fasta Hazara virus isolate JC280 segment L, complete sequence.\nNC038710.1  HAZV    M   false       M-NC_038710.1.fasta Hazara virus isolate JC280 segment M, complete sequence.\nNC038711.1  HAZV    S   false       S-NC_038711.1.fasta Hazara virus isolate JC280 segment S, complete sequence.\n</code></pre> constraints-samplesheet.csv<pre><code>id,species,segment,selection,samples,sequence,definition\nLassa-L-dataset,LASV,L,true,,LASV_L.multi.fasta,\"Collection of LASV sequences used for hybrid capture bait design, all publicly available sequences of the L segment clustered at 99.5% similarity\"\nLassa-S-dataset,LASV,S,false,\"sample1;sample3\",LASV_S.multi.fasta,\"Collection of LASV sequences used for hybrid capture bait design, all publicly available sequences of the S segment clustered at 99.5% similarity\"\nNC038709.1,HAZV,L,false,\"sample1;sample2\",L-NC_038709.1.fasta,\"Hazara virus isolate JC280 segment L, complete sequence.\"\nNC038710.1,HAZV,M,false,,M-NC_038710.1.fasta,\"Hazara virus isolate JC280 segment M, complete sequence.\"\nNC038711.1,HAZV,S,false,,S-NC_038711.1.fasta,\"Hazara virus isolate JC280 segment S, complete sequence.\"\n</code></pre> constraints-samplesheet.yaml<pre><code>- id: Lassa-L-dataset\n  species: LASV\n  segment: L\n  selection: true\n  sequence: LASV_L.multi.fasta\n  definition: 'Collection of LASV sequences used for hybrid capture bait design, all publicly available sequences of the L segment clustered at 99.5% similarity'\n- id: Lassa-S-dataset\n  species: LASV\n  segment: S\n  selection: false\n  samples: sample1;sample3\n  sequence: LASV_S.multi.fasta\n  definition: 'Collection of LASV sequences used for hybrid capture bait design, all publicly available sequences of the S segment clustered at 99.5% similarity'\n- id: NC038709.1\n  species: HAZV\n  segment: L\n  selection: false\n  samples: sample1;sample2\n  sequence: L-NC_038709.1.fasta\n  definition: 'Hazara virus isolate JC280 segment L, complete sequence.'\n- id: NC038710.1\n  species: HAZV\n  segment: M\n  selection: false\n  sequence: M-NC_038710.1.fasta\n  definition: 'Hazara virus isolate JC280 segment M, complete sequence.'\n- id: NC038711.1\n  species: HAZV\n  segment: S\n  selection: false\n  sequence: S-NC_038711.1.fasta\n  definition: 'Hazara virus isolate JC280 segment S, complete sequence.'\n</code></pre> <p>Warning</p> <p>JSON format is not supported for mapping constraints samplesheet.</p> Column Description <code>id</code> Reference identifier, needs to be unique <code>species</code> [Optional] Species name of the reference <code>segment</code> [Optional] Segment name of the reference <code>selection</code> [Optional] Specify if the multi-fasta reference file needs to undergo reference selection <code>samples</code> [Optional] List of samples that need to be mapped towards the reference. If empty, map all samples. <code>sequence</code> Full path (not relative paths) to the reference sequence file. <code>definition</code> [Optional] Definition of the reference sequence file. <p>Tip</p> <ul> <li>The <code>samples</code> column is optional - if empty, all samples will be mapped towards the reference.</li> <li>Multi-fasta files can be provided and all reads will be mapped to all genomes but stats will not be reported separately in the final report.</li> </ul>"},{"location":"usage/#metadata","title":"Metadata","text":"<p>Sample metadata can be provided to the pipeline with the argument <code>--metadata</code>. This metadata will not affect the analysis in any way and is only used to annotate the final report. Any metadata can be provided as long as the first value is the <code>sample</code> value.</p> TSVCSVYAMLJSON metadata.tsv<pre><code>sample  sample_accession    secondary_sample_accession  study_accession run_alias   library_layout\nsample1 SAMN14154201    SRS6189918  PRJNA607948 vero76_Illumina.fastq   PAIRED\nsample2 SAMN14154205    SRS6189924  PRJNA607948 veroSTAT-1KO_Illumina.fastq PAIRED\n</code></pre> metadata.csv<pre><code>sample,sample_accession,secondary_sample_accession,study_accession,run_alias,library_layout\nsample1,SAMN14154201,SRS6189918,PRJNA607948,vero76_Illumina.fastq,PAIRED\nsample2,SAMN14154205,SRS6189924,PRJNA607948,veroSTAT-1KO_Illumina.fastq,PAIRED\n</code></pre> metadata.yaml<pre><code>- sample: sample1\n  sample_accession: SAMN14154201\n  secondary_sample_accession: SRS6189918\n  study_accession: PRJNA607948\n  run_alias: vero76_Illumina.fastq\n  library_layout: PAIRED\n- sample: sample2\n  sample_accession: SAMN14154205\n  secondary_sample_accession: SRS6189924\n  study_accession: PRJNA607948\n  run_alias: veroSTAT-1KO_Illumina.fastq\n  library_layout: PAIRED\n</code></pre> <p>Warning</p> <p>JSON format is not supported for metadata samplesheet.</p>"},{"location":"usage/#running-the-pipeline","title":"Running the pipeline","text":"<p>The typical command for running the pipeline is as follows:</p> <pre><code>nextflow run Joon-Klaps/viralgenie --input ./samplesheet.csv --outdir &lt;OUTDIR&gt; -profile docker\n</code></pre> <p>This will launch the pipeline with the <code>docker</code> configuration profile. See below for more information about profiles.</p> <p>Note that the pipeline will create the following files in your working directory:</p> <pre><code>work          #(1)!\n&lt;OUTDIR&gt;      #(2)!\n.nextflow_log #(3)!\n...           #(4)!\n</code></pre> <ol> <li> <p>Directory containing the nextflow working files</p> </li> <li> <p>Finished results in specified location (defined with --outdir)</p> </li> <li> <p>Log file from Nextflow</p> </li> <li> <p>Other nextflow hidden files, eg. history of pipeline runs and old logs.</p> </li> </ol> <p>If you wish to repeatedly use the same parameters for multiple runs, rather than specifying each flag in the command, you can specify these in a params file.</p> <p>Pipeline settings can be provided in a <code>yaml</code> or <code>json</code> file via <code>-params-file &lt;file&gt;</code>.</p> <p>Warning</p> <p>Do not use <code>-c &lt;file&gt;</code> to specify parameters as this will result in errors. Custom config files specified with <code>-c</code> must only be used for tuning process resource specifications, other infrastructural tweaks (such as output directories), or module arguments (args).</p> params.jsoncommand line <p>The above pipeline run specified with a params file in yaml format:</p> <pre><code>nextflow run Joon-Klaps/viralgenie -profile docker -params-file params.yaml\n</code></pre> <p><code>params.yaml</code> will contain:</p> <pre><code>{\n    \"input\": \"./samplesheet.csv\",\n    \"outdir\": \"./results/\",\n    \"host_k2_db\": \"./databases/kraken2/host\",\n    \"mapping_constraints\": \"./mapping_constraints.tsv\",\n    \"cluster_method\": \"mmseqs-linclust\"\n    ...\n}\n</code></pre> <pre><code>nextflow run Joon-Klaps/viralgenie -profile docker \\\n    --input ./samplesheet.csv \\\n    --outdir ./results/ \\\n    --host_k2_db ./databases/kraken2/host \\\n    --mapping_constraints ./mapping_constraints.tsv \\\n    --cluster_method 'mmseqs-linclust' \\\n    ...\n</code></pre> <p>You can also generate such <code>YAML</code>/<code>JSON</code> files via <code>nf-core launch</code> if <code>nf-core</code> is installed. <pre><code>nf-core launch Joon-Klaps/viralgenie\n</code></pre></p> <p>Tip</p> <p>Use <code>nf-core launch</code> if it is the first time running the pipeline to explore all its features and options in an accessible way.</p>"},{"location":"usage/#updating-the-pipeline","title":"Updating the pipeline","text":"<pre><code>nextflow pull Joon-Klaps/viralgenie\n</code></pre> <p>When you run the above command, Nextflow automatically pulls the pipeline code from GitHub and stores it as a cached version. When running the pipeline after this, it will always use the cached version if available - even if the pipeline has been updated since. To make sure that you're running the latest version of the pipeline, make sure that you regularly update the cached version of the pipeline:</p> <pre><code>nextflow pull Joon-Klaps/viralgenie\n</code></pre>"},{"location":"usage/#reproducibility","title":"Reproducibility","text":"<p>It is a good idea to specify a pipeline version when running the pipeline on your data. This ensures that a specific version of the pipeline code and software are used when you run your pipeline. If you keep using the same tag, you'll be running the same version of the pipeline, even if there have been changes to the code since.</p> <p>First, go to the Joon-Klaps/viralgenie releases page and find the latest pipeline version - numeric only (eg. <code>1.3.1</code>). Then specify this when running the pipeline with <code>-r</code> (one hyphen) - eg. <code>-r 1.3.1</code>. Of course, you can switch to another version by changing the number after the <code>-r</code> flag.</p> <p>This version number will be logged in reports when you run the pipeline, so that you'll know what you used when you look back in the future. For example, at the bottom of the MultiQC reports.</p> <p>To further assist in reproducibility, you can use share and re-use parameter files to repeat pipeline runs with the same settings without having to write out a command with every single parameter.</p> <p>Tip</p> <p>If you wish to share such profile (such as upload as supplementary material for academic publications), make sure to NOT include cluster specific paths to files, nor institutional specific profiles.</p>"},{"location":"usage/#core-nextflow-arguments","title":"Core Nextflow arguments","text":"<p>Note</p> <p>These options are part of Nextflow and use a single hyphen (pipeline parameters use a double-hyphen).</p>"},{"location":"usage/#the-profile-parameter","title":"The <code>-profile</code> parameter","text":"<p>Use this parameter to choose a configuration profile. Profiles can give configuration presets for different compute environments.</p> <p>Several generic profiles are bundled with the pipeline which instruct the pipeline to use software packaged using different methods (Docker, Singularity, Podman, Shifter, Charliecloud, Apptainer, Conda) - see below.</p> <p>Info</p> <p>We highly recommend the use of Docker or Singularity containers for full pipeline reproducibility, however when this is not possible, Conda is also supported.</p> <p>The pipeline also dynamically loads configurations from https://github.com/nf-core/configs when it runs, making multiple config profiles for various institutional clusters available at run time. For more information and to see if your system is available in these configs please see the nf-core/configs documentation.</p> <p>Note that multiple profiles can be loaded, for example: <code>-profile test,docker</code> - the order of arguments is important! They are loaded in sequence, so later profiles can overwrite earlier profiles.</p> <p>If <code>-profile</code> is not specified, the pipeline will run locally and expect all software to be installed and available on the <code>PATH</code>. This is not recommended, since it can lead to different results on different machines dependent on the computer environment.</p> <ul> <li><code>test</code><ul> <li>A profile with a complete configuration for automated testing</li> <li>Includes links to test data so needs no other parameters</li> </ul> </li> <li><code>docker</code><ul> <li>A generic configuration profile to be used with Docker</li> </ul> </li> <li><code>singularity</code><ul> <li>A generic configuration profile to be used with Singularity</li> </ul> </li> <li><code>podman</code><ul> <li>A generic configuration profile to be used with Podman</li> </ul> </li> <li><code>shifter</code><ul> <li>A generic configuration profile to be used with Shifter</li> </ul> </li> <li><code>charliecloud</code><ul> <li>A generic configuration profile to be used with Charliecloud</li> </ul> </li> <li><code>apptainer</code><ul> <li>A generic configuration profile to be used with Apptainer</li> </ul> </li> <li><code>conda</code><ul> <li>A generic configuration profile to be used with Conda. Please only use Conda as a last resort i.e. when it's not possible to run the pipeline with Docker, Singularity, Podman, Shifter, Charliecloud, or Apptainer.</li> </ul> </li> </ul>"},{"location":"usage/#-resume","title":"<code>-resume</code>","text":"<p>Specify this when restarting a pipeline. Nextflow will use cached results from any pipeline steps where the inputs are the same, continuing from where it got to previously. For input to be considered the same, not only the names must be identical but the files' contents as well. For more info about this parameter, see this blog post.</p> <p>You can also supply a run name to resume a specific run: <code>-resume [run-name]</code>. Use the <code>nextflow log</code> command to show previous run names.</p>"},{"location":"usage/#-c","title":"<code>-c</code>","text":"<p>Specify the path to a specific config file (this is a core Nextflow command). See the nf-core website documentation for more information.</p>"},{"location":"usage/#custom-configuration","title":"Custom configuration","text":""},{"location":"usage/#resource-requests","title":"Resource requests","text":"<p>Whilst the default requirements set within the pipeline will hopefully work for most people and with most input data, you may find that you want to customize the compute resources that the pipeline requests. Each step in the pipeline has a default set of requirements for number of CPUs, memory and time. For most of the steps in the pipeline, if the job exits with any of the error codes specified here it will automatically be resubmitted with higher requests (2 x original, then 3 x original). If it still fails after the third attempt then the pipeline execution is stopped.</p> <p>To change the resource requests, please see the max resources and tuning workflow resources section of the nf-core website.</p>"},{"location":"usage/#custom-containers","title":"Custom Containers","text":"<p>In some cases you may wish to change which container or conda environment a step of the pipeline uses for a particular tool. By default Viralgenie uses containers and software from the biocontainers or bioconda projects. However in some cases the pipeline specified version maybe out of date.</p> <p>To use a different container from the default container or conda environment specified in a pipeline, please see the updating tool versions section of the nf-core website.</p>"},{"location":"usage/#custom-tool-arguments","title":"Custom Tool Arguments","text":"<p>A pipeline might not always support every possible argument or option of a particular tool used in pipeline. Fortunately, Viralgenie provides some freedom to users to insert additional parameters that the pipeline does not include by default.</p> <p>To learn how to provide additional arguments to a particular tool of the pipeline, please see the customising tool arguments section of the nf-core website.</p>"},{"location":"usage/#nf-coreconfigs","title":"nf-core/configs","text":"<p>In most cases, you will only need to create a custom config as a one-off but if you and others within your organisation are likely to be running Viralgenie regularly and need to use the same settings regularly it may be a good idea to request that your custom config file is uploaded to the <code>nf-core/configs</code> git repository. Before you do this please can you test that the config file works with your pipeline of choice using the <code>-c</code> parameter. You can then create a pull request to the <code>nf-core/configs</code> repository with the addition of your config file, associated documentation file (see examples in <code>nf-core/configs/docs</code>), and amending <code>nfcore_custom.config</code> to include your custom profile.</p> <p>See the main Nextflow documentation for more information about creating your own configuration files.</p> <p>If you have any questions or issues please send us a message on Slack on the <code>#configs</code> channel.</p>"},{"location":"usage/#running-in-the-background","title":"Running in the background","text":"<p>Nextflow handles job submissions and supervises the running jobs. The Nextflow process must run until the pipeline is finished.</p> <p>The Nextflow <code>-bg</code> flag launches Nextflow in the background, detached from your terminal so that the workflow does not stop if you log out of your session. The logs are saved to a file.</p> <p>Alternatively, you can use <code>screen</code> / <code>tmux</code> or similar tool to create a detached session which you can log back into at a later time. Some HPC setups also allow you to run nextflow within a cluster job submitted your job scheduler (from where it submits more jobs).</p>"},{"location":"usage/#nextflow-memory-requirements","title":"Nextflow memory requirements","text":"<p>In some cases, the Nextflow Java virtual machines can start to request a large amount of memory. We recommend adding the following line to your environment to limit this (typically in <code>~/.bashrc</code> or <code>~./bash_profile</code>):</p> <pre><code>NXF_OPTS='-Xms1g -Xmx4g'\n</code></pre>"},{"location":"customisation/","title":"Customisation","text":"<p>The viralgenie pipeline is highly customisable, allowing you to tailor the analysis to your specific needs. This section provides information on how to customise the pipeline, including configuration options and database customisation.</p>"},{"location":"customisation/#custom-databases","title":"Custom Databases","text":"<p>Viralgenie uses a variety of databases to analyse reads, contigs, and consensus constructs. While the default databases are sufficient for most cases, you can create custom databases if needed. This section provides guidance on how to create and use custom databases for different tools used in the pipeline. For more details, see the databases guide.</p>"},{"location":"customisation/#custom-tool-configuration","title":"Custom Tool Configuration","text":"<p>Viralgenie allows you to modify the arguments of the tools used in the pipeline by providing a custom configuration file. This can be done by copying a segment from the <code>modules.config</code> file and modifying the arguments as needed. For more details, see the configuration guide.</p>"},{"location":"customisation/configuration/","title":"Custom configuration of modules","text":"<p>Within viralgenie, all modules (tools, e.g., <code>FASTP</code>, <code>FASTQC</code>) can be run with specific arguments. The pipeline has a default configuration but this can be overwritten by supplying a custom configuration file. This file can be provided to viralgenie using the <code>-c</code> Nextflow option.</p> <p>To see which specific arguments or variables are used for a module or tool, have a look at the <code>modules.config</code> file. Here the arguments of a module are specified as follows:</p> <pre><code>withName: IVAR_CONSENSUS {\n    ext.args = [\n        '-t 0.75',          // frequency to call consensus: 0.75 just the majority rule\n        '-q 20',            // minimum quality score of base\n        '-m 10',            // minimum depth to call consensus\n        '-n N'              // Character to print in regions with less coverage\n    ].join(' ').trim()\n    ext.args2 = [\n        '--count-orphans',  // Do not skip anomalous read pairs in variant calling.\n        '--max-depth 0',    // Maximum number of reads to start to consider at each location, 0 means no limit\n        '--min-BQ 20',      // Minimum base quality\n        '--no-BAQ',         // Disable probabilistic realignment for the computation of base alignment quality\n        '-aa',              // Output absolutely all positions, including unused reference sequences\n    ].join(' ').trim()\n    ...\n}\n</code></pre> <p>In this example, the <code>IVAR_CONSENSUS</code> module is configured with the arguments <code>-q 20 -m 10</code> for the tool <code>ivar consensus</code> and <code>--ignore-overlaps --count-orphans --max-depth 0 --no-BAQ --min-BQ 0</code> for <code>samtools mpileup</code> as iVar uses the output of <code>samtools mpileup</code> directly.</p> <p>Tip</p> <p>The <code>ext.args</code> and <code>ext.args2</code> are used to specify the arguments for the tool. If unsure which tools use which arguments (<code>ivar:ext.args</code> and <code>samtools:ext.args2</code>), have a look at the nextflow module file directly! For example, at <code>modules/nf-core/ivar/consensus.nf</code>, \"\\(args\" and \"\\)args2\" are used to specify the arguments for the tools: <pre><code>\"\"\"\nsamtools \\\\\n    mpileup \\\\\n    --reference $fasta \\\\\n    $args2 \\\\               // can be modified with ext.args2\n    $bam \\\\\n    $mpileup \\\\\n    | ivar \\\\\n        consensus \\\\\n        $args \\\\            // can be modified with ext.args\n        -p $prefix\n...\n\"\"\"\n</code></pre></p> <p>In case we do want to modify the arguments of a module, we can do so by providing a custom configuration file. The easiest way to do this would be to copy a segment from the modules.config and modify the arguments. This way, none of the other configurations will get lost or modified. For example, setting the minimum depth to call consensus to 5 and the minimum quality score of base to 30 for the <code>IVAR_CONSENSUS</code> module: custom.config<pre><code>process {\n    withName: IVAR_CONSENSUS {\n        ext.args = [\n            '-t 0.75',\n            '-q 30',            // changed\n            '-m 5',             // changed\n            '-n N'\n        ].join(' ').trim()\n        ext.args2 = [\n            '--count-orphans',\n            '--max-depth 0',\n            '--min-BQ 30',      // changed\n            '--no-BAQ',\n            '-aa',\n        ].join(' ').trim()\n        ...\n    }\n}\n</code></pre></p> <p>Warning</p> <p>Make sure you include the <code>process{}</code> section.</p> <p>Next, supply the file to viralgenie using the <code>-c</code> Nextflow option: <pre><code>nextflow run Joon-Klaps/viralgenie \\\n    -profile docker \\\n    -c custom.config \\\n    --input samplesheet.csv ...\n</code></pre></p> <p>Tip</p> <p>This guide not entirely clear? Also have a look at the nf-core guide for customizing tool arguments.</p>"},{"location":"customisation/databases/","title":"Databases","text":""},{"location":"customisation/databases/#introduction","title":"Introduction","text":"<p>Viralgenie uses a multitude of databases in order to analyze reads, contigs, and consensus constructs. The default databases will be sufficient in most cases but there are always exceptions. This document will guide you towards the right documentation location for creating your custom databases.</p> <p>Tip</p> <p>Keep an eye out for nf-core createtaxdb as it can be used for the customization of the main databases but the pipeline is still under development.</p>"},{"location":"customisation/databases/#reference-pool","title":"Reference pool","text":"<p>The reference pool dataset is used to identify potential references for scaffolding. It's a fasta file that will be used to make a blast database within the pipeline. The default database is the clustered Reference Viral DataBase (C-RVDB) a database that was built for enhancing virus detection using high-throughput/next-generation sequencing (HTS/NGS) technologies. An alternative reference pool is the Virosaurus database which is a manually curated database of viral genomes.</p> <p>Any nucleotide fasta file will do. Specify it with the parameter <code>--reference_pool</code>.</p>"},{"location":"customisation/databases/#kaiju","title":"Kaiju","text":"<p>The Kaiju database will be used to classify the reads and intermediate contigs in taxonomic groups. The default database is the RVDB-prot pre-built database from Kaiju.</p> <p>A number of Kaiju pre-built indexes for reference datasets are maintained by the developers of Kaiju and made available on the Kaiju website. To build a Kaiju database, you need three components: a FASTA file with the protein sequences, the NCBI taxonomy dump files, and you need to define the uppercase characters of the standard 20 amino acids you wish to include.</p> <p>Warning</p> <p>The headers of the protein fasta file must be numeric NCBI taxon identifiers of the protein sequences.</p> <p>To download the NCBI taxonomy files, please run the following commands:</p> <pre><code>wget https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/new_taxdump/new_taxdump.zip\nunzip new_taxdump.zip\n</code></pre> <p>To build the database, run the following command (the contents of taxdump must be in the same location where you run the command):</p> <pre><code>kaiju-mkbwt -a ACDEFGHIKLMNPQRSTVWY -o proteins proteins.faa\nkaiju-mkfmi proteins\n</code></pre> <p>Tip</p> <p>You can speed up database construction by supplying the threads parameter (<code>-t</code>).</p> Expected files in database directory <ul> <li><code>kaiju</code><ul> <li><code>kaiju_db_*.fmi</code></li> <li><code>nodes.dmp</code></li> <li><code>names.dmp</code></li> </ul> </li> </ul> <p>For the Kaiju database construction documentation, see here.</p>"},{"location":"customisation/databases/#kraken2-databases","title":"Kraken2 databases","text":"<p>The Kraken2 database will be used to classify the reads and intermediate contigs in taxonomic groups.</p> <p>A number of database indexes have already been generated and maintained by @BenLangmead Lab, see here. These databases can directly be used to run the workflow with Kraken2 as well as Bracken.</p> <p>In case the databases above do not contain your desired libraries, you can build a custom Kraken2 database. This requires two components: a taxonomy (consisting of <code>names.dmp</code>, <code>nodes.dmp</code>, and <code>*accession2taxid</code>) files, and the FASTA files you wish to include.</p> <p>To pull the NCBI taxonomy, you can run the following:</p> <pre><code>kraken2-build --download-taxonomy --db &lt;YOUR_DB_NAME&gt;\n</code></pre> <p>You can then add your FASTA files with the following build command.</p> <pre><code>kraken2-build --add-to-library *.fna --db &lt;YOUR_DB_NAME&gt;\n</code></pre> <p>You can repeat this step multiple times to iteratively add more genomes prior to building.</p> <p>Once all genomes are added to the library, you can build the database (and optionally clean it up):</p> <pre><code>kraken2-build --build --db &lt;YOUR_DB_NAME&gt;\nkraken2-build --clean --db &lt;YOUR_DB_NAME&gt;\n</code></pre> <p>You can then add the <code>&lt;YOUR_DB_NAME&gt;/</code> path to your nf-core/taxprofiler database input sheet.</p> Expected files in database directory <ul> <li><code>kraken2</code><ul> <li><code>opts.k2d</code></li> <li><code>hash.k2d</code></li> <li><code>taxo.k2d</code></li> </ul> </li> </ul> <p>You can follow the Kraken2 tutorial for a more detailed description.</p>"},{"location":"customisation/databases/#host-read-removal","title":"Host read removal","text":"<p>Viralgenie uses Kraken2 to remove contaminated reads.</p> <p>Info</p> <p>The reason why we use Kraken2 for host removal over regular read mappers is nicely explained in the following papers:</p> <ul> <li>The human \u201ccontaminome\u201d: bacterial, viral, and computational contamination in whole genome sequences from 1000 families</li> <li>Reconstruction of the personal information from human genome reads in gut metagenome sequencing data</li> </ul> <p>The contamination database is likely the largest database. The default databases are made small explicitly to save storage for end users but are not optimal. I would recommend creating a database consisting of the libraries <code>human, archaea, bacteria</code> which will be more than 200GB in size. Additionally, it's good practice to include DNA &amp; RNA of the host of origin if known (i.e. mice, ticks, mosquito, ... ). Add it as described above.</p> <p>Set it with the variable <code>--host_k2_db</code></p>"},{"location":"customisation/databases/#viral-diversity-with-kraken2","title":"Viral Diversity with Kraken2","text":"<p>The metagenomic diversity estimated with Kraken2 is based on the viral refseq database which can cut short if you expect the species within your sample to have a large amount of diversity eg below 80% ANI (quasi-species). To resolve this it's better to create a database that contains a wider species diversity than only one genome per species. Databases that have this wider diversity are Virosaurus or the RVDB which can increase the accuracy of Kraken2. Add it as described above.</p> <p>Set it with the variable <code>--kraken2_db</code></p>"},{"location":"customisation/databases/#annotation-sequences","title":"Annotation sequences","text":"<p>Identifying the species and the segment of the final genome constructs is done based on a tblastx search (with MMSEQS) to an annotated sequencing dataset. This dataset is by default the Virosaurus as it contains a good representation of the viral genomes and is annotated.</p> <p>This annotation database can be specified using <code>--annotation_db</code></p>"},{"location":"customisation/databases/#creating-a-custom-annotation-dataset-with-bv-brc","title":"Creating a custom annotation dataset with BV-BRC","text":"<p>In case Virosaurus does not suffice your needs, a custom annotation dataset can be made. Creating a custom annotation dataset can easily be done as long as the annotation data is in the fasta header using this format: <code>(key)=(value)</code> or <code>(key):(value)</code>. For example, the following fasta headers are both valid:</p> <pre><code>&gt;754189.6 species=\"Ungulate tetraparvovirus 3\"|segment=\"nan\"|host_common_name=\"Pig\"|genbank_accessions=\"NC_038883\"|taxon_id=\"754189\"\n&gt;NC_001731; usual name=Molluscum contagiosum virus; clinical level=SPECIES; clinical typing=unknown; species=Molluscum contagiosum virus; taxid=10279; acronym=MOCV; nucleic acid=DNA; circular=N; segment=N/A; host=Human,Vertebrate;\n</code></pre> <p>An easy-to-use public database with a lot of metadata is BV-BRC. Sequences can be extracted using their CLI-tool and linked to their metadata</p> <p>Here we select all viral genomes that are not lab reassortments and are reference genomes and add metadata attributes to the output.</p> <p>This is an example, in case you need to have a more elaborate dataset than Virosaurus, be more inclusive towards your taxa of interest and include more metadata attributes.</p> <pre><code># download annotation metadata +/- 5s\np3-all-genomes --eq superkingdom,Viruses --eq reference_genome,Reference --ne host_common_name,'Lab reassortment' --attr genome_id,species,segment,genome_name,genome_length,host_common_name,genbank_accessions,taxon_id   &gt; all-virus-anno.txt\n# download genome data, done separately as it takes much longer to query +/- 1 hour\np3-all-genomes --eq superkingdom,Viruses --eq reference_genome,Reference --ne host_common_name,'Lab reassortment' | p3-get-genome-contigs --attr sequence &gt; all-virus.fasta\n</code></pre> <p>Tip</p> <p>Any attribute can be downloaded and will be added to the final report if the formatting remains the same. For a complete list of attributes see <code>p3-all-genomes --fields</code> or read their manual</p> <p>Next, the metadata and the genomic data are combined into a single fasta file where the metadata fields are stored in the fasta comment as <code>key1=\"value1\"|key2=\"value2\"|...</code> using the following python code.</p> <pre><code>import pandas as pd\nimport re\n\n# read in sequences with all columns as strings\nsequences = pd.read_csv(\"refseq-virus.fasta\", index_col=0, sep=\"\\t\", dtype=str)\ndata = pd.read_csv(\"refseq-virus-anno.txt\", index_col=0, sep=\"\\t\", dtype=str)\n\n# merge the df's\ndf = sequences.join(data)\n# remove 'genome' from the name\ndf.columns = df.columns.str.replace(\"genome.\", \"\")\n\n# create fasta header\ndef create_fasta_header(row):\n    annotations = \";\".join(\n        [\n            f'{column}=\"{value}\"'\n            for column, value in row.items()\n            if column != \"contig.sequence\"\n        ]\n    )\n    return f\"{annotations}\\n\"\n\n\ndf[\"fasta_header\"] = df.apply(create_fasta_header, axis=1)\n\ndf[\"fasta_entry\"] = (\n    \"&gt;\" + df.index.astype(str) + \" \" + df[\"fasta_header\"] + df[\"contig.sequence\"]\n)\nwith open(\"bv-brc-refvirus-anno.fasta\", \"w\") as f:\n    for entry in df[\"fasta_entry\"]:\n        f.write(entry + \"\\n\")\n</code></pre> Expected files in database directory <ul> <li><code>refseq-virus.fasta</code></li> <li><code>refseq-virus-anno.txt</code></li> <li><code>bv-brc-refvirus-anno.fasta</code></li> </ul>"},{"location":"template/parameter_tip/","title":"Parameter tip","text":"<p>Need Something more interactive?</p> <p>Use <code>nf-core pipelines launch</code> to interactivly set your parameters: <pre><code>nf-core pipelines launch Joon-Klaps/viralgenie\n</code></pre></p>"},{"location":"workflow/","title":"Workflow overview","text":"<p>Viralgenie takes in a set of reads and performs 5 major analyses, each of them are explained in more detail in the following sections:</p> <ol> <li>Preprocessing</li> <li>Metagenomic diversity</li> <li>Assembly &amp; Polishing</li> <li>Variant analysis &amp; iterative refinement</li> <li>Consensus evaluation</li> </ol> <p>By default all analyses are run.</p> <p>Skipping steps</p> <p>All steps can be skipped and the pipeline can be run with only the desired steps. This can be done with the <code>--skip_preprocessing</code>, <code>--skip_read_classification</code>, <code>--skip_assembly</code>, <code>--skip_polishing</code>, <code>--skip_variant_analysis</code>, <code>--skip_iterative_refinement</code>, <code>--skip_consensus_qc</code> flags.</p>"},{"location":"workflow/#subway-map","title":"Subway map","text":"<ol> <li>Read QC (<code>FastQC</code>)</li> <li>Performs optional read pre-processing<ul> <li>Adapter trimming(<code>fastp</code>, <code>Trimmomatic</code>)</li> <li>Read UMI deduplication (<code>HUMID</code>)</li> <li>Low complexity and quality filtering (<code>bbduk</code>, <code>prinseq++</code>)</li> <li>Host-read removal (<code>BowTie2</code>)</li> </ul> </li> <li>Metagenomic diversity mapping<ul> <li>Performs taxonomic classification and/or profiling using one or more of:<ul> <li><code>Kraken2</code></li> <li><code>Bracken</code>(optional)</li> <li><code>Kaiju</code></li> </ul> </li> <li>Plotting Kraken2 and Kaiju (<code>Krona</code>)</li> </ul> </li> <li>Denovo assembly (<code>SPAdes</code>, <code>TRINITY</code>, <code>megahit</code>), combine contigs.</li> <li>[Optional] extend the contigs with sspace_basic and filter with <code>prinseq++</code></li> <li>[Optional] Map reads to contigs for coverage estimation (<code>BowTie2</code>,<code>BWAmem2</code> and <code>BWA</code>)</li> <li>Contig reference idententification (<code>blastn</code>)<ul> <li>Identify top 5 blast hits</li> <li>Merge blast hit and all contigs of a sample</li> </ul> </li> <li>[Optional] Precluster contigs based on taxonomy<ul> <li>Identify taxonomy <code>Kraken2</code> and\\or <code>Kaiju</code></li> <li>Resolve potential inconsistencies in taxonomy &amp; taxon filtering | simplification <code>bin/extract_precluster.py</code></li> </ul> </li> <li>Cluster contigs (or every taxonomic bin) of samples, options are:<ul> <li><code>cdhitest</code></li> <li><code>vsearch</code></li> <li><code>mmseqs-linclust</code></li> <li><code>mmseqs-cluster</code></li> <li><code>vRhyme</code></li> <li><code>Mash</code></li> </ul> </li> <li>[Optional] Remove clusters with low read coverage. <code>bin/extract_clusters.py</code></li> <li>Scaffolding of contigs to centroid (<code>Minimap2</code>, <code>iVar-consensus</code>)</li> <li>[Optional] Annotate 0-depth regions with external reference <code>bin/lowcov_to_reference.py</code>.</li> <li>[Optional] Select best reference from <code>--mapping_constraints</code>:<ul> <li><code>Mash sketch</code></li> <li><code>Mash screen</code></li> </ul> </li> <li>Mapping filtered reads to supercontig and mapping constraints(<code>BowTie2</code>,<code>BWAmem2</code> and <code>BWA</code>)</li> <li>[Optional] Deduplicate reads (<code>Picard</code> or if UMI's are used <code>UMI-tools</code>)</li> <li>Variant calling and filtering (<code>BCFTools</code>,<code>iVar</code>)</li> <li>Create consensus genome (<code>BCFTools</code>,<code>iVar</code>)</li> </ol>"},{"location":"workflow/assembly_polishing/","title":"Assembly &amp; polishing","text":"<p>Viralgenie offers an elaborate workflow for the assembly and polishing of viral genomes:</p> <ol> <li>Assembly: combining the results of multiple assemblers.</li> <li>Extension: extending contigs using paired-end reads.</li> <li>Coverage calculation: mapping reads back to the contigs to determine coverage.</li> <li>Reference Matching: comparing contigs to a reference sequence pool.</li> <li>Taxonomy guided Clustering: clustering contigs based on taxonomy and nucleotide similarity.<ul> <li>Pre-clustering: separating contigs based on identified taxonomy-id.</li> <li>Actual clustering: clustering contigs based on nucleotide similarity.</li> </ul> </li> <li>Scaffolding: scaffolding the contigs to the centroid of each bin.</li> <li>Annotation with Reference: annotating regions with 0-depth coverage with the reference sequence.</li> </ol> <p></p> <p>The overall workflow of creating reference assisted assemblies can be skipped with the argument <code>--skip_assembly</code>. See the parameters assembly section for all relevant arguments to control the assembly steps.</p> <p>The overall refinement of contigs can be skipped with the argument <code>--skip_polishing</code>. See the parameters polishing section for all relevant arguments to control the polishing steps.</p> <p>The consensus genome of all clusters are then sent to the variant analysis &amp; iterative refinement step.</p>"},{"location":"workflow/assembly_polishing/#1-de-novo-assembly","title":"1. De-novo Assembly","text":"<p>Three assemblers are used, SPAdes, Megahit, and Trinity. The resulting contigs of all specified assemblers, are combined and processed further together.</p> <p>Modify the spades mode with <code>--spades_mode [default: rnaviral]</code> and supply specific params with <code>--spades_yml</code> or a hmm model with <code>--spades_hmm</code>.</p> <p>Specify the assemblers to use with the <code>--assemblers</code> parameter where the assemblers are separated with a ','. The default is <code>spades,megahit,trinity</code>.</p> <p>Low complexity contigs can be filtered out using prinseq++ with the <code>--skip_contig_prinseq false</code> parameter. Complexity filtering is primarily a run-time optimisation step. Low-complexity sequences are defined as having commonly found stretches of nucleotides with limited information content (e.g. the dinucleotide repeat CACACACACA). Such sequences can produce a large number of high-scoring but biologically insignificant results in database searches. Removing these reads therefore saves computational time and resources.</p>"},{"location":"workflow/assembly_polishing/#2-extension","title":"2. Extension","text":"<p>Contigs can be extended using SSPACE Basic with the <code>--skip_sspace_basic false</code> parameter. SSPACE is a tool for scaffolding contigs using paired-end reads. It is modified from SSAKE assembler and has the feature of extending contigs using reads that are unmappable in the contig assembly step. To maximize its efficiency, consider specifying the arguments <code>--read_distance</code>, <code>--read_distance_sd</code>, and <code>--read_orientation</code>.  For more information on these arguments, see the parameters assembly section.</p> <p>The extension of contigs is run by default, to skip this step, use <code>--skip_sspace_basic</code>.</p>"},{"location":"workflow/assembly_polishing/#3-coverage-calculation","title":"3. Coverage calculation","text":"<p>Processed reads are mapped back against the contigs to determine the number of reads mapping towards each contig. This is done with <code>BowTie2</code>,<code>BWAmem2</code> or <code>BWA</code>. This step is used to remove contig clusters that have little to no coverage downstream.</p> <p>Specify the mapper to use with the <code>--mapper</code> parameter. The default is <code>BWAmem2</code>. To skip contig filtering specify <code>--perc_reads_contig 0</code>.</p>"},{"location":"workflow/assembly_polishing/#4-reference-matching","title":"4. Reference Matching","text":"<p>The newly assembled contigs are compared to a reference sequence pool (<code>--reference_pool</code>) using a BLASTn search. This process not only helps annotate the contigs but also assists in linking together sets of contigs that are distant within a single genome. Essentially, it aids in identifying contigs belonging to the same genomic segment and choosing the right reference for scaffolding purposes.</p> <p>The top 5 hits for each contig are combined with the de novo contigs and sent to the clustering step.</p> <p>The reference pool can be specified with the <code>--reference_pool</code> parameter. The default is the latest clustered Reference Viral DataBase (RVDB).</p>"},{"location":"workflow/assembly_polishing/#5-taxonomy-guided-clustering","title":"5. Taxonomy guided Clustering","text":"<p>The clustering workflow of contigs consists of 2 steps, the pre-clustering using taxonomy and actual clustering on nucleotide similarity. The taxonomy guided clustering is used to separate contigs based on taxonomy and nucleotide similarity.</p> <pre><code>graph LR;\n    A[Contigs] --&gt; B[\"`**Pre-clustering**`\"];\n    B --&gt; C[\"`**Actual clustering**`\"];</code></pre>"},{"location":"workflow/assembly_polishing/#51-pre-clustering-using-taxonomy","title":"5.1 Pre-clustering using taxonomy","text":"<p>The contigs along with their references have their taxonomy assigned using Kraken2 and Kaiju.</p> <p>The default databases are the same ones used for read classification: - Kraken2: viral refseq database, <code>--kraken2_db</code> - Kaiju: clustered RVDB, <code>--kaiju_db</code></p> <p>As Kaiju and Kraken2 can have different taxonomic assignments, an additional step is performed to resolve potential inconsistencies in taxonomy and to identify the taxonomy of the contigs. This is done with a custom script that is based on <code>KrakenTools extract_kraken_reads.py</code> and <code>kaiju-Merge-Outputs</code>.</p> <pre><code>graph LR;\n    A[Contigs] --&gt; B[\"`**Kraken2**`\"];\n    A --&gt; C[\"`**Kaiju**`\"];\n    B --&gt; D[Taxon merge resolving];\n    C --&gt; D;\n    D --&gt; E[\"Taxon filtering\"];\n    E --&gt; F[\"Taxon simplification\"];</code></pre> <p>Having complex metagenomic samples?</p> <p>The pre-clustering step can be used to simplify the taxonomy of the contigs, let NCBI's taxonomy browser help you identify taxon-id's for simplification. The simplification can be done in several ways:</p> <ul> <li>Make sure your contamination database is up to date and removes the relevant taxa.</li> <li>Exclude unclassified contigs with <code>--keep_unclassified false</code> parameter.</li> <li>Simplify the taxonomy of the contigs to a higher rank using <code>--precluster_simplify_taxa</code> parameter (1).</li> <li>Specify the taxa to include or exclude with <code>--precluster_include_children</code>(2), <code>--precluster_include_parents</code>(3), <code>--precluster_exclude_children</code>, <code>--precluster_exclude_parents</code>, <code>--precluster_exclude_taxa</code> parameters.</li> </ul> <p>Warning</p> <p>Providing lists to nextflow is done by encapsulating values with <code>\"</code> and separating them with a space. For example: <code>--precluster_exclude_taxa \"taxon1 taxon2 taxon3\"</code>.</p> <ol> <li> <p>Options here are 'species', 'genus', 'family', 'order', 'class', 'phylum', 'kingdom' or 'superkingdom'.</p> </li> <li> <p><code>--precluster_include_children</code> \"genus1\" :</p> <p><pre><code>graph TD;\n    A[family] -.- B[\"genus1 (included)\"];\n    A -.- C[genus2];\n    B --- D[species1];\n    B --- E[species2];\n    C -.- F[species3];</code></pre> Dotted lines represent exclusion of taxa.</p> </li> <li> <p><code>--precluster_include_parents</code> \"species3\" :</p> <p><pre><code>graph TD;\n    A[\"family (included)\"] -.- B[\"genus1\"]\n    A --- C[genus2]\n    B -.- D[species1]\n    B -.- E[species2]\n    C --- F[species3]</code></pre> Dotted lines represent exclusion of taxa.</p> </li> </ol> <p>The pre-clustering step will be run by default but can be skipped with the argument <code>--skip_preclustering</code>. Specify which classifier to use with <code>--precluster_classifiers</code> parameter. The default is <code>kaiju,kraken2</code>. Contig taxon filtering is still enabled despite not having to solve for inconsistencies if only Kaiju or Kraken2 is run.</p>"},{"location":"workflow/assembly_polishing/#52-actual-clustering-on-nucleotide-similarity","title":"5.2 Actual clustering on nucleotide similarity","text":"<p>The clustering is performed with one of the following tools:</p> <ul> <li><code>cdhitest</code></li> <li><code>vsearch</code></li> <li><code>mmseqs-linclust</code></li> <li><code>mmseqs-cluster</code></li> <li><code>vRhyme</code></li> <li><code>mash</code></li> </ul> <p>These methods all come with their own advantages and disadvantages. For example, cdhitest is very fast but cannot be used for large viruses &gt;10Mb and similarity threshold cannot go below 80% which is not preferable for highly diverse RNA viruses. Vsearch is slower but accurate. Mmseqs-linclust is the fastest but tends to create a large amount of bins. Mmseqs-cluster is slower but can handle larger datasets and is more accurate. vRhyme is a new method that is still under development but has shown promising results but can sometimes not output any bins when segments are small. Mash is a very fast comparison method is linked with a custom script that identifies communities within a network.</p> <p>Tip</p> <p>When pre-clustering is performed, it is recommended to set a lower identity_threshold (60-70% ANI) as the new goal becomes to separate genome segments within the same bin.</p> <p>The clustering method can be specified with the <code>--clustering_method</code> parameter. The default is <code>cdhitest</code>.</p> <p>The network clustering method for <code>mash</code> can be specified with the <code>--network_clustering</code> parameter. The default is <code>connected_components</code>, alternative is <code>leiden</code>.</p> <p>The similarity threshold can be specified with the <code>--similarity_threshold</code> parameter. The default is <code>0.85</code>.</p>"},{"location":"workflow/assembly_polishing/#6-coverage-filtering","title":"6. Coverage filtering","text":"<p>The coverage of the contigs is calculated using the same method as in the coverage calculation step. A cumulative sum is taken across the contigs from every assembler. If these cumulative sums are above the specified <code>--perc_reads_contig</code> parameter, the contig is kept. If all cumulative sums are below the specified parameter, the contig is removed.</p> <p>Show me an example how it works</p> <p>If the <code>--perc_reads_contig</code> is set to <code>5</code>, the cumulative sum of the contigs from every assembler is calculated. For example:</p> <ul> <li>Cluster 1: the cumulative sum of the contigs from SPAdes is 0.6, Megahit is 0.5, the cluster is kept.</li> <li>Cluster 2: the cumulative sum of the contigs from SPAdes is 0.1, Megahit is 0.1, the cluster is removed.</li> <li>Cluster 3: the cumulative sum of the contigs from SPAdes is 0.5, Megahit is 0, the cluster is kept.</li> </ul> <p>The default is <code>5</code> and can be specified with the <code>--perc_reads_contig</code> parameter.</p>"},{"location":"workflow/assembly_polishing/#7-scaffolding","title":"7. Scaffolding","text":"<p>After classifying all contigs and their top BLAST hits into distinct clusters or bins, the contigs are then scaffolded to the centroid of each bin. Any external references that are not centroids of the cluster are subsequently removed to prevent further bias. All members of the cluster are consequently mapped towards their centroid with Minimap2 and consensus is called using iVar-consensus.</p>"},{"location":"workflow/assembly_polishing/#8-annotation-with-reference","title":"8. Annotation with Reference","text":"<p>Regions with 0-depth coverage are annotated with the reference sequence. This is done with a custom script that uses the coverage of the de novo contigs towards the reference sequence to identify regions with 0-depth coverage. The reference sequence is then annotated to these regions.</p> <p>This step can be skipped using <code>--skip_hybrid_consensus</code> parameter.</p>"},{"location":"workflow/consensus_qc/","title":"Report generation and quality control","text":"<p>Viralgenie's report and result interpretation heavily relies on MultiQC. MultiQC is a tool to create a single report from multiple analysis results. It is designed to be used with a wide range of bioinformatics tools and is compatible with a wide range of data formats. Almost all tools are summarised within the MultiQC report that have interactive plots and data tables. However, due to the number of tools included, some results are summarised in the directory <code>overview-tables</code> to reduce the size of the MultiQC report.</p> <p>Tip</p> <p>Complete output descriptions of files and images can be found in the output section.</p> <p>Within the MultiQC report, Viralgenie provides a number of custom tables based on consensus genome quality control data. These tools are:</p> <ul> <li>QUAST: QUAST is a quality assessment tool for genome assemblies. It calculates various metrics such as N50, L50, number of contigs, and total length.</li> <li>CheckV: CheckV is a tool for assessing the quality of metagenome-assembled viral genomes. It calculates various metrics such as completeness, contamination, and strain heterogeneity.</li> <li>blastn: BLAST is a tool for comparing primary biological sequence information. It calculates the similarity between the consensus genome and the reference genome.</li> <li>mmseqs-search - included as 'annotation': MMseqs is an ultra-fast and sensitive search tool for protein and nucleotide databases. Viralgenie uses MMseqs to annotate the consensus genomes and assign them a species name, segment name, expected host, etc.</li> <li>mafft: MAFFT is a multiple sequence alignment program.</li> </ul> <p>Consensus genome quality control can be skipped with <code>--skip_consensus_qc</code>.</p>"},{"location":"workflow/consensus_qc/#quast","title":"QUAST","text":"<p>QUAST is a quality assessment tool for genome assemblies. It calculates various metrics such as N50, L50, number of contigs, and total length. However, in the summary table, it is mainly used to get the number of ambiguous bases in the consensus genome.</p> <p>QUAST can be skipped with <code>--skip_quast</code>.</p>"},{"location":"workflow/consensus_qc/#checkv","title":"CheckV","text":"<p>CheckV is a tool for assessing the quality of metagenome-assembled viral genomes. It calculates various metrics such as completeness, contamination, and strain heterogeneity. CheckV estimates completeness by comparing sequences with a large database of complete viral genomes, metagenomes, metatranscriptomes, and metaviromes.</p> <p>Incomplete genomes for segmented viruses</p> <p>CheckV estimates the completeness of a virus based on all genome segments. If a virus has multiple segments, the completeness of the virus is calculated based on the length of the concatenated segments. For example, Lassa virus has 2 segments L: 7.2kb and S: 3.4kb. The completeness of the virus is calculated based on the length of the concatenated segments (7.2kb + 3.4kb = 10.6kb) and so if the generated consensus genome of the L segment is 7.1kb it will report the completeness as 7.1/10.6 ~ 67%.</p> <p>CheckV can be skipped with <code>--skip_checkv</code>.</p>"},{"location":"workflow/consensus_qc/#prokka","title":"Prokka","text":"<p>Prokka is a whole genome annotation pipeline for identifying features of interest in a set of genomic DNA sequences, and labelling them with useful information. Prokka is a software tool to annotate bacterial, archaeal and viral genomes.</p> <p>Suboptimal annotation</p> <p>Prokka was initially designed for bacterial and archaeal genomes, and may not be optimal for viral genomes. VIGOR4 is a good alternative but is species specific.</p> <p>Custom protein database</p> <p>Prokka can be given a custom protein database to annotate your genomes with, have a look at prot-RVDB for viral protein databases. Supply the database using <code>--prokka_db</code>.</p> <p>Prokka can be skipped with <code>--skip_prokka</code>.</p>"},{"location":"workflow/consensus_qc/#blast","title":"BLAST","text":"<p>blastn is a tool for comparing primary biological sequence information. It calculates the similarity between the consensus genome and the reference genome. The similarity is calculated based on the number of identical bases between the two sequences. Viralgenie uses blastn to compare the sequences against the supplied <code>--reference_pool</code> dataset.</p> <p>BLASTn can be skipped with <code>--skip_blast_qc</code>.</p>"},{"location":"workflow/consensus_qc/#mmseqs-search","title":"MMseqs-search","text":"<p>MMseqs-search is an ultra-fast and sensitive search tool for protein and nucleotide databases. Viralgenie uses MMseqs to search the consensus genomes in an annotated database, like Virousarus (see also defining your own custom annotation database), and uses the annotation data of the best hit to assign the consensus genome a species name, segment name, expected host, and any other metadata that is embedded within the database. This allows Viralgenie, in addition to the BLAST search of reference pool hits, to compare the generated consensus genomes at a species &amp; segment level.</p> <p>Info</p> <p>MMseqs was used for the annotation step instead of BLAST because of the ability to query using a tblastx search for highly diverging viruses while supplying a nucleotide annotation database. To specify another type of search (e.g. blastp, blastx, etc.), please refer to the parameters consensus-qc section.</p> <p>MMseqs-search can be skipped with <code>--skip_annotation</code>.</p>"},{"location":"workflow/consensus_qc/#mafft","title":"MAFFT","text":"<p>MAFFT is a multiple sequence alignment program. It is used to align the following genomic data:</p> <ul> <li>The final consensus genome</li> <li>The identified reference genome from <code>--reference_pool</code></li> <li>The de novo contigs from each assembler (that constituted the final consensus genome)</li> <li>Each consensus genome from the iterative refinement steps.</li> </ul> <p>MAFFT can be skipped with <code>--skip_alignment_qc</code>.</p>"},{"location":"workflow/consensus_qc/#multiqc","title":"MultiQC","text":"<p>MultiQC is a tool to create a single report with interactive plots for multiple bioinformatics analyses across many samples.</p> <p></p> <p>Reports are generated by scanning given directories for recognised log files. These are parsed and a single HTML report is generated summarising the statistics for all logs found. MultiQC reports can describe multiple analysis steps and large numbers of samples within a single plot, and multiple analysis tools making it ideal for routine fast quality control.</p> <p>MultiQC is also used to generate the <code>overview-tables</code> as it extracts the additional data from various tools. The data that needs to be extracted can be modified with the argument <code>--custom_table_headers</code> where a yml file shows which tools need to be included in the summary table in addition to BLAST, CheckV, QUAST, and MMseqs (annotation).</p> custom_table_headers.yml<pre><code>tool:\n  - Tool subsection : # if applicable\n    - name_in_mqc_table: \"new name\"\n    - output_reads: \"deduplicated reads\"\n    - percent_passing_dedup: \"% passing dedup\"\n</code></pre>"},{"location":"workflow/metagenomic_diversity/","title":"Read classification","text":"<p>Viralgenie offers two main tools for the classification of reads and a summary visualisation tool:</p> <ul> <li>Kaiju: Taxonomic classification based on maximum exact matches using protein alignments.</li> <li>Kraken2: Assigns taxonomic labels on a DNA level using a k-mer approach. (optional Bracken)</li> <li>Krona: Interactive multi-layered pie charts of hierarchical data.</li> </ul> <p></p> <p>Want more classifiers?</p> <p>Feel free to reach out and suggest more classifiers. However, if the main goal of your project is to establish the presence of a virus within a sample and are therefore only focused on metagenomic diversity, have a look at taxprofiler</p> <p>The read classification can be skipped with the argument <code>--skip_read_classification</code>, classifiers should be specified with the parameter <code>--read_classifiers 'kaiju,kraken2'</code> (no spaces, no caps). See the parameters classification section for all relevant arguments to control the classification steps.</p>"},{"location":"workflow/metagenomic_diversity/#kaiju","title":"Kaiju","text":"<p>Kaiju classifies individual metagenomic reads using a reference database comprising the annotated protein-coding genes of a set of microbial genomes. It employs a search strategy, which finds maximal exact matching substrings between query and database using a modified version of the backwards search algorithm in the Burrows-Wheeler transform. The Burrows-Wheeler transform is a text transformation that converts the reference sequence database into an easily searchable representation, which allows for exact string matching between a query sequence and the database in time proportional to the length of the query.</p>"},{"location":"workflow/metagenomic_diversity/#kraken2","title":"Kraken2","text":"<p>Kraken2 is a taxonomic sequence classifier that assigns taxonomic labels to DNA sequences. Kraken examines the k-mers within a query sequence and uses the information within those k-mers to query a database. That database maps k-mers to the lowest common ancestor (LCA) of all genomes known to contain a given k-mer.</p> <p>Bracken can optionally be enabled for more accurate estimation of abundances, although these values should be interpreted with caution as viruses don't have marker genes making it difficult to compare abundances across samples &amp; taxa. <code>--read_classifiers 'kraken2,bracken'</code> (no spaces, no caps)</p>"},{"location":"workflow/metagenomic_diversity/#krona","title":"Krona","text":"<p>Krona allows hierarchical data to be explored with zooming, multi-layered pie charts. The interactive charts are self-contained and can be viewed with any modern web browser.</p>"},{"location":"workflow/preprocessing/","title":"Preprocessing","text":"<p>Viralgenie offers three main preprocessing steps for the preprocessing of raw sequencing reads:</p> <ol> <li>Adapter trimming: adapter clipping and pair-merging.</li> <li>UMI deduplication: removal of PCR duplicates based on Unique Molecular Identifiers (UMIs) on a read level.</li> <li>Complexity filtering: removal of low-sequence complexity reads.</li> <li>Host read-removal: removal of reads aligning to reference genome(s) of a host.</li> </ol> <p></p> <p>Preprocessing can be entirely skipped with the option <code>--skip_preprocessing</code>. See the parameters preprocessing section for all relevant arguments to control the preprocessing steps.</p> <p>Tip</p> <p>Samples with fewer than <code>--min_trimmed_reads [default: 1]</code> reads will be removed from any further downstream analysis. These samples will be highlighted in the MultiQC report.</p>"},{"location":"workflow/preprocessing/#read-quality-control","title":"Read Quality control","text":"<p><code>FastQC</code> gives general quality metrics about your reads. It provides information about the quality score distribution across your reads, per base sequence content (%A/T/G/C), adapter contamination, and overrepresented sequences. <code>FastQC</code> is used before and after read processing and after host read-removal to assess the quality of the reads.</p> <pre><code>graph LR;\n    A[Raw reads] --&gt; B[\"`**FastQC**`\"];\n    B --&gt; C[Read processing];\n    C --&gt; D[\"`**FastQC**`\"];\n    D --&gt; E[UMI deduplication];\n    E --&gt; G[Complexity filtering];\n    G --&gt; H[Host read-removal];\n    H --&gt; I[\"`**FastQC**`\"];</code></pre>"},{"location":"workflow/preprocessing/#1-adapter-trimming","title":"1. Adapter trimming","text":"<p>Raw sequencing read processing in the form of adapter clipping and paired-end read merging is performed by the tools <code>fastp</code> or <code>Trimmomatic</code>. The tool <code>fastp</code> is a fast all-in-one tool for preprocessing fastq files. The tool <code>Trimmomatic</code> is a flexible read trimming tool for Illumina NGS data. Both tools can be used to remove adapters and low-quality reads from the raw sequencing reads. An adapter file can be provided through the argument <code>--adapter_fasta</code>.</p> <p>Specify the tool to use for read processing with the <code>--trim_tool</code> parameter, the default is <code>fastp</code>.</p>"},{"location":"workflow/preprocessing/#2-umi-deduplication","title":"2. UMI deduplication","text":"<p>Unique Molecular Identifiers (UMIs) are short sequences that are added during library preparation. They are used to identify and remove PCR duplicates. The tool <code>HUMID</code> is used to remove PCR duplicates based on the UMI sequences. HUMID supports two ways to group reads using their UMI. By default, HUMID uses the directional method, which takes into account the expected errors based on the PCR process. Specify the allowed amount of errors to see reads coming from the same original fragment with <code>--humid_mismatches</code>. Alternatively, HUMID supports the maximum clustering method, where all reads that are within the specified distance are grouped together.</p> <p>Directional vs maximum clustering</p> <p> <p>Taken from UMI-tools: 'The network based deduplication methods' </p></p> <ul> <li>cluster: Form networks of connected UMIs with a mismatch distance of 1. Each connected component is a read group. In the above example, all the UMIs are contained in a single connected component and thus there is one read group containing all reads, with ACGT as the \u2018selected\u2019 UMI.</li> <li>directional (default for both HUMID and UMI-tools): Form networks with edges defined based on distance threshold and $$ \\text{ node A counts} \\geq (2 \\cdot \\text{node B counts}) - 1$$ Each connected component is a read group, with the node with the highest counts selected as the top node for the component. In the example above, the directional edges yield two connected components. One with AAAT by itself and the other with the remaining UMIs with ACGT as the selected node.</li> </ul> <p>Viralgenie supports both deduplication on a read level as well as a mapping level. Specify the <code>--umi_deduplication</code> with <code>read</code> or <code>mapping</code> to choose between the two or specify <code>both</code> to both deduplicate on a read level as well as on a mapping level (after read mapping with reference).</p> <p>By default, viralgenie doesn't assume UMIs are present in the reads. If UMIs are present, specify the <code>--with_umi</code> parameter and <code>--deduplicate</code>.</p>"},{"location":"workflow/preprocessing/#3-complexity-filtering","title":"3. Complexity filtering","text":"<p>Complexity filtering is primarily a run-time optimization step. Low-complexity sequences are defined as having commonly found stretches of nucleotides with limited information content (e.g., the dinucleotide repeat CACACACACA). Such sequences can produce a large number of high-scoring but biologically insignificant results in database searches. Removing these reads therefore saves computational time and resources.</p> <p>Complexity filtering is done with <code>Bbduk</code> which is part of <code>BBtools</code> where the \"duk\" stands for Decontamination Using Kmers. Alternatively, complexity filtering can be done with <code>prinseq++</code>.</p> <p>By default, this step is skipped. If this step shouldn't be skipped, specify <code>--skip_complexity_filtering false</code>. Specify the tool to use for complexity filtering with the <code>--decomplexifier</code> parameter, <code>bbduk</code> or <code>prinseq</code> [default].</p>"},{"location":"workflow/preprocessing/#4-host-read-removal","title":"4. Host read-removal","text":"<p>Contamination, whether derived from experiments or computational processes, looms large in next-generation sequencing data. Such contamination can compromise results from WGS as well as metagenomics studies, and can even lead to the inadvertent disclosure of personal information. To avoid this, host read-removal is performed. Host read-removal is performed by the tool <code>Kraken2</code>.</p> <p>Want to know more?</p> <ul> <li>The human \u201ccontaminome\u201d: bacterial, viral, and computational contamination in whole genome sequences from 1000 families</li> <li>Reconstruction of the personal information from human genome reads in gut metagenome sequencing data</li> </ul> <p>Specify the host database with the <code>--host_k2_db</code> parameter. The default is a small subset of the human genome and we highly suggest that you make this database more elaborate (for example, complete human genome, common sequencer contaminants, bacterial genomes, ...). For this, read the section on creating custom kraken2 host databases.</p>"},{"location":"workflow/variant_and_refinement/","title":"Variant calling and consensus refinement","text":"<p>This subworkflow supports two distinct starting points:</p>"},{"location":"workflow/variant_and_refinement/#a-external-reference-based-analysis","title":"A. External Reference-based Analysis:","text":"<p>References are provided through the samplesheet <code>--mapping_constraints</code></p> <ol> <li>Best matching reference genome is selected</li> <li>Reads are mapped to these reference genome(s)</li> <li>Variants are called from the mappings</li> <li>A consensus genome is generated based on the variant calls</li> </ol>"},{"location":"workflow/variant_and_refinement/#b-de-novo-assembly-refinement","title":"B. De novo Assembly Refinement","text":"<p>Uses the (reference-assisted) de novo consensus genomes as input Performs iterative refinement:</p> <ol> <li>Uses current consensus as reference</li> <li>Maps reads back to this reference</li> <li>Variants are called from the mappings</li> <li>A consensus genome is generated based on the variant calls</li> <li>Repeats steps 1-4 for specified number of iterations (default: 2)</li> </ol> <p>Both approaches use the same variant calling and consensus generation methods, but differ in their starting point and purpose.</p> <p></p> <p>Info</p> <p>This schema is a simplification as there are some additional steps:</p> <ul> <li>Deduplication: (optional) deduplication of reads can be performed with <code>Picard</code> or if UMIs are used <code>UMI-tools</code>.</li> <li>Variant filtering: variant filtering, only variants with sufficient depth and quality are retained for consensus calling (only for BCFtools).</li> <li>Mapping statistics: (optional) generate multiple summary statistics of the BAM files.</li> </ul> <p>Info</p> <p>The variant calling and consensus refinement step can be skipped with the argument <code>--skip_iterative_refinement</code> and <code>--skip_variant_calling</code>, see the parameters iterative refinement section and parameters variant analysis section, respectively, for all relevant arguments to control the variant analysis steps.</p>"},{"location":"workflow/variant_and_refinement/#1a-selection-of-reference","title":"1a. Selection of reference","text":"<p>The reference genome(s) can be supplied with the samplesheet <code>--mapping_constraints</code>, here the reference can be a multiFasta file representing a range of genomes that could be valid reference genomes. Here, viralgenie supports a selection procedure where the reference genomes that share the highest number of k-mers with the read files will be selected and kept for read mapping, variant calling and consensus genome reconstruction.</p> <pre><code>graph LR\n    A[reference genomes] --&gt; B[Sketching]\n    B --&gt; C[Distance evaluation]\n    D[Reads] --&gt; C\n    C --&gt; E[Reference selection]</code></pre> <p>Tip</p> <p>As with any mapping tool, the reference genome(s) should be as close as possible to the sample genome(s) to avoid mapping bias, especially for fast mutating viruses. If the reference genome is too different from the sample genome, the reads will likely not map correctly and could result in incorrect variants and consensus. For this reason, use an extensive reference dataset like the RVDB, if possible even the unclustered one.</p> <p>This procedure is done with <code>Mash</code> where the reads are compared to the reference genomes and the reference genome with the highest number of shared k-mers is selected. The number of shared k-mers can be specified with the <code>--mash_sketch_kmer_size</code> (default: <code>15</code>), and the number of sketches to create with <code>--mash_sketch_size</code>, the default is <code>4000</code>.</p> <p>Tip</p> <ul> <li>As in any k-mer based method, larger k-mers will provide more specificity, while smaller k-mers will provide more sensitivity. Larger genomes will also require larger k-mers to avoid k-mers that are shared by chance.</li> </ul>"},{"location":"workflow/variant_and_refinement/#2-mapping-of-reads","title":"2. Mapping of reads","text":"<p>Mapping filtered reads to supercontig or mapping constraints is done with <code>BowTie2</code>,<code>BWAmem2</code> and <code>BWA</code>.</p> <p>The comparison of Bowtie2 and BWA-mem was done for Yao et al. (2020) where they found that BWA-MEM2 had a higher mapping rate (faster) and better accuracy. BWA-mem detected more variant bases in mapping reads than Bowtie2. The tool bwa-mem2 is the next version of the bwa-mem algorithm in bwa. It produces alignment identical to bwa and is ~1.3-3.1x faster depending on the use-case, dataset and the running machine.</p> <p>All three methods are supported to keep protocol compatibility with other pipelines and to allow the user to choose the best method for their data.</p> <p>The mapping tool can be specified with the <code>--mapper</code> parameter, the default is <code>bwamem2</code>, in case the intermediate mapper (for intermediate refinement cycles) needs to be different, this can be specified with <code>--intermediate_mapper</code> otherwise it uses the supplied <code>--mapper</code> tool.</p>"},{"location":"workflow/variant_and_refinement/#21-deduplication","title":"2.1 Deduplication","text":"<p>Read deduplication is an optional step that can be performed with <code>Picard</code> or if UMIs are used <code>UMI-tools</code>. Unless you are using UMIs it is not possible to establish whether the fragments you have sequenced from your sample were derived via true biological duplication (i.e. sequencing independent template fragments) or as a result of PCR biases introduced during the library preparation. To correct your reads, use picard MarkDuplicates to mark the duplicate reads identified amongst the alignments to allow you to gauge the overall level of duplication in your samples. So if you have UMI\u2019s, no need to use Picard, instead use UMI-tools to deduplicate your reads. Where instead of mapping location and read similarity, UMI-tools uses the UMI to identify PCR duplicates.</p> <p>Specify <code>--deduplicate</code> to enable deduplication, the default is <code>true</code>. If UMIs are used, specify <code>--with_umi</code> and <code>--umi_deduplicate 'mapping' | 'both'</code> to enable UMI-tools deduplication. UMIs can be in the read header, if it is not in the header specify <code>--skip_umi_extract false</code>, the default is <code>true</code>.</p> <p>By default the UMIs are separated in the header by ':' if this is different, specify with \"--umi_separator 'YOUR_SEPARATOR'\".</p>"},{"location":"workflow/variant_and_refinement/#22-mapping-statistics","title":"2.2 Mapping statistics","text":"<p>Viralgenie uses multiple tools to get statistics on the variants and on the read mapping. These tools are:</p> <ul> <li><code>samtools flagstat</code> to get the number of reads that are mapped, unmapped, paired, etc.</li> <li><code>samtools idxstats</code> to get the number of reads that are mapped to each reference sequence.</li> <li><code>samtools stats</code> to collect statistics from BAM files and outputs in a text format.</li> <li><code>picard CollectMultipleMetrics</code> to collect multiple metrics from a BAM file.</li> <li><code>mosdepth</code> to calculate genome-wide sequencing coverage.</li> </ul> <p>There is a little overlap between the tools, but they all provide a different perspective on the mapping statistics.</p> <p>By default, all these tools are run, but they can be skipped with the argument <code>--mapping_stats false</code>. In case the intermediate mapping statistics (for intermediate refinement cycles) don't need to be determined set <code>--intermediate_mapping_stats false</code>.</p>"},{"location":"workflow/variant_and_refinement/#3-variant-calling","title":"3. Variant calling","text":"<p>Variant calling is done with <code>BCFTools</code> and <code>iVar</code>, here a SNP will need to have at least a depth of 10 and a base quality of 20.</p> <p>BCFtools is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. iVar is a computational package that contains functions broadly useful for viral amplicon-based sequencing while each of iVar functions can be accomplished using existing tools, iVar contains an intersection of functionality from multiple tools that are required to call iSNVs and consensus sequences from viral sequencing data across multiple replicates.</p> <p>There are multiple studies on the benchmarking of variant callers as this is an area with active development. For instance Bassano et al. (2023) noticed that BCFtools called mutations with higher precision and recall than iVar. However, the reason behind this is that iVar has a lower precision than the others within their setup as it detects a lot of \u2018additional\u2019 variants within the sample, resulting in a higher amount of false positives but also true positives.</p> <p>Tip</p> <p>Bcftools doesn't handle well multiallelic sites, so if you have a lot of multiallelic sites, iVar is the better choice. iVar is also the better choice if you have a lot of low-frequency variants.</p> <p>The variant caller can be specified with the <code>--variant_caller</code> parameter, the default is <code>ivar</code>. In case the intermediate variant caller (for intermediate refinement cycles) needs to be different, this can be specified with <code>--intermediate_variant_caller</code> otherwise it uses the supplied <code>--variant_caller</code> tool.</p>"},{"location":"workflow/variant_and_refinement/#variant-filtering","title":"Variant filtering","text":"<p>The following steps are implemented for variant filtering.</p> <ul> <li>[only for <code>BCFtools</code>]: split up multiallelic sites into biallelic records and SNPs and indels should be merged into a single record.</li> <li>Variant filtering: filter out variants with an allelic depth of less than 75% of the average depth of the sample.</li> <li>[only for <code>iVar</code>]: strand bias correction &amp; collapsing variants belonging to the same codon.</li> </ul> <p>Info</p> <p>If these filtering options are not to your liking, you can modify all of them. See the section on configuration for more information on how to do so.</p>"},{"location":"workflow/variant_and_refinement/#4-consensus-calling","title":"4. Consensus calling","text":"<p>The consensus genome is updated with the variants of sufficient quality, either the ones determined previously in variant calling and filtering for the <code>--consensus_caller</code> <code>bcftools</code> or they are redetermined for <code>ivar</code>.</p> <p>There are again a couple of differences between the iVar and BCFtools:</p> <ol> <li>Low frequency deletions in iVar. <p>Areas of low frequency are more easily deleted and not carried along with iVar, this can be a bad thing during the iterative improvement of the consensus but is a good thing at the final consensus step.</p> </li> <li>Ambiguous nucleotides for multi-allelic sites in iVar. <p>iVar is capable to give lower frequency nucleotides ambiguous bases a summarising annotation instead of 'N'. For example at a certain position, the frequency of 'A' is 40% and of 'G' is 40%. Instead of reporting an 'N', iVar will report 'R'.</p> <p></p> </li> <li>Ambiguous nucleotides for low read depth. <p>In case of a low read depth at a certain position, if it doesn't get flagged by bcftools during variant calling, it will not be considered as a variant and the consensus will not be updated. iVar will update the consensus with an ambiguous base in case of low read depth.</p> <p></p> </li> </ol> <p>The consensus caller can be specified with the <code>--consensus_caller</code> parameter, the default is <code>ivar</code>. The intermediate consensus caller (for intermediate refinement cycles) can be specified with <code>--intermediate_consensus_caller</code> and is by default <code>bcftools</code>.</p>"}]}