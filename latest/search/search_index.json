{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Viralgenie","text":""},{"location":"#_2","title":"Viralgenie","text":"<p>A metagenomic analysis pipeline for eukaryotic viruses written in nextflow.</p> <p> </p> <p> </p>"},{"location":"#introduction","title":"Introduction","text":"<p>Viralgenie is a bioinformatics best-practice analysis pipeline for reconstructing consensus genomes and to identify intra-host variants from metagenomic sequencing data or enriched based sequencing data like hybrid capture.</p>"},{"location":"#pipeline-summary","title":"Pipeline summary","text":"<ol> <li>Read QC (<code>FastQC</code>)</li> <li>Performs optional read pre-processing<ul> <li>Adapter trimming(<code>fastp</code>, <code>Trimmomatic</code>)</li> <li>Low complexity and quality filtering (<code>bbduk</code>)</li> <li>Host-read removal (<code>BowTie2</code>)</li> </ul> </li> <li>Metagenomic diveristy mapping<ul> <li>Performs taxonomic classification and/or profiling using one or more of:<ul> <li><code>Kraken2</code></li> <li><code>Kaiju</code></li> </ul> </li> <li>Plotting Kraken2 and Kaiju (<code>Krona</code>)</li> </ul> </li> <li>Denovo assembly (<code>SPAdes</code>, <code>TRINITY</code>, <code>megahit</code>), combine contigs.</li> <li>Contig reference idententification (<code>blastn</code>)<ul> <li>Identify top 5 blast hits</li> <li>Merge blast hit and all contigs of a sample</li> </ul> </li> <li>[Optional] Precluster contigs based on taxonomy<ul> <li>Identify taxonomy <code>Kraken2</code> and <code>Kaiju</code></li> <li>Resolve potential incosistencies in taxonomy <code>Kaiju-mergeOutputs</code></li> </ul> </li> <li>Cluster contigs (or every taxonomic bin) of samples, options are:<ul> <li><code>cdhitest</code></li> <li><code>vsearch</code></li> <li><code>mmseqs-linclust</code></li> <li><code>mmseqs-cluster</code></li> <li><code>vRhyme</code></li> <li><code>mash</code></li> </ul> </li> <li>Scaffolding of contigs to centroid (<code>Minimap2</code>, <code>iVar-consensus</code>)</li> <li>[Optional] Annotate 0-depth regions with external reference <code>custom-script</code>.</li> <li>Mapping filtered reads to supercontig and mapping constrains(<code>BowTie2</code>,<code>BWAmem2</code> and <code>BWA</code>)</li> <li>[Optional] Deduplicate reads (<code>Picard</code> or if UMI's are used <code>UMI-tools</code>)</li> <li>Variant calling and filtering (<code>BCFTools</code>,<code>iVar</code>)</li> <li>Create consensus genome (<code>BCFTools</code>,<code>iVar</code>)</li> <li>Repeat step 10-13 multiple times for the denovo contig route</li> <li>Consensus evaluation and annotation (<code>QUAST</code>,<code>CheckV</code>,<code>blastn</code>, <code>mmseqs-search</code>)</li> <li>Result summary visualisation for raw read, alignment, assembly, variant calling and consensus calling results (<code>MultiQC</code>)</li> </ol>"},{"location":"#usage","title":"Usage","text":"<p>Note</p> <p>If you are new to Nextflow and nf-core, please refer to this page on how to set-up Nextflow. Make sure to test your setup with <code>-profile test</code> before running the workflow on actual data.</p> <p>Now, you can run the pipeline using:</p> <pre><code>nextflow run Joon-Klaps/viralgenie \\\n   -profile &lt;docker/singularity/.../institute&gt; \\\n   --input samplesheet.csv \\\n   --outdir &lt;OUTDIR&gt;\n</code></pre> <p>Warning</p> <p>Please provide pipeline parameters via the CLI or Nextflow <code>-params-file</code> option. Custom config files including those provided by the <code>-c</code> Nextflow option can be used to provide any configuration except for parameters;  see docs.</p> <p>For more details and further functionality, please refer to the usage documentation and the parameter documentation.</p>"},{"location":"#credits","title":"Credits","text":"<p>Viralgenie was originally written by <code>Joon-Klaps</code>.</p> <p>We thank the following people for their extensive assistance in the development of this pipeline:</p> <ul> <li><code>Philippe Lemey</code></li> <li><code>Liana Kafetzopoulou</code></li> <li><code>nf-core community</code></li> </ul>"},{"location":"#contributions-and-support","title":"Contributions and Support","text":"<p>If you would like to contribute to this pipeline, please see the contributing guidelines.</p>"},{"location":"#citations","title":"Citations","text":"<p>An extensive list of references for the tools used by the pipeline can be found in the <code>CITATIONS.md</code> file.</p> <p>You can cite the <code>nf-core</code> publication as follows:</p> <p>The nf-core framework for community-curated bioinformatics pipelines.</p> <p>Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso &amp; Sven Nahnsen.</p> <p>Nat Biotechnol. 2020 Feb 13. doi: 10.1038/s41587-020-0439-x.</p>"},{"location":"CITATIONS/","title":"Citations","text":""},{"location":"CITATIONS/#viralgenie","title":"Viralgenie","text":"<p>Warning</p> <p>Viralgenie is currently not Published. Please cite as:</p> <ul> <li>Klaps J, Lemey P, Kafetzopoulou L. Viralgenie: A metagenomics analysis pipeline for eukaryotic viruses. Github https://github.com/Joon-Klaps/viralgenie</li> </ul>"},{"location":"CITATIONS/#nf-core","title":"nf-core","text":"<p>Ewels PA, Peltzer A, Fillinger S, Patel H, Alneberg J, Wilm A, Garcia MU, Di Tommaso P, Nahnsen S. The nf-core framework for community-curated bioinformatics pipelines. Nat Biotechnol. 2020 Mar;38(3):276-278. doi: 10.1038/s41587-020-0439-x. PubMed PMID: 32055031.</p>"},{"location":"CITATIONS/#nextflow","title":"Nextflow","text":"<p>Di Tommaso P, Chatzou M, Floden EW, Barja PP, Palumbo E, Notredame C. Nextflow enables reproducible computational workflows. Nat Biotechnol. 2017 Apr 11;35(4):316-319. doi: 10.1038/nbt.3820. PubMed PMID: 28398311.</p>"},{"location":"CITATIONS/#pipeline-tools","title":"Pipeline tools","text":"<ul> <li> <p>Bbduk</p> <p>Bushnell B. (2022) BBMap, URL: http://sourceforge.net/projects/bbmap/</p> </li> <li> <p>BCFtools</p> <p>Danecek, Petr et al. \u201cTwelve years of SAMtools and BCFtools.\u201d GigaScience vol. 10,2 (2021): giab008. doi:10.1093/gigascience/giab008</p> </li> <li> <p>blast</p> <p>Camacho, Christiam et al. \u201cBLAST+: architecture and applications.\u201d BMC bioinformatics vol. 10 421. 15 Dec. 2009, doi:10.1186/1471-2105-10-421</p> </li> <li> <p>Bowtie2</p> <p>Langmead, Ben, and Steven L Salzberg. \u201cFast gapped-read alignment with Bowtie 2.\u201d Nature methods vol. 9,4 357-9. 4 Mar. 2012, doi:10.1038/nmeth.1923</p> </li> <li> <p>BWA-MEM</p> <p>Li H. (2013) Aligning sequence reads, clone sequences and assembly contigs with BWA-MEM. arXiv:1303.3997v2.</p> </li> <li> <p>BWA-MEM2</p> <p>M. Vasimuddin, S. Misra, H. Li and S. Aluru, \"Efficient Architecture-Aware Acceleration of BWA-MEM for Multicore Systems,\" 2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS), Rio de Janeiro, Brazil, 2019, pp. 314-324, doi: 10.1109/IPDPS.2019.00041.</p> </li> <li> <p>cdhit</p> <p>Fu, Limin et al. \u201cCD-HIT: accelerated for clustering the next-generation sequencing data.\u201d Bioinformatics (Oxford, England) vol. 28,23 (2012): 3150-2. doi:10.1093/bioinformatics/bts565</p> </li> <li> <p>checkv</p> <p>Nayfach, Stephen et al. \u201cCheckV assesses the quality and completeness of metagenome-assembled viral genomes.\u201d Nature biotechnology vol. 39,5 (2021): 578-585. doi:10.1038/s41587-020-00774-7</p> </li> <li> <p>FastQC</p> <p>Andrews, S. (2010). FastQC: A Quality Control Tool for High Throughput Sequence Data [Online].</p> </li> <li> <p>fastp</p> <p>Chen, Shifu et al. \u201cfastp: an ultra-fast all-in-one FASTQ preprocessor.\u201d Bioinformatics (Oxford, England) vol. 34,17 (2018): i884-i890. doi:10.1093/bioinformatics/bty560</p> </li> <li> <p>iVar</p> <p>Grubaugh, Nathan D et al. \u201cAn amplicon-based sequencing framework for accurately measuring intrahost virus diversity using PrimalSeq and iVar.\u201d Genome biology vol. 20,1 8. 8 Jan. 2019, doi:10.1186/s13059-018-1618-7</p> </li> <li> <p>Kaiju</p> <p>Menzel, Peter et al. \u201cFast and sensitive taxonomic classification for metagenomics with Kaiju.\u201d Nature communications vol. 7 11257. 13 Apr. 2016, doi:10.1038/ncomms11257</p> </li> <li> <p>Kraken2</p> <p>Wood, Derrick E., Jennifer Lu, and Ben Langmead. 2019. Improved Metagenomic Analysis with Kraken 2. Genome Biology 20 (1): 257. doi: 10.1186/s13059-019-1891-0.</p> </li> <li> <p>leiden-algorithm</p> <p>Traag, V A et al. \u201cFrom Louvain to Leiden: guaranteeing well-connected communities.\u201d Scientific reports vol. 9,1 5233. 26 Mar. 2019, doi:10.1038/s41598-019-41695-z</p> </li> <li> <p>Mash</p> <p>Ondov, Brian D et al. \u201cMash: fast genome and metagenome distance estimation using MinHash.\u201d Genome biology vol. 17,1 132. 20 Jun. 2016, doi:10.1186/s13059-016-0997-x</p> </li> <li> <p>Megahit</p> <p>Li, Dinghua et al. \u201cMEGAHIT v1.0: A fast and scalable metagenome assembler driven by advanced methodologies and community practices.\u201d Methods (San Diego, Calif.) vol. 102 (2016): 3-11. doi:10.1016/j.ymeth.2016.02.020</p> </li> <li> <p>Minimap2</p> <p>Li, Heng. \u201cMinimap2: pairwise alignment for nucleotide sequences.\u201d Bioinformatics (Oxford, England) vol. 34,18 (2018): 3094-3100. doi:10.1093/bioinformatics/bty191</p> </li> <li> <p>MMseqs2</p> <p>Steinegger, Martin, and Johannes S\u00f6ding. \u201cMMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets.\u201d Nature biotechnology vol. 35,11 (2017): 1026-1028. doi:10.1038/nbt.3988</p> </li> <li> <p>Mosdepth</p> <p>Pedersen, Brent S, and Aaron R Quinlan. \u201cMosdepth: quick coverage calculation for genomes and exomes.\u201d Bioinformatics (Oxford, England) vol. 34,5 (2018): 867-868. doi:10.1093/bioinformatics/btx699</p> </li> <li> <p>MultiQC</p> <p>Ewels, Philip et al. \u201cMultiQC: summarize analysis results for multiple tools and samples in a single report.\u201d Bioinformatics (Oxford, England) vol. 32,19 (2016): 3047-8. doi:10.1093/bioinformatics/btw354</p> </li> <li> <p>picard-tools</p> </li> <li> <p>QUAST</p> <p>Gurevich, Alexey et al. \u201cQUAST: quality assessment tool for genome assemblies.\u201d Bioinformatics (Oxford, England) vol. 29,8 (2013): 1072-5. doi:10.1093/bioinformatics/btt086</p> </li> <li> <p>SAMtools</p> <p>Li H. A statistical framework for SNP calling, mutation discovery, association mapping and population genetical parameter estimation from sequencing data. Bioinformatics. 2011 Nov 1;27(21):2987-93. doi: 10.1093/bioinformatics/btr509. Epub 2011 Sep 8. PMID: 21903627; PMCID: PMC3198575.</p> </li> <li> <p>SPAdes</p> <p>Bankevich, Anton et al. \u201cSPAdes: a new genome assembly algorithm and its applications to single-cell sequencing.\u201d Journal of computational biology : a journal of computational molecular cell biology vol. 19,5 (2012): 455-77. doi:10.1089/cmb.2012.0021</p> </li> <li> <p>Trimmomatic</p> <p>Bolger, Anthony M et al. \u201cTrimmomatic: a flexible trimmer for Illumina sequence data.\u201d Bioinformatics (Oxford, England) vol. 30,15 (2014): 2114-20. doi:10.1093/bioinformatics/btu170</p> </li> <li> <p>Trinity</p> <p>Haas, Brian J et al. \u201cDe novo transcript sequence reconstruction from RNA-seq using the Trinity platform for reference generation and analysis.\u201d Nature protocols vol. 8,8 (2013): 1494-512. doi:10.1038/nprot.2013.084</p> </li> <li> <p>UMI-tools</p> <p>Smith, Tom et al. \u201cUMI-tools: modeling sequencing errors in Unique Molecular Identifiers to improve quantification accuracy.\u201d Genome research vol. 27,3 (2017): 491-499. doi:10.1101/gr.209601.116</p> </li> <li> <p>vRhyme</p> <p>Kieft, Kristopher et al. \u201cvRhyme enables binning of viral genomes from metagenomes.\u201d Nucleic acids research vol. 50,14 (2022): e83. doi:10.1093/nar/gkac341</p> </li> <li> <p>VSEARCH</p> <p>Rognes, Torbj\u00f8rn et al. \u201cVSEARCH: a versatile open source tool for metagenomics.\u201d PeerJ vol. 4 e2584. 18 Oct. 2016, doi:10.7717/peerj.2584</p> </li> </ul>"},{"location":"CITATIONS/#software-packagingcontainerisation-tools","title":"Software packaging/containerisation tools","text":"<ul> <li> <p>Anaconda</p> <p>Anaconda Software Distribution. Computer software. Vers. 2-2.4.0. Anaconda, Nov. 2016. Web.</p> </li> <li> <p>Bioconda</p> <p>Gr\u00fcning B, Dale R, Sj\u00f6din A, Chapman BA, Rowe J, Tomkins-Tinch CH, Valieris R, K\u00f6ster J; Bioconda Team. Bioconda: sustainable and comprehensive software distribution for the life sciences. Nat Methods. 2018 Jul;15(7):475-476. doi: 10.1038/s41592-018-0046-7. PubMed PMID: 29967506.</p> </li> <li> <p>BioContainers</p> <p>da Veiga Leprevost F, Gr\u00fcning B, Aflitos SA, R\u00f6st HL, Uszkoreit J, Barsnes H, Vaudel M, Moreno P, Gatto L, Weber J, Bai M, Jimenez RC, Sachsenberg T, Pfeuffer J, Alvarez RV, Griss J, Nesvizhskii AI, Perez-Riverol Y. BioContainers: an open-source and community-driven framework for software standardization. Bioinformatics. 2017 Aug 15;33(16):2580-2582. doi: 10.1093/bioinformatics/btx192. PubMed PMID: 28379341; PubMed Central PMCID: PMC5870671.</p> </li> <li> <p>Docker</p> <p>Merkel, D. (2014). Docker: lightweight linux containers for consistent development and deployment. Linux Journal, 2014(239), 2. doi: 10.5555/2600239.2600241.</p> </li> <li> <p>Singularity</p> <p>Kurtzer GM, Sochat V, Bauer MW. Singularity: Scientific containers for mobility of compute. PLoS One. 2017 May 11;12(5):e0177459. doi: 10.1371/journal.pone.0177459. eCollection 2017. PubMed PMID: 28494014; PubMed Central PMCID: PMC5426675.</p> </li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing Guidelines","text":"<p>Hi there!</p> <p>\ud83e\udd29 Many thanks for taking an interest in improving Viralgenie. \ud83e\udd29</p> <p>We try to manage the required tasks for Viralgenie using GitHub issues, you probably came to this page when creating one. Please use the pre-filled template to save time.</p> <p>However, don't be put off by this template - other more general issues and suggestions are welcome! Contributions to the code are even more welcome ;</p> <p>Info</p> <p>If you need help using or modifying viralgenie then the best place to ask is on the nf-core Slack Joon-Klaps.</p>"},{"location":"CONTRIBUTING/#contribution-workflow","title":"Contribution workflow","text":"<p>If you'd like to write some code for Joon-Klaps/viralgenie, the standard workflow is as follows:</p> <ol> <li>Check that there isn't already an issue about your idea in the Joon-Klaps/viralgenie issues to avoid duplicating work. If there isn't one already, please create one so that others know you're working on this</li> <li>Fork the Joon-Klaps/viralgenie repository to your GitHub account</li> <li>Make the necessary changes / additions within your forked repository following Pipeline conventions</li> <li>Use <code>nf-core schema build</code> and add any new parameters to the pipeline JSON schema (requires nf-core tools &gt;= 1.10).</li> <li>Submit a Pull Request against the <code>dev</code> branch and wait for the code to be reviewed and merged</li> </ol> <p>If you're not used to this workflow with git, you can start with some docs from GitHub or even their excellent <code>git</code> resources.</p>"},{"location":"CONTRIBUTING/#tests","title":"Tests","text":"<p>You can optionally test your changes by running the pipeline locally. Then it is recommended to use the <code>debug</code> profile to receive warnings about process selectors and other debug info. Example: <code>nextflow run . -profile debug,test,docker --outdir &lt;OUTDIR&gt;</code>.</p> <p>When you create a pull request with changes, GitHub Actions will run automatic tests. Typically, pull-requests are only fully reviewed when these tests are passing, though of course we can help out before then.</p> <p>There are typically two types of tests that run:</p>"},{"location":"CONTRIBUTING/#lint-tests","title":"Lint tests","text":"<p><code>nf-core</code> has a set of guidelines which viralgenie adheres to. To enforce these and ensure that viralgenie stays in sync, we have developed a helper tool which runs checks on the pipeline code. This is in the nf-core/tools repository and once installed can be run locally with the <code>nf-core lint &lt;pipeline-directory&gt;</code> command.</p> <p>If any failures or warnings are encountered, please follow the listed URL for more documentation.</p>"},{"location":"CONTRIBUTING/#pipeline-tests","title":"Pipeline tests","text":"<p>Viralgenie is set up with a minimal set of test-data. <code>GitHub Actions</code> then runs the pipeline on this data to ensure that it exits successfully. If there are any failures then the automated tests fail. These tests are run both with the latest available version of <code>Nextflow</code> and also the minimum required version that is stated in the pipeline code.</p>"},{"location":"CONTRIBUTING/#getting-help","title":"Getting help","text":"<p>For further information/help, please consult the Joon-Klaps/viralgenie documentation and don't hesitate to get in touch on Slack Joon-Klaps channel (join the nf-core Slack here).</p>"},{"location":"CONTRIBUTING/#pipeline-contribution-conventions","title":"Pipeline contribution conventions","text":"<p>To make the Joon-Klaps/viralgenie code and processing logic more understandable for new contributors and to ensure quality, we semi-standardise the way the code and other contributions are written.</p>"},{"location":"CONTRIBUTING/#adding-a-new-step","title":"Adding a new step","text":"<p>If you wish to contribute a new step, please use the following coding standards:</p> <ol> <li>Define the corresponding input channel into your new process from the expected previous process channel</li> <li>Write the process block (see below).</li> <li>Define the output channel if needed (see below).</li> <li>Add any new parameters to <code>nextflow.config</code> with a default (see below).</li> <li>Add any new parameters to <code>nextflow_schema.json</code> with help text (via the <code>nf-core schema build</code> tool).</li> <li>Add sanity checks and validation for all relevant parameters.</li> <li>Perform local tests to validate that the new code works as expected.</li> <li>If applicable, add a new test command in <code>.github/workflow/ci.yml</code>.</li> <li>Update MultiQC config <code>assets/multiqc_config.yml</code> so relevant suffixes, file name clean up and module plots are in the appropriate order. If applicable, add a MultiQC module.</li> <li>Add a description of the output files and if relevant any appropriate images from the MultiQC report to <code>docs/output.md</code>.</li> </ol>"},{"location":"CONTRIBUTING/#default-values","title":"Default values","text":"<p>Parameters should be initialised / defined with default values in <code>nextflow.config</code> under the <code>params</code> scope.</p> <p>Once there, use <code>nf-core schema build</code> to add to <code>nextflow_schema.json</code>.</p>"},{"location":"CONTRIBUTING/#default-processes-resource-requirements","title":"Default processes resource requirements","text":"<p>Sensible defaults for process resource requirements (CPUs / memory / time) for a process should be defined in <code>conf/base.config</code>. These should generally be specified generic with <code>withLabel:</code> selectors so they can be shared across multiple processes/steps of the pipeline. A nf-core standard set of labels that should be followed where possible can be seen in the nf-core pipeline template, which has the default process as a single core-process, and then different levels of multi-core configurations for increasingly large memory requirements defined with standardised labels.</p> <p>The process resources can be passed on to the tool dynamically within the process with the <code>${task.cpu}</code> and <code>${task.memory}</code> variables in the <code>script:</code> block.</p>"},{"location":"CONTRIBUTING/#naming-schemes","title":"Naming schemes","text":"<p>Please use the following naming schemes, to make it easy to understand what is going where.</p> <ul> <li>initial process channel: <code>ch_output_from_&lt;process&gt;</code></li> <li>intermediate and terminal channels: <code>ch_&lt;previousprocess&gt;_for_&lt;nextprocess&gt;</code></li> </ul>"},{"location":"CONTRIBUTING/#nextflow-version-bumping","title":"Nextflow version bumping","text":"<p>If you are using a new feature from core Nextflow, you may bump the minimum required version of nextflow in the pipeline with: <code>nf-core bump-version --nextflow . [min-nf-version]</code></p>"},{"location":"CONTRIBUTING/#images-and-figures","title":"Images and figures","text":"<p>For overview images and other documents we follow the nf-core style guidelines and examples.</p>"},{"location":"CONTRIBUTING/#github-codespaces","title":"GitHub Codespaces","text":"<p>This repo includes a devcontainer configuration which will create a GitHub Codespaces for Nextflow development! This is an online developer environment that runs in your browser, complete with VSCode and a terminal.</p> <p>To get started:</p> <ul> <li>Open the repo in Codespaces</li> <li>Tools installed<ul> <li>nf-core</li> <li>Nextflow</li> </ul> </li> </ul> <p>Devcontainer specs:</p> <ul> <li>DevContainer config</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>Viralgenie uses Nextflow, and a package/container management system (Docker, singularity or conda) so both need to be installed on the system where you launch your analysis.</p> <p>New to bioinformatics?</p> <p>If the word \"terminal\" brings to mind an airport boarding area, you can become a little lost. This blogpost (up until Configuring an Xserver ...) will help you setup nextflow and docker on a windows computer, if you are new to bioinformatics.</p>"},{"location":"installation/#software-managers-docker-singularity-and-conda","title":"Software managers: Docker, singularity, and conda","text":"<p>Viralgenie can be run using either Docker, singularity or conda. The choice of container system is up to the user, but it is important to note that Docker and Singularity are the most reproducible. Nextflow supports more containers in addition to Docker and Singularity, such as Podman, Shifter, and Charliecloud. You can read the full list of supported containers and how to set them up here.</p> <p>When using these containers, Nextflow will use the manager for each process that is executed. In other words, Nextflow will be using <code>docker run</code> or <code>singularity exec</code> without the need for you to do anything else.</p> DockerSingularityConda | Mamba <p>Docker is a containerisation system that allows you to package your code, tools and data into a single image that can be run on most operating systems. It is the most widely used containerisation system in bioinformatics.</p> <p>To install Docker, follow the instructions on the Docker website.</p> <p>Warning</p> <p>Docker requires root access to run. If you do not have root access like, i.e. a user on a HPC or on a cloud - use Singularity instead.</p> <p>Singularity is a containerisation system that allows you to package your code, tools and data into a single image that can be run on most operating systems. It is the most widely used containerisation system in bioinformatics.</p> <p>To install Singularity, follow the instructions on the Singularity website.</p> <p>Conda is a package manager that allows you to install software packages and dependencies in isolated environments. It is a good choice if you are facing issues while installing Docker or Singularity.</p> <ul> <li>To install Conda, follow the instructions on the Conda website.</li> <li>To install Mamba, a faster alternative to Conda, follow the instructions on the Mamba miniforge website.</li> </ul> <p>Warning</p> <p>Conda environments are great! However, conda tools can easily become broken or incompatible due to dependency issues. For this reason, conda is not as reproducible as Docker or Singularity containers. If you encounter issues with Conda, please try running the pipeline with Docker or Singularity first to see if the issue persists. In other words, if you have a container system, use it over conda!</p>"},{"location":"installation/#nextflow","title":"Nextflow","text":"<p>Nextflow runs on most POSIX systems (Linux, macOS, etc) and requires java 11 or later. It can be installed in several ways, including using the Nextflow installer or Bioconda.</p> Nextflow installerBioconda <p>Tip</p> <p>Unsure how to install Nextflow with these commands? Check out the Nextflow installation documentation for more information.</p> <pre><code># Make sure that Java v11+ is installed:\njava -version\n\n# Install Nextflow\ncurl -fsSL get.nextflow.io | bash\n\n# Try a simple demo\n./nextflow run hello\n</code></pre> <p>Tip</p> <p>Add Nextflow binary to your user's <code>PATH</code>: <pre><code>mv nextflow ~/bin/\n</code></pre> Or to install it system-wide: <pre><code>sudo mv nextflow /usr/local/bin/\n</code></pre></p> <p>First, set up Bioconda according to the Bioconda documentation, notably setting up channels: <pre><code>conda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\n</code></pre></p> <p>A best practice with conda is to create an environment and install the tools in it. Therefore you will prevent version conflicts and keep everything clean. To do so use the following command:</p> <pre><code>conda create --name nextflow-env nextflow\nconda activate nextflow-env\n</code></pre> <p>To deactivate the conda environment, run the following command:</p> <pre><code>conda deactivate\n</code></pre> <p>If you're already in the conda environment you want to use, you can just install Nextflow directly:</p> <pre><code>conda install nextflow\n</code></pre>"},{"location":"installation/#viralgenie","title":"Viralgenie","text":"<p>If you have both nextflow and a software manager installed, you are all set! You can test the pipeline using the following command:</p> <pre><code>nextflow run Joon-Klaps/viralgenie \\\n    -profile test,&lt;docker/singularity/.../institute&gt;\n</code></pre> <p>Note</p> <p>With the argument <code>-profile &lt;docker/singularity/.../institute&gt;</code>, you can specify the container system you want to use. The <code>test</code> profile is used to run the pipeline with a small dataset to verify if everything is working correctly.</p> <p>Running nextflow on a High performance computing (HPC) system?</p> <p>You might not be the first person to run a nextflow pipeline on your infrastructure! Check out the nf-core configuration website as it might already contain a specific configuration for your infrastructure.</p> <p>Apple silicon (ARM)</p> <p>If you are using an Apple silicon (ARM) machine, you may encounter issues. Most tools are not yet compatible with ARM architecture, therefore conda will most likely fail. In this case, use Docker in combination with the profile <code>arm</code>. <pre><code>nextflow run Joon-Klaps/viralgenie \\\n    -profile test,docker,arm\n</code></pre> If you still encounter issues, you can setup a nextflow tower account and run the pipeline with wave containers. In this config file, supply the following: <pre><code>wave {\n    enabled = true\n    wave.strategy = ['dockerfile']\n}\ntower {\n    accessToken = '&lt;your access token&gt;'\n}\n</code></pre></p>"},{"location":"output/","title":"Output","text":""},{"location":"output/#introduction","title":"Introduction","text":"<p>This document describes the output produced by the pipeline. Most of the plots are taken from the MultiQC report, which summarises results at the end of the pipeline.</p> <p>The directories listed below will be created in the results directory after the pipeline has finished. All paths are relative to the top-level results directory.</p> <p>Tip</p> <p>A global, partly, random prefix can be created using the argument <code>--prefix &lt;string&gt;</code>. The following string will then be used as a prefix to all output files. <pre><code>\"&lt;prefix_string&gt;_&lt;date&gt;_&lt;pipeline_version&gt;_&lt;workflow_runName&gt;\"\n</code></pre></p>"},{"location":"output/#preprocessing","title":"Preprocessing","text":"<p>All output files of the preprocessing steps can be found in the directory <code>preprocessing/</code>.</p>"},{"location":"output/#fastqc","title":"FastQC","text":"Output files <ul> <li><code>fastqc/{raw,trim,host}</code><ul> <li><code>*_fastqc.html</code>: FastQC report containing quality metrics.</li> <li><code>*_fastqc.zip</code>: Zip archive containing the FastQC report, tab-delimited data file and plot images.</li> </ul> </li> </ul> <p>FastQC gives general quality metrics about your sequenced reads. It provides information about the quality score distribution across your reads, per base sequence content (%A/T/G/C), adapter contamination and overrepresented sequences. For further reading and documentation see the FastQC help pages.</p> <p></p> <p></p> <p></p> <p>Tip</p> <p>The FastQC plots displayed in the MultiQC report shows untrimmed, trimmed and host filtered reads. Make sure to check the section titles for the correct set of reads.</p>"},{"location":"output/#fastp","title":"fastp","text":"<p>fastp is a FASTQ pre-processing tool for quality control, trimmming of adapters, quality filtering and other features.</p> Output files <ul> <li><code>fastp/</code><ul> <li><code>report/&lt;sample-id&gt;.*{html,json}</code>: report files in different formats.</li> <li><code>log/&lt;sample-id&gt;.*{html,json}</code>: log files.</li> <li><code>&lt;sample-id&gt;.fastp.fastq.gz</code>: file with the trimmed fastq reads.</li> <li><code>fail/&lt;sample-id&gt;.fail.fastq.gz</code>: file with reads that didn't suffice quality controls.</li> </ul> </li> </ul> <p>By default viralgenie will only provide the report and log files if fastp is selected. The trimmed reads can be saved by specifying <code>--save_intermediate_reads</code> or <code>--save_final_reads 'trimming'</code>. Similarly, the saving of the output reads can be enabled with <code>--save_trimmed_fail</code>.</p>"},{"location":"output/#trimmomatic","title":"Trimmomatic","text":"<p>Trimmomatic is a FASTQ pre-processing tool for quality control, trimmming of adapters, quality filtering and other features.</p> Output files <ul> <li><code>trimmomatic/</code><ul> <li><code>&lt;sample-id&gt;.fastq.gz</code>: file with the trimmed fastq reads.</li> <li><code>log/&lt;sample-id&gt;.*{html,txt,zip}</code>: log files generated by trimmomatic.</li> </ul> </li> </ul> <p>By default viralgenie will only provide the report and log files if Trimmomatic is selected. The trimmed reads can be saved by specifying <code>--save_intermediate_reads</code> or <code>--save_final_reads 'trimming'</code>.</p>"},{"location":"output/#bbduk","title":"BBDuk","text":"<p>BBDuk stands for Decontamination Using Kmers. BBDuk was developed to combine most common data-quality-related trimming, filtering, and masking operations into a single high-performance tool.</p> <p>It is used in viralgenie for complexity filtering using different algorithms. This means that it will remove reads with low sequence diversity (e.g. mono- or dinucleotide repeats).</p> Output files <ul> <li><code>bbduk/</code><ul> <li><code>log/&lt;sample-id&gt;.bbduk.log</code>: log file containing filtering statistics</li> <li><code>&lt;sample-id&gt;.fastq.gz</code>: resulting FASTQ file without low-complexity reads</li> </ul> </li> </ul> <p>By default viralgenie will only provide the log files of bbduk. The filtered reads can be saved by specifying <code>--save_intermediate_reads</code> or <code>--save_final_reads 'complexity'</code>.</p>"},{"location":"output/#hostremoval-kraken2","title":"Hostremoval-Kraken2","text":"<p>Kraken2 is a taxonomic sequence classifier that assigns taxonomic labels to DNA sequences. Kraken examines the k-mers within a query sequence and uses the information within those k-mers to query a database. That database maps -mers to the lowest common ancestor (LCA) of all genomes known to contain a given k-mer.</p> Output files <ul> <li><code>hostremoval-kraken2/</code><ul> <li><code>&lt;sample-id&gt;_kraken2_host.report.txt</code>: A profile of the aligned reads to a given host contamination database.</li> <li><code>&lt;sample-id&gt;_kraken2_host.unclassified*.fastq.gz</code>: resulting FASTQ file with reads that don't have any matches to the given host contamination database.</li> </ul> </li> </ul> <p>By default viralgenie will only provide the log files of kraken2 which are visualised in Multiqc. The filtered reads can be saved by specifying <code>--save_intermediate_reads</code> or <code>--save_final_reads 'host'</code>.</p>"},{"location":"output/#metagenomic-diversity","title":"Metagenomic Diversity","text":"<p>The results of the metagenomic diversity analysis are stored in the directory <code>metagenomic_diversity/</code>. Results are also visualised in the MultiQC report. </p>"},{"location":"output/#kraken2","title":"Kraken2","text":"<p>Kraken is a taxonomic sequence classifier that assigns taxonomic labels to DNA sequences. Kraken examines the k-mers within a query sequence and uses the information within those k-mers to query a database. That database maps -mers to the lowest common ancestor (LCA) of all genomes known to contain a given k-mer.</p> Output files <ul> <li> <p><code>metagenomic_diversity/kraken2/</code></p> <ul> <li><code>&lt;sample-id&gt;.report.txt</code>:A Kraken2 report that summarises the fraction abundance, taxonomic ID, number of Kmers, taxonomic path of all the hits in the Kraken2 run for a given sample. Will be 6 column rather than 8 if <code>--save_minimizers</code> specified.</li> <li><code>&lt;sample-id&gt;_kraken2_host.unclassified*.fastq.gz</code>: resulting FASTQ file with reads that don't have any matches to the given host contamination database.</li> <li><code>&lt;sample-id&gt;.classified.fastq.gz</code>: FASTQ file containing all reads that had a hit against a reference in the database for a given sample.</li> <li><code>&lt;sample-id&gt;.unclassified.fastq.gz</code>: FASTQ file containing all reads that did not have a hit in the database for a given sample.</li> <li><code>&lt;sample-id&gt;.classifiedreads.txt</code>: A list of read IDs and the hits each read had against the database for a given sample.</li> </ul> </li> </ul> <p>By default viralgenie will provide any classified or unclassified fastq files, specify this with --kraken2_save_reads. Similarly for the classifiedreads table, specify this with --kraken2_save_readclassification.</p>"},{"location":"output/#kaiju","title":"Kaiju","text":"<p>Kaiju is a program for sensitive taxonomic classification of high-throughput sequencing reads from metagenomic data. It is based on the Burrows-Wheeler transform and the lowest common ancestor algorithm.</p> Output files <ul> <li><code>metagenomic_diversity/kaiju/</code><ul> <li><code>&lt;sample-id&gt;.tsv</code>: Raw output from Kaiju with taxonomic rank, read ID and taxonic ID</li> <li><code>&lt;sample-id&gt;.txt</code>: A summary of the taxonomic classification of the reads in the sample.</li> </ul> </li> </ul>"},{"location":"output/#krona","title":"Krona","text":"<p>Krona is a hierarchical data visualisation tool that can be used to visualise the taxonomic classification of metagenomic data.</p> Output files <ul> <li><code>metagenomic_diversity/krona/</code><ul> <li><code>&lt;kaiju|kraken2&gt;_.html</code>: A HTML file containing the Krona visualisation of the taxonomic classification of the reads in the sample.</li> </ul> </li> </ul> <p></p>"},{"location":"output/#assembly-polishing","title":"Assembly &amp; Polishing","text":"<p>The results of the assembly processes &amp; polishing are stored in the directory <code>assembly/</code>.</p> <p>Multiple intermediate files can be genarated during the assembly process, some of them might not always be interesting to have. For this reason, there is an option to save the intermediate files with the <code>--save_intermediate_polishing</code> argument which is by default off.</p>"},{"location":"output/#assemblers","title":"Assemblers","text":"<p>Multiple assemblers [spades, trinity, megahit] can be used which have their results combined. Each assembler has its own directory in the <code>assembly/assemblers</code> directory, where there will be a subfolder for the contigs and the QC results from quast.</p> Output files <ul> <li><code>assemblers/</code><ul> <li><code>spades/&lt;spades_mode&gt;/</code><ul> <li><code>contigs/&lt;sample-id&gt;_spades.fa.gz</code>: Contigs generated by SPAdes.</li> <li><code>log/&lt;sample-id&gt;_spades.log</code>: Directory containing the log file of the spades run.</li> <li><code>quast/&lt;sample-id&gt;_spades.tsv</code>: Directory containing the QUAST report.</li> </ul> </li> <li><code>trinity/</code><ul> <li><code>contigs/&lt;sample-id&gt;_trinity.fa.gz</code>: Contigs generated by Trinity.</li> <li><code>quast/&lt;sample-id&gt;_trinity.tsv</code>: Directory containing the QUAST report.</li> </ul> </li> <li><code>megahit/</code><ul> <li><code>contigs/&lt;sample-id&gt;_megahit.fa.gz</code>: Contigs generated by Megahit.</li> <li><code>quast/&lt;sample-id&gt;_megahit.tsv</code>: Directory containing the QUAST report.</li> </ul> </li> </ul> </li> </ul> <p>Quast results are also summarised and plotted in the MultiQC report.</p> <p></p> <p>Finally, the results of the assemblers are combined and stored in the <code>tools_combined/</code> directory.</p> Output files <ul> <li><code>assemblers</code><ul> <li><code>tools_combined/&lt;sample-id&gt;.combined.fa</code> : Contigs generated by combining the results of the assemblers.</li> </ul> </li> </ul>"},{"location":"output/#blast","title":"BLAST","text":"<p>BLAST is a sequence comparison tool that can be used to compare a query sequence against a database of sequences. In viralgenie, BLAST is used to compare the contigs generated by the assemblers to a database of viral sequences.</p> <p>By default, viralgenie will only provide the BLAST results in a tabular format. It will have selected only for the top five hits and will also have filtered version where it will only include hits with an e-value of 0.01 or lower, a bitscore of 50 or higher and a alignment percentage of 0.80 or higher.</p> Column names <ul> <li>qseqid</li> <li>sseqid</li> <li>stitle</li> <li>pident</li> <li>qlen</li> <li>slen</li> <li>length</li> <li>mismatch</li> <li>gapopen</li> <li>qstart</li> <li>qend</li> <li>sstart</li> <li>send</li> <li>evalue</li> <li>bitscore</li> </ul> Output files <ul> <li><code>polishing/</code><ul> <li><code>blast/&lt;sample-id&gt;_filter.tsv</code>: Filtered BLAST results in tabular format.</li> <li><code>intermediate/blast/filtered-sequences/&lt;sample-id&gt;_withref.fa</code>: Contigs with the blast hit sequence in a fasta file.</li> <li><code>intermediate/blast/hits/&lt;sample-id&gt;.txt</code>: unfiltered BLAST results in tabular format.</li> </ul> </li> </ul> <p>By default viralgenie will only provide the filtered blast.txt file. The intermediate files can be saved by specifying <code>--save_intermediate_polishing</code>.</p>"},{"location":"output/#preclustering-kaiju-kraken2","title":"Preclustering - Kaiju &amp; Kraken2","text":"<p>Kaiju is a program for sensitive taxonomic classification of high-throughput sequencing reads from metagenomic data. It is based on the Burrows-Wheeler transform and the lowest common ancestor algorithm.</p> <p>Kraken2 is a taxonomic sequence classifier that assigns taxonomic labels to DNA sequences. Kraken examines the k-mers within a query sequence and uses the information within those k-mers to query a database. That database maps -mers to the lowest common ancestor (LCA) of all genomes known to contain a given k-mer.</p> Output files <ul> <li><code>polishing/intermediate/precluster</code><ul> <li><code>kaiju/&lt;sample-id&gt;_kaiju.tsv</code>: Raw output from Kaiju with taxonomic rank, read ID and taxonic ID</li> <li><code>kraken2/&lt;sample-id&gt;_kraken2_reports.txt</code>: A Kraken2 report that summarises the fraction abundance, taxonomic ID, number of Kmers, taxonomic path of all the hits in the Kraken2 run for a given sample. Will be 6 column rather than 8 if <code>--save_minimizers</code> specified.</li> <li><code>kraken2/&lt;sample-id&gt;_kraken2.classifiedreads.txt</code>: A list of read IDs and the hits each read had against the database for a given sample.</li> <li><code>merged_classifications/&lt;sample-id&gt;_merged.tsv</code>: Taxonomy merged based on the specified strategy with the columns being taxonomic rank, read ID and taxonic ID.</li> <li><code>sequences/&lt;sample-id&gt;/&lt;sample-id&gt;_taxid&lt;taxonic ID&gt;.fa</code>: Fasta file with the contigs that were classified to that specific taxonomic ID.</li> </ul> </li> </ul> <p>By default viralgenie will not provide any preclustering file. The intermediate files can be saved by specifying <code>--save_intermediate_polishing</code>.</p>"},{"location":"output/#clustering","title":"Clustering","text":"<p>The output files of each clustering method is directly put in te <code>assembly/polishing</code> directory. With the exception of a summary file that is generated by the pipeline for each cluster with the size of the cluster, the centroid, ... .</p> Output files <ul> <li><code>polishing/intermediate/cluster/</code><ul> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_cl#.json</code>: A json file containign information that is used by viralgenie internally like clusterID, centroid, members, cluster size, if the reference is external or a denovo contig.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;.summary_mqc.tsv</code>: A tabular file with comments used for Multiqc with statistics on the number of identified clusters in a sample</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;.clusters.tsv</code>: A tabular file with metadata on all clusters in a samples. It's the json file of all clusters in a table format.</li> </ul> </li> </ul> <p>Tip</p> <p>Whenever there is a 'cl#' in the file name, it refers to the cluster number of that sample.</p> <p>By default viralgenie will not provide any clustering overview files. The intermediate files can be saved by specifying <code>--save_intermediate_polishing</code>.</p>"},{"location":"output/#cd-hit-est","title":"CD-HIT-EST","text":"<p>CD-HIT is a very fast, widely used program for clustering and comparing protein or nucleotide sequences.</p> Output files <ul> <li><code>polishing/cdhit/</code><ul> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;.fa.clstr</code>: A cluster file containing the clustering information. where  \"&gt;\" starts a new cluster, a \"*\" at the end means that this sequence is the representative or centroid of this cluster, and a \"%\" is the identity between this sequence and the representative</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;.fa</code>: A fasta file containing the centroid sequence.</li> </ul> </li> </ul>"},{"location":"output/#vsearch-cluster","title":"vsearch-cluster","text":"<p>vsearch implements a single-pass, greedy centroid-based clustering algorithm, similar to the algo- rithms implemented in usearch, DNAclust and sumaclust for example. The output has to be in the <code>--uc</code> format or else the pipeline will not be able to process the output.</p> Output files <ul> <li><code>polishing/vsearch/</code><ul> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;.tsv.gz</code>: A cluster file containing the clustering information.</li> </ul> </li> </ul> vsearch -uc columns <ol> <li>Entry (S, H, or C):</li> <li>Record type: S, H, or C.</li> <li>Cluster number (zero-based).</li> <li>Centroid length (S), query length (H), or cluster size (C).</li> <li>Percentage of similarity with the centroid sequence (H), or set to \u2019*\u2019 (S, C).</li> <li>Match orientation + or - (H), or set to \u2019\u2019 (S, C). Not used, always set to \u2019\u2019 (S, C) or to zero (H).</li> <li>Not used, always set to \u2019*\u2019 (S, C) or to zero (H).</li> <li>Set to \u2019*\u2019 (S, C) or, for H, compact representation of the pairwise alignment using the CIGAR format (Compact Idiosyncratic Gapped Alignment Report): M (match/mismatch), D (deletion), and I (insertion). The equal sign \u2019=\u2019 indicates that the query is identical to the centroid sequence.</li> <li>Label of the query sequence (H), or of the centroid sequence (S, C). 10. Label of the centroid sequence (H), or set to \u2019*\u2019 (S, C).</li> </ol>"},{"location":"output/#mmseqs2","title":"MMseqs2","text":"<p>MMseqs2 is a software suite to search and cluster huge protein and nucleotide sequence sets. The cascaded clustering workflow (<code>mmseqs-cluster</code>) first runs linclust, the linear-time clustering module of mmseqs (<code>mmseqs-linclust</code>), that can produce clustering\u2019s down to 50% sequence identity in very short time.</p> Output files <ul> <li><code>polishing/</code><ul> <li><code>mmseqs2/&lt;sample-id&gt;/&lt;sample-id&gt;.tsv</code>: A cluster file containing the clustering information. Where the first column is the cluster representative and the second column the member.</li> <li>intermediate/mmseqs/clustered_db/*`: A MMseqs2 database of the clustered sequences. <li>intermediate/mmseqs/sequence_db/*`: A MMseqs2 database of the input sequences (contigs + blast hits)."},{"location":"output/#vrhyme","title":"vRhyme","text":"<p>vRhyme is a multi-functional tool for binning virus genomes from metagenomes. vRhyme functions by utilizing coverage variance comparisons and supervised machine learning classification of sequence features to construct viral metagenome-assembled genomes (vMAGs).</p> Output files <ul> <li><code>polishing/vrhyme</code><ul> <li><code>&lt;sample-id&gt;/vRhyme_best_bins.#.membership.tsv</code>: scaffold membership of best bins</li> <li><code>&lt;sample-id&gt;/vRhyme_best_bins.#.summary.tsv</code>: summary stats of best bins</li> </ul> </li> </ul>"},{"location":"output/#mash","title":"Mash","text":"<p>Mash calculates the distance between two sequences based on the jaccard distance. The Mash distance can be quickly computed from the size-reduced sketches alone, yet produces a result that strongly correlates with alignment-based measures such as the Average Nucleotide Identity (ANI).</p> Output files <ul> <li><code>polishing/mash</code><ul> <li><code>&lt;sample-id&gt;/dist/*.tsv</code>: A distance matrix of the genomes with ANI</li> <li><code>&lt;sample-id&gt;/cluster/*.tsv</code>: A table where the first column represents the contig/genome and the second column it's corresponding cluster.</li> <li><code>&lt;sample-id&gt;/visual/*.png</code>: A visualisation of the network.</li> </ul> </li> </ul> <p>The network of a triple segmented Hazara virus looks like this, each node represents a contig colored on cluster. The edge represents that the ANI is higher then the specified <code>--identity_threshold</code>.</p> <p></p> <p>What are those names?</p> <p>Most assemblers tend to give each contig name a specific prefix. For example,</p> <ul> <li>Trinity: <code>'TRINITY_...'</code></li> <li>SPAdes: <code>'NODE_...'</code></li> <li>Megahit: <code>'k\\d{3}_...'</code></li> </ul> <p>Based on these prefixes viralgenie separates external references from denovo contigs. If any assemblers are added, consider specifying a specific regex for <code>--assembler_patterns</code>.</p>"},{"location":"output/#minimap2","title":"Minimap2","text":"<p>Minimap2 is a versatile sequence alignment program that aligns larger DNA or mRNA sequences against a large reference database.</p> Output files <ul> <li><code>polishing/scaffolding/&lt;sample-id&gt;/minimap</code><ul> <li><code>&lt;sample-id&gt;_cl#.bam</code>: A BAM file containing the alignment of contigs to the centroid.</li> <li><code>&lt;sample-id&gt;_cl#.mmi</code>: The centroid index file.</li> </ul> </li> </ul> <p>By default viralgenie will not provide the minimap output files. The intermediate files can be saved by specifying <code>--save_intermediate_polishing</code>.</p>"},{"location":"output/#ivar-contig-consensus","title":"iVar contig consensus","text":"<p>iVar is a computational method for calling consensus sequences from viral populations.</p> Output files <ul> <li><code>polishing/scaffolding/&lt;sample-id&gt;/ivar</code><ul> <li><code>&lt;sample-id&gt;_cl#_consensus.fa</code>: A fasta file containing the consensus sequence of the cluster.</li> <li><code>&lt;sample-id&gt;_cl#_consensus.mpileup</code>: A mpileup file containing depth at each position of the consensus sequence.</li> <li><code>&lt;sample-id&gt;_cl#_consensus.qual.txt</code>: A text file contianing the quality of each base in the consensus sequence.</li> <li><code>hybrid-&lt;sample-id&gt;_cl#_consensus.fa</code>: A fasta file containing the hybrid consensus sequence of the cluster and the reference.</li> </ul> </li> </ul> <p>By default viralgenie will not provide the iVar output files. The intermediate files can be saved by specifying <code>--save_intermediate_polishing</code>.</p> <p>Info</p> <p>The hybrid consensus is generated by mapping the contigs to the reference and then calling the consensus sequence. This is done to fill in the gaps in the contigs with the reference sequence, if there are no postions with 0 coverage there will not be a hybrid consensus and the output from iVar will be used.</p>"},{"location":"output/#variant-calling-iterative-refinement","title":"Variant Calling &amp; Iterative Refinement","text":"<p>The results from variant calling, resulting from the mapping constrains &amp; the final round of polishing are stored in the directory <code>variant_calling/</code>.</p> <p>Info</p> <p>Mapping constrains are combined with the specified samples, here, the identifier of the mapping constrain combined with the sample identifier. All results will have a new prefix which is <code>&lt;sample-id&gt;_&lt;mapping_constrain_id&gt;-CONSTRAIN</code>.</p> <p>The results from the iterations are stored with the same structure as the final round of polishing in the <code>assembly/polishing/iterations/it#</code> directory.</p> <p>Info</p> <p>To be able to make a distinction between the output files of the iterations, viralgenie follows a schema where it starts from <code>singletons</code> or a <code>consensus</code> goes through the iterations and ends with the <code>variant-calling</code>. The output files will have the following structure: <pre><code>graph LR\n    F[singleton] --&gt; B[Iteration 1: 'it1']\n    A[consensus] --&gt; B[Iteration 1: 'it1']\n    B --&gt; C[Iteration 2: 'it2']\n    C --&gt; D[...]\n    D --&gt; E[Variant-calling: 'itvariant-calling']</code></pre></p> <p>The prefix of the sample is combined with the previous state of sample. For example, in the first iteration (directory <code>iterations/it1</code>), reads will be mapped to the reference-assisted de novo consensus sequence (ie <code>consensus</code>) and so the output file will be <code>assembly/polishing/iterations/it1/bwamem2/bam/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_consensus.bam</code>.</p>"},{"location":"output/#read-mapping","title":"Read mapping","text":"<p>The mapping results are stored in the directory <code>variants/mapping-info/</code> or in the iterations directory <code>assembly/polishing/iterations/it#</code>.</p> <p>If bowtie is used, the output from the raw mapping results (in addition to the results after deduplication) are included in the multiqc report.</p> <p></p> Output files - variants <ul> <li><code>variants/mapping-info/</code><ul> <li><code>bwamem2/</code><ul> <li><code>index/&lt;sample-id&gt;_&lt;constrain-id&gt;/*</code>: The index files of the consensus .</li> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>unmapped/&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.fastq.gz</code>: A fastq file containing the unmapped reads.</li> </ul> </li> <li><code>bwamem/</code><ul> <li><code>index/&lt;sample-id&gt;_&lt;constrain-id&gt;/*</code>: The index files of the consensus .</li> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>unmapped/&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.fastq.gz</code>: A fastq file containing the unmapped reads.</li> </ul> </li> <li><code>bowtie2/</code><ul> <li><code>build/&lt;sample-id&gt;_&lt;constrain-id&gt;/*</code>: The index files of the consensus .</li> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>unmapped/&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.fastq.gz</code>: A fastq file containing the unmapped reads.</li> <li><code>log/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.log</code>: A log file of the bowtie2 run.</li> </ul> </li> </ul> </li> </ul> Output files - iterations <ul> <li><code>assembly/polishing/iterations/it#/</code><ul> <li><code>bwamem2/</code><ul> <li><code>index/&lt;sample-id&gt;/*</code>: The index files of the consensus .</li> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>unmapped/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.fastq.gz</code>: A fastq file containing the unmapped reads.</li> </ul> </li> <li><code>bwamem/</code><ul> <li><code>index/&lt;sample-id&gt;/*</code>: The index files of the consensus .</li> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>unmapped/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.fastq.gz</code>: A fastq file containing the unmapped reads.</li> </ul> </li> <li><code>bowtie2/</code><ul> <li><code>build/&lt;sample-id&gt;/*</code>: The index files of the consensus .</li> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>unmapped/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.fastq.gz</code>: A fastq file containing the unmapped reads.</li> <li><code>log/&lt;sample-id&gt;_cl#_it#.log</code>: A log file of the bowtie2 run.</li> </ul> </li> </ul> </li> </ul>"},{"location":"output/#deduplication","title":"Deduplication","text":"<p>To accomodate for PCR duplicates, the reads are deduplicated. The deduplication results are stored in the directory <code>variants/mapping-info/deduplicate/</code> or in the iterations directory <code>assembly/polishing/iterations/it#/deduplicate</code>.</p> <p>Deduplication results are also visualised within the MultiQC report.</p>"},{"location":"output/#umi-tools","title":"UMI-tools","text":"<p><code>UMI-tools</code> is a set of tools for handling Unique Molecular Identifiers (UMIs) in NGS data. The deduplication is done by the <code>dedup</code> tool.</p> <p>Number of deduplicated reads: </p> <p>Summary statistics: </p> Output files - variants <ul> <li><code>variants/mapping-info/deduplicate/</code><ul> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.umi_deduplicated.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>log/</code><ul> <li><code>&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.umi_deduplicated.log</code>: A log file of the UMI-tools run.</li> <li><code>&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.umi_deduplicated_edit_distance.tsv</code>: Reports the (binned) average edit distance between the UMIs at each position..</li> <li><code>&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.umi_deduplicated_per_umi.tsv</code>: UMI-level summary statistics..</li> <li><code>&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.umi_deduplicated_per_umi_per_position.tsv</code>: Tabulates the counts for unique combinations of UMI and position..</li> </ul> </li> </ul> </li> </ul> Output files - iterations <ul> <li><code>assembly/polishing/iterations/it#/deduplicate</code><ul> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.umi_deduplicated.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>log/</code><ul> <li><code>&lt;sample-id&gt;_cl#_it#.umi_deduplicated.log</code>: A log file of the UMI-tools run.</li> <li><code>&lt;sample-id&gt;_cl#_it#.umi_deduplicated_edit_distance.tsv</code>: Reports the (binned) average edit distance between the UMIs at each position..</li> <li><code>&lt;sample-id&gt;_cl#_it#.umi_deduplicated_per_umi.tsv</code>: UMI-level summary statistics..</li> <li><code>&lt;sample-id&gt;_cl#_it#.umi_deduplicated_per_umi_per_position.tsv</code>: Tabulates the counts for unique combinations of UMI and position..</li> </ul> </li> </ul> </li> </ul>"},{"location":"output/#picard-mark-duplicates","title":"Picard - Mark Duplicates","text":"<p><code>Picard</code> is a set of command line tools for manipulating high-throughput sequencing data and formats such as SAM/BAM/CRAM and VCF. The deduplication is done by the <code>MarkDuplicates</code> tool.</p> <p></p> Output files - variants <ul> <li><code>variants/mapping-info/deduplicate/</code><ul> <li><code>picard/</code><ul> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>log/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.dedup.MarkDuplicates.metrics.txt</code>: Dedpulication metrics from Picard.</li> </ul> </li> </ul> </li> </ul> Output files - iterations <ul> <li><code>assembly/polishing/iterations/it#/deduplicate</code><ul> <li><code>picard/</code><ul> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>log/&lt;sample-id&gt;_cl#_it#.dedup.MarkDuplicates.metrics.txt</code>: Dedpulication metrics from Picard.</li> </ul> </li> </ul> </li> </ul>"},{"location":"output/#mapping-statistics","title":"Mapping statistics","text":"<p>Info</p> <p>If <code>--deduplicate</code> is set to <code>true</code> [default], all metrics will be calculated on the deduplicated bam file.</p>"},{"location":"output/#samtools","title":"Samtools","text":"<p>Samtools is a suite of programs for interacting with high-throughput sequencing data. We use samtools in this pipeline to obtain mapping statistics from three tools: <code>flagstat</code>, <code>idxstats</code> and <code>stats</code>.</p> <p> </p> Output files - variants <ul> <li><code>variants/mapping-info/metrics</code><ul> <li><code>flagstat/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.flagstat</code>: A text file containing the flagstat output.</li> <li><code>idxstats/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.idxstats</code>: A text file containing the idxstats output.</li> <li><code>stats/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.stats</code>: A text file containing the stats output.</li> </ul> </li> </ul> Output files - iterations <ul> <li><code>assembly/polishing/iterations/it#/metrics</code><ul> <li><code>flagstat/&lt;sample-id&gt;_cl#_it#.flagstat</code>: A text file containing the flagstat output.</li> <li><code>idxstats/&lt;sample-id&gt;_cl#_it#.idxstats</code>: A text file containing the idxstats output.</li> <li><code>stats/&lt;sample-id&gt;_cl#_it#.stats</code>: A text file containing the stats output.</li> </ul> </li> </ul>"},{"location":"output/#picard-collect-multiple-metrics","title":"Picard - Collect Multiple Metrics","text":"<p><code>Picard</code> is a set of command line tools for manipulating high-throughput sequencing data. We use picard-tools in this pipeline to obtain mapping and coverage metrics.</p> Output files - variants <ul> <li><code>variants/mapping-info/metrics/picard</code><ul> <li><code>*.CollectMultipleMetrics.*</code>: Alignment QC files from picard CollectMultipleMetrics in <code>*_metrics</code> textual format.</li> <li><code>*.pdf</code> plots for metrics obtained from CollectMultipleMetrics.</li> </ul> </li> </ul> Output files - iterations <ul> <li><code>assembly/polishing/iterations/it#/metrics/picard</code><ul> <li><code>*.CollectMultipleMetrics.*</code>: Alignment QC files from picard CollectMultipleMetrics in <code>*_metrics</code> textual format.</li> <li><code>*.pdf</code> plots for metrics obtained from CollectMultipleMetrics.</li> </ul> </li> </ul>"},{"location":"output/#mosdepth-coverage","title":"Mosdepth - Coverage","text":"<p>mosdepth is a fast BAM/CRAM depth calculation for WGS, exome, or targeted sequencing. mosdepth is used in this pipeline to obtain genome-wide coverage values in 200bp windows. The results are rendered in MultiQC (genome-wide coverage).</p> <p> </p> Output files - variants <ul> <li><code>variants/mapping-info/metrics/mosdepth</code>     -<ul> <li><code>&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.per-base.bed.gz</code>: A bed file containing the coverage values in 200bp windows.</li> <li><code>&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.per-base.bed.gz.csi</code>: Indexed bed file.</li> <li><code>&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.mosdepth.summary.txt</code>: Summary metrics including mean, min and max coverage values.</li> <li><code>&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.mosdepth.global.dist.txt</code>: A cumulative distribution indicating the proportion of total bases that were covered for at least a given coverage value.</li> </ul> </li> </ul> Output files - iterations <ul> <li><code>assembly/polishing/iterations/it#/metrics/mosdepth</code><ul> <li><code>&lt;sample-id&gt;_cl#_it#.per-base.bed.gz</code>: A bed file containing the coverage values in 200bp windows.</li> <li><code>&lt;sample-id&gt;_cl#_it#.per-base.bed.gz.csi</code>: Indexed bed file.</li> <li><code>&lt;sample-id&gt;_cl#_it#.mosdepth.summary.txt</code>: Summary metrics including mean, min and max coverage values.</li> <li><code>&lt;sample-id&gt;_cl#_it#.mosdepth.global.dist.txt</code>: A cumulative distribution indicating the proportion of total bases that were covered for at least a given coverage value.</li> </ul> </li> </ul>"},{"location":"output/#variant-calling-filtering","title":"Variant calling &amp; filtering","text":"<p>Variant calling is done with <code>BCFTools mpileup</code> or <code>iVar</code>, the filtering with <code>BCFtools filter</code>.</p> <p>Variant files are visualised in the MultiQC report.</p> <p></p> Output files - variants <ul> <li><code>variants/variant_calling</code><ul> <li><code>bcftools/</code><ul> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.vcf.gz</code>: A VCF file containing the variant calls.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.norm.vcf.gz</code>: A compressed VCF file where multiallelic sites are split up into biallelic records and SNPs and indels should be merged into a single record.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.filtered.vcf.gz</code>: A compressed VCF file containing the filtered variants.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.vcf.gz.tbi</code>: An index file for the compressed VCF file.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN_stats.txt</code>: A text file stats which is suitable for machine processing and can be plotted using plot-vcfstats.</li> </ul> </li> <li><code>ivar/</code><ul> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.ivar.tsv</code>: A tabular file containing the variant calls.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.ivar.vcf</code>: A VCF file containing the variant calls.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.ivar.variant_counts.log</code>: A summary file containing the number of indels and SNPs.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN.filtered.vcf.gz</code>: A compressed VCF file containing the variant calls.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN_stats.txt</code>: A text file stats which is suitable for machine processing and can be plotted using plot-vcfstats.</li> </ul> </li> </ul> </li> </ul> Output files - iterations <pre><code>- `assembly/polishing/iterations/it#/variants/variant_calling`\n    - `bcftools/`\n        - `&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.vcf.gz`: A VCF file containing the variant calls.\n        - `&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.norm.vcf.gz`: A compressed VCF file where multiallelic sites are split up into biallelic records and SNPs and indels shoul\u00a0d be merged into a single record.\n        - `&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.filtered.vcf.gz`: A compressed VCF file containing the filtered variants.\n        - `&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.vcf.gz.tbi`: An index file for the compressed VCF file.\n        - `&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.stats.txt`: A text file stats which is suitable for machine processing and can be plotted using plot-vcfstats.\n    - `ivar/`\n        - `&lt;sample-id&gt;_cl#_it#.ivar.tsv`: A tabular file containing the variant calls.\n        - `&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.ivar.vcf`: A VCF file containing the variant calls.\n        - `&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.ivar.variant_counts.log`: A summary file containing the number of indels and SNPs.\n        - `&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.filtered.vcf.gz`: A compressed VCF file containing the variant calls.\n        - `&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.stats.txt`: A text file stats which is suitable for machine processing and can be plotted using plot-vc&lt;sample-id&gt;/fstats.\n</code></pre>"},{"location":"output/#consensus-generation","title":"Consensus generation","text":"<p>The consensus sequences are generated by <code>BCFTools</code> or <code>iVar</code>. The consensus sequences are stored in the directory <code>consensus/</code> or in the iterations directory <code>assembly/polishing/iterations/it#/consensus</code>.</p> <p><code>BCFtools</code> will use the filtered variants file whereas, <code>iVar</code> will redetermine the variants to collapse in the consensus using their own workflow, read more about their differences in the consensus calling section.</p> Output files - variants <ul> <li><code>consensus</code><ul> <li><code>seq/</code><ul> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN_itvariant_calling.fa</code>: A fasta file containing the consensus sequence.</li> </ul> </li> <li><code>mask/</code><ul> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN*.qual.txt</code>: A log file of the consensus run containing statistics. [<code>iVar</code> only]</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN*.bed</code>: A bed file containing the masked regions. [<code>BCFtools</code> only]</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constrain-id&gt;-CONSTRAIN*.mpileup</code>: A mpileup file containing information on the depth and the quality of each alinged base.</li> </ul> </li> </ul> </li> </ul> Output files - iterations <pre><code>- `assembly/polishing/iterations/it#/consensus`\n    - `seq/`\n        - `&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_itvariant_calling.fa`: A fasta file containing the consensus sequence.\n    - `mask/`\n        - `&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it*.qual.txt`: A log file of the consensus run containing statistics. [`iVar` only]\n        - `&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it*.bed`: A bed file containing the masked regions. [`BCFtools` only]\n        - `&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it*.mpileup`: A mpileup file containing information on the depth and the quality of each alinged base.\n</code></pre>"},{"location":"output/#consensus-quality-control","title":"Consensus Quality control","text":"<p>Consensus quality control is done with multiple tools, the results are stored in the directory <code>consensus/quality_control/</code>.</p>"},{"location":"output/#quast","title":"Quast","text":"<p>QUAST is a quality assessment tool for genome assemblies. It calculates various metrics such as N50, L50, number of contigs, number of mismatches, number of indels, etc.</p> Output files <ul> <li><code>consensus/quality_control/quast/</code><ul> <li><code>&lt;sample-id&gt;/&lt;iteration&gt;/&lt;sample-id&gt;_&lt;cl# | constrain-id&gt;.tsv</code>: A tabular file containing the QUAST report.</li> </ul> </li> </ul> <p>If no iterative refiment was ran, the output will be in the <code>consensus/quality_control/quast/&lt;sample-id&gt;/constrain</code> directory.</p>"},{"location":"output/#checkv","title":"CheckV","text":"<p><code>CheckV</code> is a tool for assessing the quality of viral genomes recovered from metagenomes. It calculates various metrics such as the number of viral genes, the number of viral contigs, the number of viral genomes, etc.</p> Output files <ul> <li><code>consensus/quality_control/checkv/</code><ul> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;cl# | constrain-id&gt;/quality_summary.tsv</code>: A tabular file that integrates the results from the three main modules of checkv and should be the main output referred to.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;cl# | constrain-id&gt;/completeness.tsv</code>: A detailed overview of how completeness was estimated.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;cl# | constrain-id&gt;/contamination.tsv</code>: A detailed overview of how contamination was estimated.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;cl# | constrain-id&gt;/complete_genomes.tsv</code>: A detailed overview of putative genomes identified.</li> </ul> </li> </ul>"},{"location":"output/#blastn","title":"BLASTn","text":"<p>BLAST is a tool for comparing primary biological sequence information. The output from the BLAST run is stored in the directory <code>consensus/quality_control/blast/</code>. Final consensus genomes are searched against the <code>--reference_pool</code>.</p> Column names <ul> <li>qseqid</li> <li>sseqid</li> <li>stitle</li> <li>pident</li> <li>qlen</li> <li>slen</li> <li>length</li> <li>mismatch</li> <li>gapopen</li> <li>qstart</li> <li>qend</li> <li>sstart</li> <li>send</li> <li>evalue</li> <li>bitscore</li> </ul> Output files <ul> <li><code>consensus/quality_control/blast/</code><ul> <li><code>&lt;sample-id&gt;/&lt;iteration&gt;/&lt;sample-id&gt;_&lt;cl# | constrain-id&gt;.txt</code>: A tabular file containing the BLAST report.</li> </ul> </li> </ul> <p>If no iterative refiment was ran, the output will be in the <code>consensus/quality_control/blast/&lt;sample-id&gt;/constrain</code> directory.</p>"},{"location":"output/#mmseqs-search-annotation","title":"MMseqs-search (annotation)","text":"<p>MMseqs-search is a ultra fast and sensitive search tool for protein and nucleotide databases. Viralgenie uses MMseqs to search the consensus genomes in a annotated database, like Virousarus (see also defining your own custom annotation database), and uses the annotation data of the best hit to assign the consensus genome a species name, segment name, expected host and any other metadata that is embedded within the database.</p> Column names <ul> <li>qseqid</li> <li>sseqid</li> <li>stitle</li> <li>pident</li> <li>qlen</li> <li>slen</li> <li>length</li> <li>mismatch</li> <li>gapopen</li> <li>qstart</li> <li>qend</li> <li>sstart</li> <li>send</li> <li>evalue</li> <li>bitscore</li> </ul> Output files <ul> <li><code>consensus/quality_control/mmseqs-search/all_genomes_annotation.hits.tsv</code>: A tabular file containing the MMseqs-search hits, all genomes are combined to reduce the number of jobs.</li> </ul>"},{"location":"output/#mafft","title":"MAFFT","text":"<p>MAFFT is a multiple sequence alignment program for amino acid or nucleotide sequences. The output from the MAFFT run is stored in the directory <code>consensus/quality_control/mafft/</code>.</p> <p>It is used to align the following genomic data: - The final consensus genome - The identified reference genome from <code>--reference_pool</code> - The denovo contigs from each assembler (that constituted the final consensus genome) - Each consensus genome from the iterative refinement steps.</p> Output files <ul> <li><code>consensus/quality_control/mafft/</code><ul> <li><code>&lt;sample-id&gt;/&lt;sample_id&gt;_cl#_iterations.fas</code>: A fasta file containing a multiple sequence alignment of only the iterations.</li> <li><code>&lt;sample-id&gt;/&lt;sample_id&gt;_cl#_aligned.fas</code>: A fasta file containing a multiple sequence alignment of the denovo contigs, the reference from reference_pool and the consensus from iteratations.</li> </ul> </li> </ul> <p>Alignment can then be opened with MSA viewer, for example Jalview</p> <p></p>"},{"location":"output/#multiqc","title":"MultiQC","text":"<p>MultiQC is a visualization tool that generates a single HTML report summarising all samples in your project. Most of the pipeline QC results are visualised in the report and further statistics are available in the report data directory.</p> <p>Results generated by MultiQC collate pipeline QC from supported tools e.g. FastQC. The pipeline has special steps which also allow the software versions to be reported in the MultiQC output for future traceability. For more information about how to use MultiQC reports, see http://multiqc.info.</p> <p>Furthermore, viralgenie runs MultiQC 2 times, as it uses the output from multiqc to create multiple summary tables of the consensus genomes and their iterations.</p> Output files <ul> <li><code>multiqc/</code><ul> <li><code>custom_tables/</code>: a directory with a set of commented TSV (comments taken from <code>--multiqc_comment_headers</code>) that summarise aspects of the pipeline runs.</li> <li><code>multiqc_report.html</code>: a standalone HTML file that can be viewed in your web browser.</li> <li><code>multiqc_data/</code>: directory containing parsed statistics from the different tools used in the pipeline.</li> <li><code>multiqc_dataprep/</code>: preparation files for the generated custom tables.</li> <li><code>multiqc_plots/</code>: directory containing static images from the report in various formats.</li> </ul> </li> </ul>"},{"location":"output/#pipeline-information","title":"Pipeline information","text":"Output files <ul> <li><code>pipeline_info/</code><ul> <li>Reports generated by Nextflow: <code>execution_report.html</code>, <code>execution_timeline.html</code>, <code>execution_trace.txt</code> and <code>pipeline_dag.dot</code>/<code>pipeline_dag.svg</code>.</li> <li>Reports generated by the pipeline: <code>pipeline_report.html</code>, <code>pipeline_report.txt</code> and <code>software_versions.yml</code>. The <code>pipeline_report*</code> files will only be present if the <code>--email</code> / <code>--email_on_fail</code> parameter's are used when running the pipeline.</li> <li>Reformatted samplesheet files used as input to the pipeline: <code>samplesheet.valid.csv</code>.</li> <li>Parameters used by the pipeline run: <code>params.json</code>.</li> </ul> </li> </ul> <p>Nextflow provides excellent functionality for generating various reports relevant to the running and execution of the pipeline. This will allow you to troubleshoot errors with the running of the pipeline, and also provide you with other information such as launch commands, run times and resource usage.</p>"},{"location":"parameters/","title":"Viralgenie pipeline parameters","text":"<p>A pipeline to reconstruct consensus genomes and identify intrahost variants from metagenomic sequencing data or enriched based sequencing data like hybrid capture.</p>"},{"location":"parameters/#inputoutput-options","title":"Input/output options","text":"<p>Define where the pipeline should find input data and save output data.</p> Parameter Description Default <code>input</code> Path to comma-separated file containing information about the samples in the experiment. HelpYou will need to create a design file with information about the samples in your experiment before running the pipeline. Use this parameter to specify its location. It has to be a comma-separated file with 3 columns, and a header row. See usage docs. <code>outdir</code> The output directory where the results will be saved. You have to use absolute paths to storage on Cloud infrastructure. <code>metadata</code> Sample metadata that is included in the multiqc report <code>email</code> Email address for completion summary. HelpSet this parameter to your e-mail address to get a summary e-mail with details of the run sent to you when the workflow exits. If set in your user config file (<code>~/.nextflow/config</code>) then you don't need to specify this on the command line for every run."},{"location":"parameters/#preprocessing-options","title":"Preprocessing options","text":"<p>Options related to the trimming, low complexity and host removal steps of the reads</p> Parameter Description Default <code>skip_preprocessing</code> Skip read preprocessing and use input reads for downstream analysis <code>skip_fastqc</code> Skip read quality statistics summary tool 'fastqc' <code>save_final_reads</code> Save reads after the final preprocessing step True <code>save_intermediate_reads</code> Save reads after every preprocessing step <code>with_umi</code> With or without umi detection <code>skip_umi_extract</code> With or without umi extraction True <code>umi_discard_read</code> Discard R1 / R2 if required 0 <code>trim_tool</code> The used trimming tool fastp <code>skip_trimming</code> Skip read trimming <code>fastp_deduplicate</code> Use Fastp's deduplicate option <code>fastp_dedup_accuracy</code> Define the accuracy used for hashes while deduplicating with faspt <code>adapter_fasta</code> Fasta file of adapters <code>save_trimmed_fail</code> Specify true to save files that failed to pass trimming thresholds ending in <code>*.fail.fastq.gz</code> <code>save_merged</code> Specify true to save all merged reads to the a file ending in <code>*.merged.fastq.gz</code> <code>min_trimmed_reads</code> Inputs with fewer than this reads will be filtered out of the \"reads\" output channel 1 <code>skip_complexity_filtering</code> Skip filtering of low complexity regions in reads HelpLow-complexity sequences are defined as having commonly found stretches of nucleotides with limited information content (e.g. the dinucleotide repeat CACACACACA). Such sequences can produce a large number of high-scoring but biologically insignificant results in database searches True <code>contaminants</code> Reference files containing adapter and/or contaminant sequences for sequence kmer matching (used by bbduk) <code>skip_hostremoval</code> Skip the removal of host read sequences <code>host_k2_db</code> Kraken2 database used to remove host and conamination s3://ngi-igenomes/test-data/viralrecon/kraken2_human.tar.gz <code>host_k2_library</code> Kraken2 library(s) required to remove host and contamination HelpOnly used when no host kraken2 database is specified. human <code>skip_host_fastqc</code> Skip the fastqc step after host &amp; contaminants were removed True"},{"location":"parameters/#metagenomic-diveristy","title":"Metagenomic diveristy","text":"<p>Parameters used to determine the metagenomic diversity of the sample</p> Parameter Description Default <code>skip_metagenomic_diversity</code> Skip determining the metagenomic diversity of the sample <code>save_databases</code> Save the used databases <code>skip_kraken2</code> Skip the use of Kraken2 to determine metagenomic diversity <code>kraken2_db</code> Location of the Kraken2 database https://genome-idx.s3.amazonaws.com/kraken/k2_viral_20230314.tar.gz <code>kraken2_save_reads</code> Save classified and unclassified reads  as fastq files <code>kraken2_save_readclassification</code> Save summary overview of read classifications in a txt file <code>kraken2_save_minimizers</code> Save kraken2's used minimizers <code>skip_bracken</code> Skip recalculation of taxa abundance using bracken True <code>bracken_db</code> Location of bracken database https://genome-idx.s3.amazonaws.com/kraken/k2_viral_20230314.tar.gz <code>skip_kaiju</code> kip the use of Kaiju to determine metagenomic diversity <code>kaiju_db</code> Location of Kaiju database https://kaiju-idx.s3.eu-central-1.amazonaws.com/2023/kaiju_db_rvdb_2023-05-26.tgz <code>kaiju_taxon_rank</code> Level of taxa rank that needs to be determined species"},{"location":"parameters/#assembly","title":"Assembly","text":"<p>Parameters relating to the used assembly methods</p> Parameter Description Default <code>skip_assembly</code> Skip de novo assembly of reads <code>assemblers</code> The specified tools for denovo assembly, multiple options are possible spades,trinity,megahit <code>spades_mode</code> specific SPAdes mode to run rnaviral <code>spades_hmm</code> File or directory with amino acid HMMs for Spades HMM-guided mode. <code>spades_yml</code> Path to yml file containing read information. HelpThe raw FASTQ files listed in this YAML file MUST be supplied to the respective illumina/pacbio/nanopore input channel(s) in addition to this YML. File entries in this yml must contain only the file name and no paths. <code>assembler_patterns</code> Regex pattern to identify contigs that have been made by the assemblers"},{"location":"parameters/#polishing","title":"Polishing","text":"<p>Parameters relating to the refinement of denovo contigs</p> Parameter Description Default <code>skip_polishing</code> Skip the refinement/polishing of contigs through reference based scaffolding and read mapping <code>save_intermediate_polishing</code> Save intermediate polishing files HelpThere are multiple processes within the polishing subworkflow that might not contain relevant information   <code>reference_pool</code> Set of fasta sequences used as potential references for the contigs https://rvdb.dbi.udel.edu/download/C-RVDBvCurrent.fasta.gz <code>skip_precluster</code> Skip the preclustering of assemblies to facilitate downstream processing of assemblies <code>keep_unclassified</code> Keep the contigs that could not be classified with the taxonomic databases (<code>kaiju_db</code> &amp; <code>kraken2_db</code>) HelpWithin the preclustering step, all contigs will get a taxonomic classification using the provided databases for the metagenomic tools. In some cases, the number of unclassified contigs, can be very large if the database is restrictive. This will result in large clusters in downstream processing that can take up a lot of resources despite not being a priority in some analyses. So set it to <code>True</code> if you want to keep unclassified contigs and set it to <code>False</code> if you don't want to keep them.  True <code>taxon_merge_strategy</code> Taxon conflict resolution mode, must be 1 (Kaiju), 2 (Kraken),  lca, or lowest. HelpThe option -c determines the method of resolving conflicts in the taxonomic assignment for a read.Possible values are '1', '2', 'lca', 'lowest':  '1' -&gt; the taxon id from Kaiju is used.  '2' -&gt; the taxon id from Kraken is used.  'lca' -&gt; the least common ancestor of the two taxon ids from both input files is used.  'lowest' -&gt; the lower rank of the two taxa is used if they are within the same lineage. Otherwise the LCA is used. lca <code>cluster_method</code> Cluster algorithm used for contigs mash <code>network_clustering</code> (only with mash) Algorithm to partition the network. HelpMash creates a distance matrix that gets translated into a network of connectected nodes where the edges represent the similarity. This network is then split up using the specified method. - leiden algorithm: a hierarchical clustering algorithm, that recursively merges communities into single nodes by greedily optimizing the modularity - [connected_components] algorithm: a clustering algorithm that defines the largest possible communities where each node within a subset is reachable from every other node in the same subset via any edge . connected_components <code>skip_hybrid_consensus</code> Skip creation of the hybrid consensus, instead keep the scaffold with ambiguous bases if the depth of scaffolds is not high enough. <code>identity_threshold</code> Identity threshold value used in clustering algorithms 0.6 <code>min_contig_size</code> Minimum allowed contig size HelpSetting this to a low value will result in a large number of questionable contigs and an increase in computation time  500 <code>max_contig_size</code> Maximum allowed contig size 10000000 <code>max_n_perc</code> 50 <code>skip_singleton_filtering</code> Skip the filtering of contigs that did not cluster together with other contigs HelpSetting this to true will cause the pipeline not to remove contigs that don't have similar contigs. Filtering settings can be further specified with <code>min_contig_size</code> and <code>max_n_100kbp</code>."},{"location":"parameters/#iterative-consensus-refinement","title":"Iterative consensus refinement","text":"<p>Define parameters for iterations to update denovo consensus using  reference based improvements</p> Parameter Description Default <code>skip_iterative_refinement</code> Don't realign reads to consensus sequences and redefine the consensus through (multiple) iterations <code>iterative_refinement_cycles</code> number of iterations 2 <code>intermediate_mapper</code> mapping tool used during iterations bwamem2 <code>intermediate_variant_caller</code> variant caller used during iterations ivar <code>call_intermediate_variants</code> call variants during the iterations HelpWill always be done when iterative consensus caller is bcftools <code>intermediate_consensus_caller</code> consensus tool used for calling new consensus during iterations bcftools <code>intermediate_mapping_stats</code> calculate summary statistics during iterations True"},{"location":"parameters/#variant-analysis","title":"Variant analysis","text":"<p>Parameters relating to the analysis of variants associated to contigs and scaffolds</p> Parameter Description Default <code>skip_variant_calling</code> Skip the analysis of variants for the external reference or contigs <code>mapper</code> Define which mapping tool needs to be used when mapping reads to reference bwamem2 <code>mapping_constrains</code> Sequence to use as a mapping reference instead of the de novo contigs or scaffolds <code>deduplicate</code> deduplicate the reads HelpIf used with umi's, <code>umi tools</code> will be used to group and call consensus of each indiual read group. If not used with umi's use <code>PicardsMarkDuplicates</code>.  True <code>variant_caller</code> ivar <code>consensus_caller</code> consensus tool used for calling new consensus in final iteration ivar <code>umi_separator</code> UMI seperator in fastq header. HelpIf you have used an alternative method which does not separate the read id and UMI with a \u201c_\u201d, such as bcl2fastq which uses \u201c:\u201d, you can specify the separator with the option --umi_separator=, replacing  with e.g \u201c:\u201d. : <code>min_mapped_reads</code> 200 <code>mapping_stats</code> calculate summary statistics in final iteration True <code>multiqc_comment_headers</code> Directory containing the mutliqc headers for multiple tables like 'clusters_summary_mqc.txt', 'blast_mqc.txt', ... ${projectDir}/assets/mqc_comment <code>ivar_header</code>"},{"location":"parameters/#consensus-qc","title":"Consensus QC","text":"<p>Apply different quality control techniques on the generated consensus genomes</p> Parameter Description Default <code>skip_consensus_qc</code> Skip the quality measurements on consensus genomes <code>skip_checkv</code> Skip the use of checkv for quality check <code>checkv_db</code> Reference database used by checkv for consensus quality control HelpIf not given, the most recent one is downloaded. <code>skip_annotation</code> Skip the annotation of the consensus constructs <code>annotation_db</code> Database used for annotation of the cosensus constructs HelpThe metada fields are stored in the fasta comment as <code>key1:\"value1\"|key2:\"value2\"|...</code> see docs/databases.md for more information. https://viralzone.expasy.org/resources/Virosaurus/2020%5F4/virosaurus90%5Fvertebrate-20200330.fas.gz <code>skip_quast</code> Skip the use of QUAST for quality check <code>skip_blast_qc</code> Skip the blast search of contigs to the provided reference DB <code>skip_alignment_qc</code> Skip creating an alignment of each the collapsed clusters and each iterative step <code>mmseqs_searchtype</code> Specify the search algorithm to use for mmseqs. 0: auto 1: amino acid, 2: translated, 3: nucleotide, 4: translated nucleotide alignment HelpOnly search-type 3 supports both forward and reverse search1 - BLASTP;2 - TBLASTN;3 - BLASTN;4 - TBLASTX 4"},{"location":"parameters/#institutional-config-options","title":"Institutional config options","text":"<p>Parameters used to describe centralised config profiles. These should not be edited.</p> Parameter Description Default <code>custom_config_version</code> Git commit id for Institutional configs. master <code>custom_config_base</code> Base directory for Institutional configs. HelpIf you're running offline, Nextflow will not be able to fetch the institutional config files from the internet. If you don't need them, then this is not a problem. If you do need them, you should download the files from the repo and tell Nextflow where to find them with this parameter. https://raw.githubusercontent.com/nf-core/configs/master <code>config_profile_name</code> Institutional config name. <code>config_profile_description</code> Institutional config description. <code>config_profile_contact</code> Institutional config contact information. <code>config_profile_url</code> Institutional config URL link."},{"location":"parameters/#max-job-request-options","title":"Max job request options","text":"<p>Set the top limit for requested resources for any single job.</p> Parameter Description Default <code>max_cpus</code> Maximum number of CPUs that can be requested for any single job. HelpUse to set an upper-limit for the CPU requirement for each process. Should be an integer e.g. <code>--max_cpus 1</code> 16 <code>max_memory</code> Maximum amount of memory that can be requested for any single job. HelpUse to set an upper-limit for the memory requirement for each process. Should be a string in the format integer-unit e.g. <code>--max_memory '8.GB'</code> 128.GB <code>max_time</code> Maximum amount of time that can be requested for any single job. HelpUse to set an upper-limit for the time requirement for each process. Should be a string in the format integer-unit e.g. <code>--max_time '2.h'</code> 240.h"},{"location":"parameters/#generic-options","title":"Generic options","text":"<p>Less common options for the pipeline, typically set in a config file.</p> Parameter Description Default <code>help</code> Display help text. <code>version</code> Display version and exit. <code>publish_dir_mode</code> Method used to save pipeline results to output directory. HelpThe Nextflow <code>publishDir</code> option specifies which intermediate files should be saved to the output directory. This option tells the pipeline what method should be used to move these files. See Nextflow docs for details. copy <code>email_on_fail</code> Email address for completion summary, only when pipeline fails. HelpAn email address to send a summary email to when the pipeline is completed - ONLY sent if the pipeline does not exit successfully. <code>plaintext_email</code> Send plain-text email instead of HTML. <code>max_multiqc_email_size</code> File size limit when attaching MultiQC reports to summary emails. 25.MB <code>monochrome_logs</code> Do not use coloured log outputs. <code>hook_url</code> Incoming hook URL for messaging service HelpIncoming hook URL for messaging service. Currently, MS Teams and Slack are supported. <code>multiqc_title</code> MultiQC report title. Printed as page header, used for filename if not otherwise specified. <code>multiqc_config</code> Custom config file to supply to MultiQC. <code>multiqc_logo</code> Custom logo file to supply to MultiQC. File name must also be set in the MultiQC config file <code>multiqc_methods_description</code> Custom MultiQC yaml file containing HTML including a methods description. <code>clean_output_on_error</code> Delete the output directory if the pipeline fails <code>custom_table_headers</code> Custom yaml file contian g the table column names selection and new names. ${projectDir}/assets/custom_table_headers.yml <code>validate_params</code> Boolean whether to validate parameters against the schema at runtime True <code>validationShowHiddenParams</code> Show all params when using <code>--help</code> HelpBy default, parameters set as hidden in the schema are not shown on the command line when a user runs with <code>--help</code>. Specifying this option will tell the pipeline to show all parameters. <code>validationFailUnrecognisedParams</code> Validation of parameters fails when an unrecognised parameter is found. HelpBy default, when an unrecognised parameter is found, it returns a warinig. <code>validationSchemaIgnoreParams</code> global_prefix <code>validationLenientMode</code> Validation of parameters in lenient more. HelpAllows string values that are parseable as numbers or booleans. For further information see JSONSchema docs. <code>prefix</code> Prefix of all output files followed by [date][pipelineversion][runName] HelpUse '--global_prefix' to not have metadata embedded. <code>global_prefix</code> Global prefix set if you don't want metadata embedded in the prefix"},{"location":"usage/","title":"Usage","text":"<p>Try out the pipeline right now!</p> <pre><code>nextflow run Joon-Klaps/viralgenie -profile test,docker\n</code></pre> <p>Make sure you have Nextflow and a container manager (for example, Docker) installed. See the installation instructions for more info.</p> <p>Tip</p> <p>Did your analysis fail? After fixing the issue add <code>-resume</code> to the command to continue from where it left off.</p>"},{"location":"usage/#input","title":"Input","text":""},{"location":"usage/#samples","title":"Samples","text":"<p>The pipeline requires a samplesheet as input. This samplesheet should contain the name and the absolute locations of reads.</p> <pre><code>--input '[path to samplesheet file]'\n</code></pre> <p>The pipeline will auto-detect whether a sample is single- or paired-end using the information provided in the samplesheet (i.e. if the <code>fastq_2</code> column is empty, the sample is assumed to be single-end).</p> <p>An example samplesheet file consisting of both single- and paired-end data may look something like the one below.</p> TSVCSVYAMLJSON input-samplesheet.tsv<pre><code>sample  fastq_1 fastq_2\nsample1 AEG588A1_S1_L002_R1_001.fastq.gz    AEG588A1_S1_L002_R2_001.fastq.gz\nsample2 AEG588A5_S5_L003_R1_001.fastq.gz\nsample3 AEG588A3_S3_L002_R1_001.fastq.gz    AEG588A3_S3_L002_R2_001.fastq.gz\n</code></pre> input-samplesheet.csv<pre><code>sample,fastq_1,fastq_2\nsample1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\nsample2,AEG588A5_S5_L003_R1_001.fastq.gz,\nsample3,AEG588A3_S3_L002_R1_001.fastq.gz,AEG588A3_S3_L002_R2_001.fastq.gz\n</code></pre> input-samplesheet.yaml<pre><code>- sample: sample1\nfastq1: AEG588A1_S1_L002_R1_001.fastq.gz\nfastq2: AEG588A1_S1_L002_R2_001.fastq.gz\n- sample: sample2\nfastq1: AEG588A5_S5_L003_R1_001.fastq.gz\n- sample: sample3\nfastq1: AEG588A3_S3_L002_R1_001.fastq.gz\nfastq2: AEG588A3_S3_L002_R2_001.fastq.gz\n</code></pre> samplesheet.json<pre><code>[\n    {\n        \"sample\": \"sample1\",\n        \"fastq1\": \"AEG588A1_S1_L002_R1_001.fastq.gz\",\n        \"fastq2\": \"AEG588A1_S1_L002_R2_001.fastq.gz\",\n    },\n    {\n        \"sample\": \"sample2\",\n        \"fastq1\": \"AEG588A5_S5_L003_R1_001.fastq.gz\",\n    },\n    {\n        \"sample\": \"sample3\",\n        \"fastq1\": \"AEG588A3_S3_L002_R1_001.fastq.gz\",\n        \"fastq2\": \"AEG588A3_S3_L002_R2_001.fastq.gz\",\n    }\n]\n</code></pre> Value Description <code>sample</code> Custom sample name, needs to be unique <code>fastq_1</code> Full path (not relative paths) to FastQ file for Illumina short reads 1. File has to be gzipped and have the extension \".fastq.gz\" or \".fq.gz\". <code>fastq_2</code> Full path (not relative paths) to FastQ file for Illumina short reads 2. File has to be gzipped and have the extension \".fastq.gz\" or \".fq.gz\"."},{"location":"usage/#mapping-constrains","title":"Mapping constrains","text":"<p>Viralgenie can in addition to constructing denovo consensus genomes map the sample reads to a series of references. These references are provided through the parameter <code>--mapping_constrains</code>.</p> <p>An example mapping constrain samplesheet file consisting of 5 references, may look something like the one below.</p> <p>This is for 5 references, 2 of them being a multi-fasta file</p> TSVCSVYAMLJSON constrains-samplesheet.tsv<pre><code>id  species segment samples sequence    definition\nLassa-L-dataset LASV    L       LASV_L.multi.fasta  Collection of LASV sequences used for hybrid capture bait design, all publicly availble sequences of the L segment clustered at 99.5% similarity\nLassa-S-dataset LASV    S   sample1;sample3 LASV_S.multi.fasta  Collection of LASV sequences used for hybrid capture bait design, all publicly availble sequences of the S segment clustered at 99.5% similarity\nNC038709.1  HAZV    L   sample1;sample2 L-NC_038709.1.fasta Hazara virus isolate JC280 segment L, complete sequence.\nNC038710.1  HAZV    M       M-NC_038710.1.fasta Hazara virus isolate JC280 segment M, complete sequence.\nNC038711.1  HAZV    S       S-NC_038711.1.fasta Hazara virus isolate JC280 segment S, complete sequence.\n</code></pre> constrains-samplesheet.csv<pre><code>'id','species','segment','samples','sequence','definition'\n'Lassa-L-dataset','LASV','L','','LASV_L.multi.fasta','Collection of LASV sequences used for hybrid capture bait design, all publicly availble sequences of the L segment clustered at 99.5% similarity'\n'Lassa-S-dataset','LASV','S','sample1;sample3','LASV_S.multi.fasta','Collection of LASV sequences used for hybrid capture bait design, all publicly availble sequences of the S segment clustered at 99.5% similarity'\n'NC038709.1','HAZV','L','sample1;sample2','L-NC_038709.1.fasta','Hazara virus isolate JC280 segment L, complete sequence.'\n'NC038710.1','HAZV','M','','M-NC_038710.1.fasta','Hazara virus isolate JC280 segment M, complete sequence.'\n'NC038711.1','HAZV','S','','S-NC_038711.1.fasta','Hazara virus isolate JC280 segment S, complete sequence.'\n</code></pre> constrains-samplesheet.yaml<pre><code>- id: Lassa-L-dataset\nspecies: LASV\nsegment: L\nsequence: LASV_L.multi.fasta\ndefinition: 'Collection of LASV sequences used for hybrid capture bait design, all publicly availble sequences of the L segment clustered at 99.5% similarity'\n- id: Lassa-S-dataset\nspecies: LASV\nsegment: S\nsamples: sample1;sample3\nsequence: LASV_S.multi.fasta\n        definition: 'Collection of LASV sequences used for hybrid capture bait design, all publicly availble sequences of the S segment clustered at 99.5% similarity'\n- id: NC038709.1\nspecies: HAZV\nsegment: L\nsamples: sample1;sample2\nsequence: L-NC_038709.1.fasta\n        definition: 'Hazara virus isolate JC280 segment L, complete sequence.'\n- id: NC038710.1\nspecies: HAZV\nsegment: M\nsequence: M-NC_038710.1.fasta\ndefinition: 'Hazara virus isolate JC280 segment M, complete sequence.'\n- id: NC038711.1\nspecies: HAZV\nsegment: S\nsequence: S-NC_038711.1.fasta\ndefinition: 'Hazara virus isolate JC280 segment S, complete sequence.'\n</code></pre> <p>Warning</p> <p>JSON format is not supported for mapping constrains samplesheet.</p> Column Description <code>id</code> Reference identifier, needs to be unique' <code>species</code> [Optional] Species name of the reference <code>segment</code> [Optional] Segment name of the reference <code>samples</code> [Optional] List of samples that need to be mapped towards the reference. If empty, map all samples. <code>sequence</code> Full path (not relative paths) to the reference sequence file. <code>definition</code> [Optional] Definition of the reference sequence file. <p>Tip</p> <ul> <li>The <code>samples</code> column is optional - if empty, all samples will be mapped towards the reference.</li> <li>Multi-fasta files can be provided and all reads will mapped to all genomes but stats will not be reported separately in the final report.</li> </ul>"},{"location":"usage/#metadata","title":"Metadata","text":"<p>Sample metadata can be provided to the pipeline with the argument <code>--metadata</code>. This metadata will not affect the analysis in anyway and is only used to annotate the final report. Any metadata can be provided as long as the first value is the <code>sample</code> value.</p> TSVCSVYAMLJSON metadata.tsv<pre><code>sample  sample_accession    secondary_sample_accession  study_accession run_alias   library_layout\nsample1 SAMN14154201    SRS6189918  PRJNA607948 vero76_Illumina.fastq   PAIRED\nsample2 SAMN14154205    SRS6189924  PRJNA607948 veroSTAT-1KO_Illumina.fastq PAIRED\n</code></pre> metadata.csv<pre><code>sample,sample_accession,secondary_sample_accession,study_accession,run_alias,library_layout\nsample1,SAMN14154201,SRS6189918,PRJNA607948,vero76_Illumina.fastq,PAIRED\nsample2,SAMN14154205,SRS6189924,PRJNA607948,veroSTAT-1KO_Illumina.fastq,PAIRED\n</code></pre> metadata.yaml<pre><code>- sample: sample1\nsample_accession: SAMN14154201\nsecondary_sample_accession: SRS6189918\nstudy_accession: PRJNA607948\nrun_alias: vero76_Illumina.fastq\nlibrary_layout: PAIRED\n- sample: sample2\nsample_accession: SAMN14154205\nsecondary_sample_accession: SRS6189924\nstudy_accession: PRJNA607948\nrun_alias: veroSTAT-1KO_Illumina.fastq\nlibrary_layout: PAIRED\n</code></pre> <p>Warning</p> <p>JSON format is not supported for metadata samplesheet.</p>"},{"location":"usage/#running-the-pipeline","title":"Running the pipeline","text":"<p>The typical command for running the pipeline is as follows:</p> <pre><code>nextflow run Joon-Klaps/viralgenie --input ./samplesheet.csv --outdir &lt;OUTDIR&gt; -profile docker\n</code></pre> <p>This will launch the pipeline with the <code>docker</code> configuration profile. See below for more information about profiles.</p> <p>Note that the pipeline will create the following files in your working directory:</p> <pre><code>work          #(1)!\n&lt;OUTDIR&gt;      #(2)!\n.nextflow_log #(3)!\n...           #(4)!\n</code></pre> <ol> <li> <p>Directory containing the nextflow working files</p> </li> <li> <p>Finished results in specified location (defined with --outdir)</p> </li> <li> <p>Log file from Nextflow</p> </li> <li> <p>Other nextflow hidden files, eg. history of pipeline runs and old logs.</p> </li> </ol> <p>If you wish to repeatedly use the same parameters for multiple runs, rather than specifying each flag in the command, you can specify these in a params file.</p> <p>Pipeline settings can be provided in a <code>yaml</code> or <code>json</code> file via <code>-params-file &lt;file&gt;</code>.</p> <p>Warning</p> <p>Do not use <code>-c &lt;file&gt;</code> to specify parameters as this will result in errors. Custom config files specified with <code>-c</code> must only be used for tuning process resource specifications, other infrastructural tweaks (such as output directories), or module arguments (args).</p> params.jsoncommand line <p>The above pipeline run specified with a params file in yaml format:</p> <pre><code>nextflow run Joon-Klaps/viralgenie -profile docker -params-file params.yaml\n</code></pre> <p><code>params.yaml</code> will contain:</p> <pre><code>{\n    input: \"./samplesheet.csv\",\n    outdir: \"./results/\",\n    host_k2_db: \"./databases/kraken2/host\",\n    mapping_constrains: \"./mapping_constrains.tsv\",\n    cluster_method: \"mmseqs-linclust\"\n    ...\n}\n</code></pre> <pre><code>nextflow run Joon-Klaps/viralgenie -profile docker \\\n    --input ./samplesheet.csv \\\n    --outdir ./results/ \\\n    --host_k2_db ./databases/kraken2/host \\\n    --mapping_constrains ./mapping_constrains.tsv \\\n    --cluster_method 'mmseqs-linclust' \\\n    ...\n</code></pre> <p>You can also generate such <code>YAML</code>/<code>JSON</code> files via <code>nf-core launch</code>  if <code>nf-core</code> is installed. <pre><code>nf-core launch Joon-Klaps/viralgenie\n</code></pre></p> <p>Tip</p> <p>Use <code>nf-core launch</code> if it is the first time running the pipeline to explore all its features and options in an accesible way.</p>"},{"location":"usage/#updating-the-pipeline","title":"Updating the pipeline","text":"<pre><code>nextflow pull Joon-Klaps/viralgenie\n</code></pre> <p>When you run the above command, Nextflow automatically pulls the pipeline code from GitHub and stores it as a cached version. When running the pipeline after this, it will always use the cached version if available - even if the pipeline has been updated since. To make sure that you're running the latest version of the pipeline, make sure that you regularly update the cached version of the pipeline:</p> <pre><code>nextflow pull Joon-Klaps/viralgenie\n</code></pre>"},{"location":"usage/#reproducibility","title":"Reproducibility","text":"<p>It is a good idea to specify a pipeline version when running the pipeline on your data. This ensures that a specific version of the pipeline code and software are used when you run your pipeline. If you keep using the same tag, you'll be running the same version of the pipeline, even if there have been changes to the code since.</p> <p>First, go to the Joon-Klaps/viralgenie releases page and find the latest pipeline version - numeric only (eg. <code>1.3.1</code>). Then specify this when running the pipeline with <code>-r</code> (one hyphen) - eg. <code>-r 1.3.1</code>. Of course, you can switch to another version by changing the number after the <code>-r</code> flag.</p> <p>This version number will be logged in reports when you run the pipeline, so that you'll know what you used when you look back in the future. For example, at the bottom of the MultiQC reports.</p> <p>To further assist in reproducibility, you can use share and re-use parameter files to repeat pipeline runs with the same settings without having to write out a command with every single parameter.</p> <p>Tip</p> <p>If you wish to share such profile (such as upload as supplementary material for academic publications), make sure to NOT include cluster specific paths to files, nor institutional specific profiles.</p>"},{"location":"usage/#core-nextflow-arguments","title":"Core Nextflow arguments","text":"<p>Note</p> <p>These options are part of Nextflow and use a single hyphen (pipeline parameters use a double-hyphen).</p>"},{"location":"usage/#the-profile-parameter","title":"The <code>-profile</code> parameter","text":"<p>Use this parameter to choose a configuration profile. Profiles can give configuration presets for different compute environments.</p> <p>Several generic profiles are bundled with the pipeline which instruct the pipeline to use software packaged using different methods (Docker, Singularity, Podman, Shifter, Charliecloud, Apptainer, Conda) - see below.</p> <p>Info</p> <p>We highly recommend the use of Docker or Singularity containers for full pipeline reproducibility, however when this is not possible, Conda is also supported.</p> <p>The pipeline also dynamically loads configurations from https://github.com/nf-core/configs when it runs, making multiple config profiles for various institutional clusters available at run time. For more information and to see if your system is available in these configs please see the nf-core/configs documentation.</p> <p>Note that multiple profiles can be loaded, for example: <code>-profile test,docker</code> - the order of arguments is important! They are loaded in sequence, so later profiles can overwrite earlier profiles.</p> <p>If <code>-profile</code> is not specified, the pipeline will run locally and expect all software to be installed and available on the <code>PATH</code>. This is not recommended, since it can lead to different results on different machines dependent on the computer enviroment.</p> <ul> <li><code>test</code><ul> <li>A profile with a complete configuration for automated testing</li> <li>Includes links to test data so needs no other parameters</li> </ul> </li> <li><code>docker</code><ul> <li>A generic configuration profile to be used with Docker</li> </ul> </li> <li><code>singularity</code><ul> <li>A generic configuration profile to be used with Singularity</li> </ul> </li> <li><code>podman</code><ul> <li>A generic configuration profile to be used with Podman</li> </ul> </li> <li><code>shifter</code><ul> <li>A generic configuration profile to be used with Shifter</li> </ul> </li> <li><code>charliecloud</code><ul> <li>A generic configuration profile to be used with Charliecloud</li> </ul> </li> <li><code>apptainer</code><ul> <li>A generic configuration profile to be used with Apptainer</li> </ul> </li> <li><code>conda</code><ul> <li>A generic configuration profile to be used with Conda. Please only use Conda as a last resort i.e. when it's not possible to run the pipeline with Docker, Singularity, Podman, Shifter, Charliecloud, or Apptainer.</li> </ul> </li> </ul>"},{"location":"usage/#-resume","title":"<code>-resume</code>","text":"<p>Specify this when restarting a pipeline. Nextflow will use cached results from any pipeline steps where the inputs are the same, continuing from where it got to previously. For input to be considered the same, not only the names must be identical but the files' contents as well. For more info about this parameter, see this blog post.</p> <p>You can also supply a run name to resume a specific run: <code>-resume [run-name]</code>. Use the <code>nextflow log</code> command to show previous run names.</p>"},{"location":"usage/#-c","title":"<code>-c</code>","text":"<p>Specify the path to a specific config file (this is a core Nextflow command). See the nf-core website documentation for more information.</p>"},{"location":"usage/#custom-configuration","title":"Custom configuration","text":""},{"location":"usage/#resource-requests","title":"Resource requests","text":"<p>Whilst the default requirements set within the pipeline will hopefully work for most people and with most input data, you may find that you want to customize the compute resources that the pipeline requests. Each step in the pipeline has a default set of requirements for number of CPUs, memory and time. For most of the steps in the pipeline, if the job exits with any of the error codes specified here it will automatically be resubmitted with higher requests (2 x original, then 3 x original). If it still fails after the third attempt then the pipeline execution is stopped.</p> <p>To change the resource requests, please see the max resources and tuning workflow resources section of the nf-core website.</p>"},{"location":"usage/#custom-containers","title":"Custom Containers","text":"<p>In some cases you may wish to change which container or conda environment a step of the pipeline uses for a particular tool. By default Viralgenie uses containers and software from the biocontainers or bioconda projects. However in some cases the pipeline specified version maybe out of date.</p> <p>To use a different container from the default container or conda environment specified in a pipeline, please see the updating tool versions section of the nf-core website.</p>"},{"location":"usage/#custom-tool-arguments","title":"Custom Tool Arguments","text":"<p>A pipeline might not always support every possible argument or option of a particular tool used in pipeline. Fortunately, Viralgenie provides some freedom to users to insert additional parameters that the pipeline does not include by default.</p> <p>To learn how to provide additional arguments to a particular tool of the pipeline, please see the customising tool arguments section of the nf-core website.</p>"},{"location":"usage/#nf-coreconfigs","title":"nf-core/configs","text":"<p>In most cases, you will only need to create a custom config as a one-off but if you and others within your organisation are likely to be running Viralgenie regularly and need to use the same settings regularly it may be a good idea to request that your custom config file is uploaded to the <code>nf-core/configs</code> git repository. Before you do this please can you test that the config file works with your pipeline of choice using the <code>-c</code> parameter. You can then create a pull request to the <code>nf-core/configs</code> repository with the addition of your config file, associated documentation file (see examples in <code>nf-core/configs/docs</code>), and amending <code>nfcore_custom.config</code> to include your custom profile.</p> <p>See the main Nextflow documentation for more information about creating your own configuration files.</p> <p>If you have any questions or issues please send us a message on Slack on the <code>#configs</code> channel.</p>"},{"location":"usage/#azure-resource-requests","title":"Azure Resource Requests","text":"<p>To be used with the <code>azurebatch</code> profile by specifying the <code>-profile azurebatch</code>. We recommend providing a compute <code>params.vm_type</code> of <code>Standard_D16_v3</code> VMs by default but these options can be changed if required.</p> <p>Note that the choice of VM size depends on your quota and the overall workload during the analysis. For a thorough list, please refer the Azure Sizes for virtual machines in Azure.</p>"},{"location":"usage/#running-in-the-background","title":"Running in the background","text":"<p>Nextflow handles job submissions and supervises the running jobs. The Nextflow process must run until the pipeline is finished.</p> <p>The Nextflow <code>-bg</code> flag launches Nextflow in the background, detached from your terminal so that the workflow does not stop if you log out of your session. The logs are saved to a file.</p> <p>Alternatively, you can use <code>screen</code> / <code>tmux</code> or similar tool to create a detached session which you can log back into at a later time. Some HPC setups also allow you to run nextflow within a cluster job submitted your job scheduler (from where it submits more jobs).</p>"},{"location":"usage/#nextflow-memory-requirements","title":"Nextflow memory requirements","text":"<p>In some cases, the Nextflow Java virtual machines can start to request a large amount of memory. We recommend adding the following line to your environment to limit this (typically in <code>~/.bashrc</code> or <code>~./bash_profile</code>):</p> <pre><code>NXF_OPTS='-Xms1g -Xmx4g'\n</code></pre>"},{"location":"customisation/configuration/","title":"Custom configuration of modules","text":"<p>Within viralgenie, all modules or runned tools can be configured with additional arguments, or these arguments can be modified. This can be done by supplying viralgenie a custom configuration file. This file can be provided to viralgenie using the <code>-c</code> Nextflow option.</p> <p>To see which specific arguments or variables are used for a module or tool, have a look at the <code>modules.config</code> file. Here the arguments of a module is specified as followed:</p> <pre><code>withName: IVAR_CONSENSUS {\n    ext.args = [\n        '-t 0.75',          // frequency to call consensus: 0.75 just the majority rule\n        '-q 20',            // minimum quality score of base\n        '-m 10',            // minimum depth to call consensus\n        '-n N'              // Character to print in regions with less coverage\n    ].join(' ').trim()\n    ext.args2 = [\n        '--count-orphans',  // Do not skip anomalous read pairs in variant calling.\n        '--max-depth 0',    // Maximum number of reads to start to consider at each location, 0 means no limit\n        '--min-BQ 20',      // Minimum base quality\n        '--no-BAQ',         // Disable probabilistic realignment for the computation of base alignment quality\n        '-aa',              // Output absolutely all positions, including unused reference sequences\n    ].join(' ').trim()\n    ...\n}\n</code></pre> <p>In this example, the <code>IVAR_CONSENSUS</code> module is configured with the arguments <code>-q 20 -m 10</code> for the tool <code>ivar consensus</code> and <code>--ignore-overlaps --count-orphans --max-depth 0 --no-BAQ --min-BQ 0</code> for <code>samtools mpileup</code> as iVar uses the output of <code>samtools mpileup</code> directly.</p> <p>Tip</p> <p>The <code>ext.args</code> and <code>ext.args2</code> are used to specify the arguments for the tool. If unsure which tools uses which arguments (<code>ivar:ext.args</code>and <code>samtools:ext.args2</code>), have a look at the nextflow module file directly! For example, at <code>modules/nf-core/ivar/consensus.nf</code>, \"$args\" and \"$args2\" are used to specify the arguments for the tools: <pre><code>\"\"\"\nsamtools \\\\\n    mpileup \\\\\n    --reference $fasta \\\\\n    $args2 \\\\               # can be modified with ext.args2\n    $bam \\\\\n    $mpileup \\\\\n    | ivar \\\\\n        consensus \\\\\n        $args \\\\            # can be modified with ext.args\n        -p $prefix\n...\n\"\"\"\n</code></pre></p> <p>In case we do want to modify the arguments of a module, we can do so by providing a custom configuration file, the easiest way to do this would to then just copy a segment from the modules.config and modify the arguments. This way, none of the other configuration will get lost or modified. For example, setting the minimum depth to call consensus to 5 and the minimum quality score of base to 30 for the <code>IVAR_CONSENSUS</code> module: custom.config<pre><code>process {\n    withName: IVAR_CONSENSUS {\n        ext.args = [\n            '-t 0.75',\n            '-q 30',            // changed\n            '-m 5',             // changed\n            '-n N'\n        ].join(' ').trim()\n        ext.args2 = [\n            '--count-orphans',\n            '--max-depth 0',\n            '--min-BQ 30',      // changed\n            '--no-BAQ',\n            '-aa',\n        ].join(' ').trim()\n        ...\n    }\n}\n</code></pre></p> <p>Warning</p> <p>Make sure you include the <code>process{}</code> section.</p> <p>Next, supply the file to viralgenie using the <code>-c</code> Nextflow option: <pre><code>nextflow run Joon-Klaps/viralgenie \\\n    -profile docker \\\n    -c custom.config \\\n    --input samplesheet.csv ...\n</code></pre></p> <p>Tip</p> <p>This guide not entirely clear? Also have a look at the nf-core guide for customising tool arguments.</p>"},{"location":"customisation/databases/","title":"Databases","text":""},{"location":"customisation/databases/#introduction","title":"Introduction","text":"<p>Viralgenie uses a multitude of databases in order to analyse reads, contigs and consensus constructs. The default databases will be sufficient in most cases but there are always exceptions. This document will guide you towards the right documentation location for creating your custom databases.</p> <p>Tip</p> <p>Keep an eye out for nf-core createtaxdb as it can be used for the customization of the main databases but the pipeline is still under development.</p>"},{"location":"customisation/databases/#reference-pool","title":"Reference pool","text":"<p>The reference pool dataset is used to identify potential references for scaffolding. It's a fasta file that will be used to make a blast database within the pipeline. The default database is the clustered Reference Viral DataBase (C-RVDB) a database that was build for enhancing virus detection using high-throughput/next-generation sequencing (HTS/NGS) technologies. An alternative reference pool is the Virosaurus database which is a manually curated database of viral genomes.</p> <p>Any nucleotide fasta file will do. Specify it with the parameter <code>--reference_pool</code>.</p>"},{"location":"customisation/databases/#kaiju","title":"Kaiju","text":"<p>The kaiju database will be used to classify the reads and intermediate contigs in taxonomic groups. The default database is the RVDB-prot pre-built database from Kaiju.</p> <p>A number of Kaiju pre-built indexes for reference datasets are maintained by the the developers of Kaiju and made available on the Kaiju website. To build a kaiju database, you need three components: a FASTA file with the protein sequences, the NCBI taxonomy dump files, and you need to define the uppercase characters of the standard 20 amino acids you wish to include.</p> <p>Warning</p> <p>The headers of the protein fasta file must be numeric NCBI taxon identifiers of the protein sequences.</p> <p>To download the NCBI taxonomy files, please run the following commands:</p> <pre><code>wget https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/new_taxdump/new_taxdump.zip\nunzip new_taxdump.zip\n</code></pre> <p>To build the database, run the following command (the contents of taxdump must be in the same location where you run the command):</p> <pre><code>kaiju-mkbwt -a ACDEFGHIKLMNPQRSTVWY -o proteins proteins.faa\nkaiju-mkfmi proteins\n</code></pre> <p>Tip</p> <p>You can speed up database construction by supplying the threads parameter (<code>-t</code>).</p> Expected files in database directory <ul> <li><code>kaiju</code><ul> <li><code>kaiju_db_*.fmi</code></li> <li><code>nodes.dmp</code></li> <li><code>names.dmp</code></li> </ul> </li> </ul> <p>For the Kaiju database construction documentation, see here.</p>"},{"location":"customisation/databases/#kraken2-databases","title":"Kraken2 databases","text":"<p>The Kraken2 database will be used to classify the reads and intermediate contigs in taxonomic groups.</p> <p>A number of database indexes have already been generated and maintained by @BenLangmead Lab, see here. These databases can directly be used to run the workflow with Kraken2 as well as Bracken.</p> <p>In case the databases above do not contain your desired libraries, you can build a custom Kraken2 database. This requires two components: a taxonomy (consisting of <code>names.dmp</code>, <code>nodes.dmp</code>, and <code>*accession2taxid</code>) files, and the FASTA files you wish to include.</p> <p>To pull the NCBI taxonomy, you can run the following:</p> <pre><code>kraken2-build --download-taxonomy --db &lt;YOUR_DB_NAME&gt;\n</code></pre> <p>You can then add your FASTA files with the following build command.</p> <pre><code>kraken2-build --add-to-library *.fna --db &lt;YOUR_DB_NAME&gt;\n</code></pre> <p>You can repeat this step multiple times to iteratively add more genomes prior building.</p> <p>Once all genomes are added to the library, you can build the database (and optionally clean it up):</p> <pre><code>kraken2-build --build --db &lt;YOUR_DB_NAME&gt;\nkraken2-build --clean --db &lt;YOUR_DB_NAME&gt;\n</code></pre> <p>You can then add the <code>&lt;YOUR_DB_NAME&gt;/</code> path to your nf-core/taxprofiler database input sheet.</p> Expected files in database directory <ul> <li><code>kraken2</code><ul> <li><code>opts.k2d</code></li> <li><code>hash.k2d</code></li> <li><code>taxo.k2d</code></li> </ul> </li> </ul> <p>You can follow the Kraken2 tutorial for a more detailed description.</p>"},{"location":"customisation/databases/#host-read-removal","title":"Host read removal","text":"<p>Viralgenie uses kraken2 to remove contaminated reads.</p> <p>Info</p> <p>The reason why we use Kraken2 for host removal over regular read mappers is nicely explained in the following papers:</p> <ul> <li>The human \u201ccontaminome\u201d: bacterial, viral, and computational contamination in whole genome sequences from 1000 families</li> <li>Reconstruction of the personal information from human genome reads in gut metagenome sequencing data</li> </ul> <p>The contamination database is likely the largest database. The default databases is made small explicitly made smaller to save storage for end users but is not optimal. I would recommend to create a database consisting of the libraries <code>human, archea, bacteria</code> which will be more then 200GB in size. Additionally, it's good practice to include DNA &amp; RNA of the host of origin if known (i.e. mice, ticks, mosquito, ... ). Add it as described above.</p> <p>Set it with the variable <code>--host_k2_db</code></p>"},{"location":"customisation/databases/#viral-diversity-with-kraken2","title":"Viral Diversity with Kraken2","text":"<p>The metagenomic diveristy estimated with kraken2 is based on the viral refseq database which can cut short if you expect your the species within your sample to have a large amount of diversity eg below 80% ANI (quasi-species). To resolve this it's better to create a database that contains a wider species diversity then only one genome per species. Databases that have this wider diversity is Virosaurus or the RVDB which can increase the accuracy of kraken2. Add it as described above.</p> <p>Set it with the variable <code>--kraken2_db</code></p>"},{"location":"customisation/databases/#annotation-sequences","title":"Annotation sequences","text":"<p>Identifying the species and the segment of the final genome constructs is done based on a tblastx search (with MMSEQS) to a annotated sequencing dataset. This dataset is by default the Virosaurus as it contains a good representation of the viral genomes and is annotated.</p> <p>This annotation database can be specified using <code>--annotation_db</code></p>"},{"location":"customisation/databases/#creating-a-custom-annotation-dataset-with-bv-brc","title":"Creating a custom annotation dataset with BV-BRC","text":"<p>In case Virosaurus does not suffice your needs, a custom annotation dataset can be made. Creating a custom annotation dataset can easily be done as long as the annotation data is in the fasta header using this format: <code>(key)=(value)</code> or <code>(key):(value)</code>. For example, the following fasta headers are both valid:</p> <pre><code>&gt;754189.6 species:\"Ungulate tetraparvovirus 3\"|segment:\"nan\"|host_common_name:\"Pig\"|genbank_accessions:\"NC_038883\"|taxon_id:\"754189\"\n&gt;NC_001731; usual name=Molluscum contagiosum virus; clinical level=SPECIES; clinical typing=unknown; species=Molluscum contagiosum virus; taxid=10279; acronym=MOCV; nucleic acid=DNA; circular=N; segment=N/A; host=Human,Vertebrate;\n</code></pre> <p>An easy to use public database with a lot of metadata is BV-BRC. Sequences can be extracted using their CLI-tool and linked to their metadata</p> <p>Here we select all viral genomes that are not lab reassortments and are reference genomes and add metadata attributes to the output.</p> <p>This is an example, in case you need to have a more elaborate dataset then virosaurus, be more inclusive towards your taxa of interest and include more metadata attributes.</p> <pre><code># download annotation metadata +/- 5s\np3-all-genomes --eq superkingdom,Viruses --eq reference_genome,Reference --ne host_common_name,'Lab reassortment' --attr genome_id,species,segment,genome_name,genome_length,host_common_name,genbank_accessions,taxon_id   &gt; all-virus-anno.txt\n# download genome data, done seperatly as it takes much longer to query +/- 1 hour\np3-all-genomes --eq superkingdom,Viruses --eq reference_genome,Reference --ne host_common_name,'Lab reassortment' | p3-get-genome-contigs --attr sequence &gt; all-virus.fasta\n</code></pre> <p>Tip</p> <p>Any attribute can be downloaded and will be added to the final report if the formatting remains the same. For a complete list of attributes see <code>p3-all-genomes --fields</code> or read their manual</p> <p>Next, the metadata and the genomic data is combined into a single fasta file where the metada fields are stored in the fasta comment as <code>key1:\"value1\"|key2:\"value2\"|...</code> using the following python code.</p> <pre><code>import pandas as pd\nimport re\n\n# read in sequences with all columns as strings\nsequences = pd.read_csv(\"refseq-virus.fasta\", index_col=0, sep=\"\\t\", dtype=str)\ndata = pd.read_csv(\"refseq-virus-anno.txt\", index_col=0, sep=\"\\t\", dtype=str)\n\n# merge the df's\ndf = sequences.join(data)\n# remove 'genome' from the name\ndf.columns = df.columns.str.replace(\"genome.\", \"\")\n\n# create fasta header\ndef create_fasta_header(row):\n    annotations = \"|\".join(\n        [\n            f'{column}:\"{value}\"'\n            for column, value in row.items()\n            if column != \"contig.sequence\"\n        ]\n    )\n    return f\"{annotations}\\n\"\n\n\ndf[\"fasta_header\"] = df.apply(create_fasta_header, axis=1)\n\ndf[\"fasta_entry\"] = (\n    \"&gt;\" + df.index.astype(str) + \" \" + df[\"fasta_header\"] + df[\"contig.sequence\"]\n)\nwith open(\"bv-brc-refvirus-anno.fasta\", \"w\") as f:\n    for entry in df[\"fasta_entry\"]:\n        f.write(entry + \"\\n\")\n</code></pre> Expected files in database directory <ul> <li><code>refseq-virus.fasta</code></li> <li><code>refseq-virus-anno.txt</code></li> <li><code>bv-brc-refvirus-anno.fasta</code></li> </ul>"},{"location":"workflow/assembly_polishing/","title":"Assembly &amp; polishing","text":"<p>Viralgenie offers an elaborate workflow for the assembly and polishing of viral genomes:</p> <ul> <li>Assembly: combining the results of multiple assemblers.</li> <li>Reference Matching: comparing the newly assembled contigs to a reference sequence pool.</li> <li>Clustering: clustering the contigs based on taxonomy and similarity.</li> <li>Scaffolding: scaffolding the contigs to the centroid of each bin.</li> <li>Annotation with Reference: annotating regions with 0-depth coverage with the reference sequence.</li> </ul> <p></p> <p>The overal workflow of creating reference assisted assemblies can be skipped with the argument <code>--skip_assembly</code>. See the parameters assembly section for all relevant arguments to control the assembly steps.</p> <p>The overall refinement of contigs can be skipped with the argument <code>--skip_polishing</code>. See the parameters polishing section for all relevant arguments to control the polishing steps.</p> <p>The consensus genome of all clusters are then send to the variant analysis &amp; iterative refinement step.</p>"},{"location":"workflow/assembly_polishing/#assembly","title":"Assembly","text":"<p>Three assemblers are used, SPAdes, Megahit, and Trinity. The resulting contigs of all specified assemblers, are combined and processed further together.</p> <p>Modify the spades mode with <code>--spades_mode [default: rnaviral]</code> and supply specific params with <code>--spades_yml</code> or a hmm model with <code>--spades_hmm</code>.</p> <p>Specify the assemblers to use with the <code>--assemblers</code> parameter where the assemblers are separated with a ','. The default is <code>spades,megahit,trinity</code>.</p>"},{"location":"workflow/assembly_polishing/#reference-matching","title":"Reference Matching","text":"<p>The newly assembled contigs are compared to a reference sequence pool (--reference_pool) using a BLASTn search. This process not only helps annotate the contigs but also assists in linking together sets of contigs that are distant within a single genome. Essentially, it aids in identifying contigs belonging to the same genomic segment and choosing the right reference for scaffolding purposes.</p> <p>The top 5 hits for each contig are combined with the denovo contigs and send to the clustering step.</p> <p>The reference pool can be specified with the <code>--reference_pool</code> parameter. The default is the latest clustered Reference Viral DataBase (RVDB).</p>"},{"location":"workflow/assembly_polishing/#clustering","title":"Clustering","text":"<p>The clustering workflow of contigs consists out of 2 steps, the pre-clustering and actual clustering. Here contigs are first separated based on identified taxonomy-id (Kraken2, Kaiju) and are subsequently clustered further to identify genome segments.</p>"},{"location":"workflow/assembly_polishing/#pre-clustering","title":"Pre-clustering","text":"<p>The contigs along with their references have their taxonomy assigned using Kraken2 and Kaiju.</p> <p>The default databases are the same ones used for read classification: - Kraken2: viral refseq database, <code>--kraken2_db</code> - Kaiju: clustered RVDB, <code>--kaiju_db</code></p> <p>Tip</p> <p>Sometimes large metagenomic datasets, could still contain a large number of contigs that are not viral in origin and are 'unclassified' despite having accurate databases. These contigs can be removed with the <code>--keep_unclassified false</code> argument.</p> <p>As Kajiu and Kraken2 can have different taxonomic assignments, an additional step is performed to resolve potential inconsistencies in taxonomy and to identify the taxonomy of the contigs. The pre-clustering step is performed with Kaiju-mergeOutputs.</p> <p>Tip</p> <p>Specify the strategy to resolve inconsistencies with <code>--taxon_merge_strategy</code> options:</p> <ul> <li>'1' the taxon id from Kaiju is used.</li> <li>'2' the taxon id from Kraken is used.</li> <li>'lca' the least common ancestor of the two taxon ids from both input files is used.</li> <li>'lowest' the lower rank of the two taxa is used if they are within the same lineage. Otherwise the LCA is used.</li> </ul> <p>The pre-clustering step will be run by default but can be skipped with the argument <code>--skip_preclustering</code>.</p>"},{"location":"workflow/assembly_polishing/#actual-clustering","title":"Actual clustering","text":"<p>The clustering is performed with one of the following tools:     - <code>cdhitest</code>     - <code>vsearch</code>     - <code>mmseqs-linclust</code>     - <code>mmseqs-cluster</code>     - <code>vRhyme</code>     - <code>mash</code></p> <p>These methods all come with their own advantages and disadvantages. For example, cdhitest is very fast but cannot be used for large viruses &gt;10Mb and similarity threshold cannot go below 80% which is not preferable for highly diverse RNA viruses. Vsearch is slower but accurate. Mmseqs-linclust is the fastest but tends to create a large amount of bins. Mmseqs-cluster is slower but can handle larger datasets and is more accurate. vRhyme is a new method that is still under development but has shown promising results but can sometimes not output any bins when segments are small. Mash is a very fast comparison method is linked with a custom script that identifies communities within a network.</p> <p>Tip</p> <p>When pre-clustering is performed, it is recommended to set a lower identity_threshold (60-70% ANI) as the new goal becomes to separate genome segments within the same bin.</p> <p>The clustering method can be specified with the <code>--clustering_method</code> parameter. The default is <code>mash</code>.</p> <p>The network clustering method for <code>mash</code> can be specified with the <code>--network_clustering</code> parameter. The default is <code>connected_components</code>, alternative is <code>leiden</code>.</p> <p>The similarity threshold can be specified with the <code>--similarity_threshold</code> parameter. The default is <code>0.6</code>. However, for cdhit the default is <code>0.8</code> which is its minimum value.</p>"},{"location":"workflow/assembly_polishing/#scaffolding","title":"Scaffolding","text":"<p>After classifying all contigs and their top BLAST hits into distinct clusters or bins, the contigs are then scaffolded to the centroid of each bin. Any external references that are not centroids of the cluster are subsequently removed to prevent further bias. All members of the cluster are consequently mapped towards their centroid with Minimap2 and conensus is called using iVar-consensus.</p>"},{"location":"workflow/assembly_polishing/#annotation-with-reference","title":"Annotation with Reference","text":"<p>Regions with 0-depth coverage are annotated with the reference sequence. This is done with a custom script that uses the coverage of the denovo contigs towards the reference sequence to identify regions with 0-depth coverage. The reference sequence is then annotated to these regions.</p> <p>This step can be skipped using <code>--skip_hybrid_consensus</code> parameter.</p>"},{"location":"workflow/consensus_qc/","title":"Report generation and quality control","text":"<p>Viralgenie's report and result interpreation heavily relies on MultiQC. MultiQC is a tool to create a single report from multiple analysis results. It is designed to be used with a wide range of bioinformatics tools and is compatible with a wide range of data formats. Almost all tools are summarised within the MultiQC report that have interactive plots and data tables.</p> <p>Tip</p> <p>Complete output descriptions of files and images can be found in the output section.</p> <p>Within the multiqc report, viralgenie provides a number of custom tables based consensus genome quality control data. These tools are:</p> <ul> <li>QUAST: QUAST is a quality assessment tool for genome assemblies. It calculates various metrics such as N50, L50, number of contigs, and total length.</li> <li>CheckV: CheckV is a tool for assessing the quality of metagenome-assembled viral genomes. It calculates various metrics such as completeness, contamination, and strain heterogeneity.</li> <li>blastn: BLAST is a tool for comparing primary biological sequence information. It calculates the similarity between the consensus genome and the reference genome.</li> <li>mmseqs-search: MMseqs is a ultra fast and sensitive search tool for protein and nucleotide databases. Viralgenie uses MMseqs to annotate the consensus genomes and assign them a species name, segment name, expected host etc.</li> <li>mafft - not included in MultiQC: MAFFT is a multiple sequence alignment program.</li> </ul> <p>Consensus genome quality control can be skipped with <code>--skip_consensus_qc</code>.</p>"},{"location":"workflow/consensus_qc/#quast","title":"QUAST","text":"<p>QUAST is a quality assessment tool for genome assemblies. It calculates various metrics such as N50, L50, number of contigs, and total length. However, in the summary table it is mainly used to get the number of ambigous bases in the consensus genome.</p> <p>Quast can be skipped with <code>--skip_quast</code>.</p>"},{"location":"workflow/consensus_qc/#checkv","title":"CheckV","text":"<p>CheckV is a tool for assessing the quality of metagenome-assembled viral genomes. It calculates various metrics such as completeness, contamination, and strain heterogeneity. CheckV estimates completeness by comparing sequences with a large database of complete viral genomes, metagenomes, metatranscriptomes and metaviromes</p> <p>Incomplete genomes for segmented viruses</p> <p>Checkv estimates the completeness of virus based on the all genome segments. If a virus has multiple segments, the completeness of the virus is calculated based on the length of the concatenated segments. For example, Lassa virus has 2 segments L: 7.2kb and S: 3.4kb. The completeness of the virus is calculated based on the length of the concatenated segments (7.2kb + 3.4kb = 10.6kb) and so if the generated consensus genome of the L segment is 7.1kb it will report the completeness as 7.1/10.6 ~ 67%.</p> <p>Checkv can be skipped with <code>--skip_checkv</code>.</p>"},{"location":"workflow/consensus_qc/#blastn","title":"BLASTn","text":"<p>blastn is a tool for comparing primary biological sequence information. It calculates the similarity between the consensus genome and the reference genome. The similarity is calculated based on the number of identical bases between the two sequences. Viralgenie uses blastn to compare the sequences against the supplied <code>--reference_pool</code> dataset.</p> <p>Blastn can be skipped with <code>--skip_blast_qc</code>.</p>"},{"location":"workflow/consensus_qc/#mmseqs-search","title":"MMseqs-search","text":"<p>MMseqs-search is a ultra fast and sensitive search tool for protein and nucleotide databases. Viralgenie uses MMseqs to search the consensus genomes in a annotated database, like Virousarus (see also defining your own custom annotation database), and uses the annotation data of the best hit to assign the consensus genome a species name, segment name, expected host and any other metadata that is embedded within the database. This allows viralgenie in addition to the blast search of reference pool hits to compare the generated consensus genomes a species &amp; segment level.</p> <p>Info</p> <p>MMseqs was used for the annotation step instead of blast because of the ability to query using a tblastx search for highly diverging viruses while supplying a nucleotide annotation database. To specify another type of search (e.g. blastp, blastx, etc.), please refer to the parameters consensus-qc section.</p> <p>MMseqs-search can be skipped with <code>--skip_annotation</code>.</p>"},{"location":"workflow/consensus_qc/#mafft","title":"MAFFT","text":"<p>MAFFT is a multiple sequence alignment program. It is used to align the following genomic data: - The final consensus genome - The identified reference genome from <code>--reference_pool</code> - The denovo contigs from each assembler (that constituted the final consensus genome) - Each consensus genome from the iterative refinement steps.</p> <p>MAFFT can be skipped with <code>--skip_alignment_qc</code>.</p>"},{"location":"workflow/consensus_qc/#multiqc","title":"MultiQC","text":"<p>MultiQC is a tool to create a single report with interactive plots for multiple bioinformatics analyses across many samples.</p> <p></p> <p>Reports are generated by scanning given directories for recognised log files. These are parsed and a single HTML report is generated summarising the statistics for all logs found. MultiQC reports can describe multiple analysis steps and large numbers of samples within a single plot, and multiple analysis tools making it ideal for routine fast quality control.</p> <p>Multiqc is ran twice, as the first run is to extract data from all of the tools used so they can be summarised in a multiple tables. These tables can be customised with the argument <code>--custom_table_headers</code> where a yml file, shows which tools need to be included in the summary table in addition to blast, checkv, quast, and mmseqs (annotation).</p> custom_table_headers.yml<pre><code>tool:\n  - name_in_mqc_table: \"new name\"\n  - output_reads: \"deduplicated reads\"\n  - percent_passing_dedup: \"% passing dedup\"\n</code></pre>"},{"location":"workflow/metagenomic_diversity/","title":"Read classification","text":"<p>Viralgenie offers two main tools for the classification of reads and a summary visualisation tool:</p> <ul> <li>Kaiju: Taxonomic classification based on maximum exact matches using protein alignments.</li> <li>Kraken2: Assigns taxonomic labels on a DNA level using a k-mer approach.</li> <li>Krona: Interactive multi-layered pie charts of hierarchial data.</li> </ul> <p></p> <p>Want more classifiers?</p> <p>Feel free to reach out and suggest more classifiers. However, if the main goal of your project is to establish the presence of a virus within a sample and are therefore only focused on metagenomic diversity, have a look at taxprofiler</p>"},{"location":"workflow/metagenomic_diversity/#kaiju","title":"Kaiju","text":"<p>Kaiju classifies individual metagenomic reads using a reference database comprising the annotated protein-coding genes of a set of microbial genomes. It  employ a search strategy, which finds maximal exact matching substrings between query and database using a modified version of the backwards search algorithm in the Burrows-Wheeler transform is a text transformation that converts the reference sequence database into an easily searchable representation, which allows for exact string matching between a query sequence and the database in time proportional to the length of the query.</p>"},{"location":"workflow/metagenomic_diversity/#kraken2","title":"Kraken2","text":"<p>Kraken is a taxonomic sequence classifier that assigns taxonomic labels to DNA sequences. Kraken examines the k-mers within a query sequence and uses the information within those k-mers to query a database. That database maps -mers to the lowest common ancestor (LCA) of all genomes known to contain a given k-mer.</p>"},{"location":"workflow/metagenomic_diversity/#krona","title":"Krona","text":"<p>Krona allows hierarchical data to be explored with zooming, multi-layered pie charts. The interactive charts are self-contained and can be viewed with any modern web browser.</p>"},{"location":"workflow/overview/","title":"Pipeline overview","text":""},{"location":"workflow/overview/#workflow","title":"Workflow","text":"<p>Viralgenie takes in a set of reads and performs 5 major analyses, each of them are explained in more detail in the following sections:</p> <ol> <li>Preprocessing</li> <li>Metagenomic diversity</li> <li>Assembly &amp; Polishing</li> <li>Variant analysis &amp; iterative refinement</li> <li>Consensus evaluation</li> </ol> <p>By default all analyses are run.</p> <p>Skipping steps</p> <p>All steps can be skipped and the pipeline can be run with only the desired steps. This can be done with the <code>--skip_preprocessing</code>, <code>--skip_metagenomic_diversity</code>, <code>--skip_assembly</code>, <code>--skip_polishing</code>, <code>--skip_variant_analysis</code>, <code>--skip_iterative_refinement</code>, <code>--skip_consensus_qc</code> flags.</p>"},{"location":"workflow/overview/#subway-map","title":"Subway map","text":"<ol> <li>Read QC (<code>FastQC</code>)</li> <li>Performs optional read pre-processing<ul> <li>Adapter trimming(<code>fastp</code>, <code>Trimmomatic</code>)</li> <li>Low complexity and quality filtering (<code>bbduk</code>)</li> <li>Host-read removal (<code>BowTie2</code>)</li> </ul> </li> <li>Metagenomic diveristy mapping<ul> <li>Performs taxonomic classification and/or profiling using one or more of:<ul> <li><code>Kraken2</code></li> <li><code>Kaiju</code></li> </ul> </li> <li>Plotting Kraken2 and Kaiju (<code>Krona</code>)</li> </ul> </li> <li>Denovo assembly (<code>SPAdes</code>, <code>TRINITY</code>, <code>megahit</code>), combine contigs.</li> <li>Contig reference idententification (<code>blastn</code>)<ul> <li>Identify top 5 blast hits</li> <li>Merge blast hit and all contigs of a sample</li> </ul> </li> <li>[Optional] Precluster contigs based on taxonomy<ul> <li>Identify taxonomy <code>Kraken2</code> and <code>Kaiju</code></li> <li>Resolve potential incosistencies in taxonomy <code>Kaiju-mergeOutputs</code></li> </ul> </li> <li>Cluster contigs (or every taxonomic bin) of samples, options are:<ul> <li><code>cdhitest</code></li> <li><code>vsearch</code></li> <li><code>mmseqs-linclust</code></li> <li><code>mmseqs-cluster</code></li> <li><code>vRhyme</code></li> <li><code>mash</code></li> </ul> </li> <li>Scaffolding of contigs to centroid (<code>Minimap2</code>, <code>iVar-consensus</code>)</li> <li>[Optional] Annotate 0-depth regions with external reference <code>custom-script</code>.</li> <li>Mapping filtered reads to supercontig and mapping constrains(<code>BowTie2</code>,<code>BWAmem2</code> and <code>BWA</code>)</li> <li>[Optional] Deduplicate reads (<code>Picard</code> or if UMI's are used <code>UMI-tools</code>)</li> <li>Variant calling and filtering (<code>BCFTools</code>,<code>iVar</code>)</li> <li>Create consensus genome (<code>BCFTools</code>,<code>iVar</code>)</li> <li>Repeat step 10-13 multiple times for the denovo contig route</li> <li>Consensus evaluation and annotation (<code>QUAST</code>,<code>CheckV</code>,<code>blastn</code>, <code>mmseqs-search</code>)</li> <li>Result summary visualisation for raw read, alignment, assembly, variant calling and consensus calling results (<code>MultiQC</code>)</li> </ol>"},{"location":"workflow/preprocessing/","title":"Preprocessing","text":"<p>Viralgenie offers three main preprocessing steps for the preprocessing of raw sequencing reads:</p> <ul> <li>Read quality control: read quality assessment and filtering.</li> <li>Read processing: adapter clipping and pair-merging.</li> <li>Complexity filtering: removal of low-sequence complexity reads.</li> <li>Host read-removal: removal of reads aligning to reference genome(s) of a host.</li> </ul> <p></p> <p>Preprocessing can be entirely skipped with the option <code>--skip_preprocessing</code>. See the parameters preprocessing section for all relevant arguments to control the preprocessing steps.</p> <p>Tip</p> <p>Samples with fewer then <code>--min_trimmed_reads [default: 1]</code> reads, will be removed from any further downstream analysis. These samples will be highlighted in the MultiQC report.</p>"},{"location":"workflow/preprocessing/#read-quality-control","title":"Read Quality control","text":"<p><code>FastQC</code> gives general quality metrics about your reads. It provides information about the quality score distribution across your reads, per base sequence content (%A/T/G/C), adapter contamination and overrepresented sequences. <code>FastQC</code> is used before and after read processing and after host read-removal to assess the quality of the reads.</p> <pre><code>graph LR;\n    A[Raw reads] --&gt; B[\"`**FastQC**`\"];\n    B --&gt; C[Read processing];\n    C --&gt; D[\"`**FastQC**`\"];\n    D --&gt; E[Complexity filtering];\n    E --&gt; G[Host read-removal];\n    G --&gt; H[\"`**FastQC**`\"];</code></pre>"},{"location":"workflow/preprocessing/#read-processing","title":"Read processing","text":"<p>Raw sequencing read processing in the form of adapter clipping and paired-end read merging is performed by the tools <code>fastp</code> or <code>Trimmomatic</code>. The tool <code>fastp</code> is a fast all-in-one tool for preprocessing fastq files. The tool <code>Trimmomatic</code> is a flexible read trimming tool for Illumina NGS data. Both tools can be used to remove adapters and low-quality reads from the raw sequencing reads. An adapter file can be provided through the argument <code>--adapter_fasta</code>.</p> <p>Specify the tool to use for read processing with the <code>--trim_tool</code> parameter, the default is <code>fastp</code>.</p>"},{"location":"workflow/preprocessing/#complexity-filtering","title":"Complexity filtering","text":"<p>Complexity filtering is primarily a run-time optimisation step. Low-complexity sequences are defined as having commonly found stretches of nucleotides with limited information content (e.g. the dinucleotide repeat CACACACACA). Such sequences can produce a large number of high-scoring but biologically insignificant results in database searches. Removing these reads therefore saves computational time and resources.</p> <p>Complexity filtering is done with <code>Bbduk</code> which is part of <code>BBtools</code> where the \"duk\" stands for Decontamination Using Kmers.</p> <p>By default this step is skipped, if this step shouldn't be skipped specify <code>--skip_complexity_filtering false</code>.</p>"},{"location":"workflow/preprocessing/#host-read-removal","title":"Host read-removal","text":"<p>Contamination, whether derived from experiments or computational processes, looms large in next-generation sequencing data. Such contamination can compromise results from WGS as well as metagenomics studies, and can even lead to the inadvertent disclosure of personal information. To avoid this, host read-removal is performed. Host read-removal is performed by the tool <code>Kraken2</code>.</p> <p>Want to know more?</p> <ul> <li>The human \u201ccontaminome\u201d: bacterial, viral, and computational contamination in whole genome sequences from 1000 families</li> <li>Reconstruction of the personal information from human genome reads in gut metagenome sequencing data</li> </ul> <p>Specify the host database with the <code>--host_k2_db</code> parameter. The default is small subset of the human genome and we highly suggest that you make this database more elaborate (for example, complete human genome, common sequencer contaminants, bacterial genomes, ...). For this, read the section on creating custom kraken2 host databases.</p>"},{"location":"workflow/variant_and_refinement/","title":"Variant calling and consensus refinement","text":"<p>Viralgenie offers the opportunity to call variants and call consensus given a reference. References are supplied with the samplesheet <code>--mapping_constrains</code>, here reads will be mapped to the reference genome(s) and a consensus genome will be called. The same strategy is used for the iterative refinement of the generated (reference-assisted) de novo consensus genomes. During iterative refinement, the consensus genome is used as a reference for the variant calling and the variant calls are used to refine the consensus genome, this step is can then be repeated multiple times (1-4).</p> <p></p> <p>Mapping constrains</p> <p>A set of given references can be given to the pipeline for variant &amp; consensus calling through the samplesheet <code>--mapping_constrains</code>. They will have exactly the same workflow but will only use it once. <pre><code>graph LR\n    A[Reads] --&gt; B[Mapping constrains]\n    B --&gt; C[Variant calling]\n    C --&gt; D[Consensus calling]\n    D --&gt; E[Report]</code></pre></p> <p>The variant calling and consensus refinement step consists of the following steps:</p> <ul> <li>Mapping of reads: (un)filtered but trimmed reads are mapped to the reference.</li> <li>Deduplication: (optional) deduplication of reads can be performed with</li> <li>Variant calling: identify differences (variants) between the mapped reads and the supplied reference.</li> <li>Variant filtering: variant filtering, only variants with sufficient depth and quality are retained for consensus calling ( only for BCFtools).</li> <li>Mapping statistics: (optional) generate multiple summary statistics of the BAM files.</li> <li>Consensus calling: reference sequence is updated with the variants of sufficient quality.</li> </ul> <p>Info</p> <p>The variant calling and consensus refinement step can be skipped with the argument <code>--skip_iterative_refinement</code> and <code>--skip_variant_calling</code>, see the parameters iterative refinement section and parameters variant analysis section, respectively, for all relevant arguments to control the variant analysis steps.</p>"},{"location":"workflow/variant_and_refinement/#mapping-of-reads","title":"Mapping of reads","text":"<p>Mapping filtered reads to supercontig or mapping constrains is done with <code>BowTie2</code>,<code>BWAmem2</code> and <code>BWA</code>.</p> <p>The comparison of Bowtie2 and BWA-mem was done for Yao et al. (2020) where they found that BWA-MEM2 had a higher mapping rate (faster) and better accuracy. BWA-mem detected more variant bases in mapping reads than Bowtie2. The tool bwa-mem2 is the next version of the bwa-mem algorithm in bwa. It produces alignment identical to bwa and is ~1.3-3.1x faster depending on the use-case, dataset and the running machine.</p> <p>All three methods are supported to keep protocol compatibility with other pipelines and to allow the user to choose the best method for their data.</p> <p>The mapping tool can be specified with the <code>--mapper</code> parameter, the default is <code>bwamem2</code>, in case the intermediate mapper (for intermediate refinement cycles) needs to be different, this can be specified with <code>--intermediate_mapper</code> otherwise it uses the supplied <code>--mapper</code> tool.</p>"},{"location":"workflow/variant_and_refinement/#deduplication","title":"Deduplication","text":"<p>Read deduplication is an optional step that can be performed with <code>Picard</code> or if UMI's are used <code>UMI-tools</code>. Unless you are using UMIs it is not possible to establish whether the fragments you have sequenced from your sample were derived via true biological duplication (i.e. sequencing independent template fragments) or as a result of PCR biases introduced during the library preparation. To correct your reads, use picard MarkDuplicates to mark the duplicate reads identified amongst the alignments to allow you to gauge the overall level of duplication in your samples. So if you have UMI\u2019s, no need to use Picard, instead use UMI-tools to deduplicate your reads. Where instead of mapping location and read similarity, UMI-tools uses the UMI to identify PCR duplicates.</p> <p>Specify <code>--deduplicate</code> to enable deduplication, the default is <code>true</code>. If UMI's are used, specify <code>--with_umi</code> to enable UMI-tools deduplication. UMI's can be in the read header, if it is not in the header specify <code>--skip_umi_extract false</code>, the default is <code>true</code>.</p> <p>By default the UMI's are seperated in the header by ':' if this is different, specify with \"--umi_separator 'YOUR_SEPARATOR'\".</p>"},{"location":"workflow/variant_and_refinement/#variant-calling","title":"Variant calling","text":"<p>Variant calling is done with <code>BCFTools</code> and <code>iVar</code>, here a SNP will need to have at least a depth of 10 and a base quality of 20.</p> <p>BCFtools is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. iVar is a computational package that contains functions broadly useful for viral amplicon-based sequencing while each of iVar functions can be accomplished using existing tools, iVar contains an intersection of functionality from multiple tools that are required to call iSNVs and consensus sequences from viral sequencing data across multiple replicates.</p> <p>There are multiple studies on the benchmarking of variant callers as this is a area with active development. For instance Bassano et al. (2023) noticed that BCFtools called mutations with higher precision and recall than iVar. However, the reason behind this is that iVar has a lower precision then the others within their setup as it detects a lot of \u2018additional\u2019 variants within the sample, resulting in a higher amount of false positives but also true positives.</p> <p>Tip</p> <p>Bcftools doesn't handle well multiallelic sites, so if you have a lot of multiallelic sites, iVar is the better choice. iVar is also the better choice if you have a lot of low-frequency variants.</p> <p>The variant caller can be specified with the <code>--variant_caller</code> parameter, the default is <code>ivar</code>. In case the intermediate variant caller (for intermediate refinement cycles) needs to be different, this can be specified with <code>--intermediate_variant_caller</code> otherwise it uses the supplied <code>--variant_caller</code> tool.</p>"},{"location":"workflow/variant_and_refinement/#variant-filtering","title":"Variant filtering","text":"<p>The following steps are implemented for variant filtering.</p> <ul> <li>[only for <code>BCFtools</code>]: split up multiallelic sites into biallelic records and SNPs and indels should be merged into a single record.</li> <li>Variant filtering: filter out variants with an allelic depth of less than 75% of the average depth of the sample.</li> <li>[only for <code>iVar</code>]: strand bias correction &amp; collapsing variants belonging to the same codon.</li> </ul> <p>Info</p> <p>If these filtering options are not to your liking, you can modify all of them. See the section on configuration for more information on how to do so.</p>"},{"location":"workflow/variant_and_refinement/#mapping-statistics","title":"Mapping statistics","text":"<p>Viralgenie uses multiple tools to get statitics on the variants and on the read mapping. These tools are:</p> <ul> <li><code>samtools flagstat</code> to get the number of reads that are mapped, unmapped, paired, etc.</li> <li><code>samtools idxstats</code> to get the number of reads that are mapped to each reference sequence.</li> <li><code>samtools stats</code> to collects statistics from BAM files and outputs in a text format.</li> <li><code>picard CollectMultipleMetrics</code> to collect multiple metrics from a BAM file.</li> <li><code>mosdepth</code> to calculate genome-wide sequencing coverage.</li> </ul> <p>There is a little overlap between the tools, but they all provide a different perspective on the mapping statistics.</p> <p>By default, all these tools are run, but they can be skipped with the argument <code>--mapping_stats false</code>. In case the intermediate mapping statistics (for intermediate refinement cycles) don't need to be determined set <code>--intermediate_mapping_stats false</code>.</p>"},{"location":"workflow/variant_and_refinement/#consensus-calling","title":"Consensus calling","text":"<p>The consensus genome is updated with the variants of sufficient quality, either the ones determined previously in variant calling and filtering for the <code>--consensus_caller</code> <code>bcftools</code> or they are redetermined for <code>ivar</code>.</p> <p>There are again a couple of differences between the iVar and BCFtools:</p> <ol> <li>Low frequency deletions in iVar. <p>Area\u2019s of low frequency are more easily deleted and not carried along with iVar, this can be a bad thing during the iterative improvement of the consensus but is a good thing at the final consensus step.</p> </li> <li>Ambiguous nucleotides for multi-allelic sites in iVar. <p>iVar is capable to give lower frequency nucleotides ambiguous bases a summarising annotation instead of 'N'. For example at a certain position, the frequency of 'A' is 40% and of 'G' is 40%. Instead of reporting an 'N', iVar will report 'R'.</p> <p></p> </li> <li>Ambiguous nucleotides for low read depth. <p>In case of a low read depth at a certain position, if it doesn't get flagged by bcftools during variant calling, it will not be considered as a variant and the consensus will not be updated. iVar will update the consensus with an ambiguous base in case of low read depth.</p> <p></p> </li> </ol> <p>The consensus caller can be specified with the <code>--consensus_caller</code> parameter, the default is <code>ivar</code>. The intermediate consensus caller (for intermediate refinement cycles) can be specififed with <code>--intermediate_consensus_caller</code> and is by default <code>bcftools</code>.</p>"}]}