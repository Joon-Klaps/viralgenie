{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Viralgenie","text":""},{"location":"#_2","title":"Viralgenie","text":"<p>A metagenomic analysis pipeline for eukaryotic viruses written in nextflow.</p> <p> </p> <p> </p>"},{"location":"#introduction","title":"Introduction","text":"<p>Joon-Klaps/viralgenie is a bioinformatics best-practice analysis pipeline for reconstructing consensus genomes and to identify intra-host variants from metagenomic sequencing data or enriched based sequencing data like hybrid capture.</p>"},{"location":"#pipeline-summary","title":"Pipeline summary","text":"<ol> <li>Read QC (<code>FastQC</code>)</li> <li>Performs optional read pre-processing<ul> <li>Adapter trimming(<code>fastp</code>, <code>Trimmomatic</code>)</li> <li>Low complexity and quality filtering (<code>bbduk</code>)</li> <li>Host-read removal (<code>BowTie2</code>)</li> </ul> </li> <li>Metagenomic diveristy mapping<ul> <li>Performs taxonomic classification and/or profiling using one or more of:<ul> <li><code>Kraken2</code></li> <li><code>Kaiju</code></li> </ul> </li> <li>Plotting Kraken2 and Kaiju (<code>Krona</code>)</li> </ul> </li> <li>Denovo assembly (<code>SPAdes</code>, <code>TRINITY</code>, <code>megahit</code>), combine contigs.</li> <li>Contig reference idententification (<code>blastn</code>)<ul> <li>Identify top 5 blast hits</li> <li>Merge blast hit and all contigs of a sample</li> </ul> </li> <li>[Optional] Precluster contigs based on taxonomy<ul> <li>Identify taxonomy <code>Kraken2</code> and <code>Kaiju</code></li> <li>Resolve potential incosistencies in taxonomy <code>Kaiju-mergeOutputs</code></li> </ul> </li> <li>Cluster contigs (or every taxonomic bin) of samples, options are:<ul> <li><code>cdhitest</code></li> <li><code>vsearch</code></li> <li><code>mmseqs-linclust</code></li> <li><code>mmseqs-cluster</code></li> <li><code>vRhyme</code></li> <li><code>mash</code></li> </ul> </li> <li>Scaffolding of contigs to centroid (<code>Minimap2</code>, <code>iVar-consensus</code>)</li> <li>[Optional] Annotate 0-depth regions with external reference <code>custom-script</code>.</li> <li>Mapping filtered reads to supercontig and mapping constrains(<code>BowTie2</code>,<code>BWAmem2</code> and <code>BWA</code>)</li> <li>[Optional] Deduplicate reads (<code>Picard</code> or if UMI's are used <code>UMI-tools</code>)</li> <li>Variant calling and filtering (<code>BCFTools</code>,<code>iVar</code>)</li> <li>Create consensus genome (<code>BCFTools</code>,<code>iVar</code>)</li> <li>Repeat step 10-13 multiple times for the denovo contig route</li> <li>Contig evaluation and annotation (<code>QUAST</code>,<code>CheckV</code>,<code>blastn</code>, <code>mmseqs-search</code>)</li> <li>Present QC and visualisation for raw read, alignment, assembly, variant calling and consensus calling results (<code>MultiQC</code>)</li> </ol>"},{"location":"#usage","title":"Usage","text":"<p>Note</p> <p>If you are new to Nextflow and nf-core, please refer to this page on how to set-up Nextflow. Make sure to test your setup with <code>-profile test</code> before running the workflow on actual data.</p> <p>Now, you can run the pipeline using:</p> <pre><code>nextflow run Joon-Klaps/viralgenie \\\n   -profile &lt;docker/singularity/.../institute&gt; \\\n   --input samplesheet.csv \\\n   --outdir &lt;OUTDIR&gt;\n</code></pre> <p>Warning</p> <p>Please provide pipeline parameters via the CLI or Nextflow <code>-params-file</code> option. Custom config files including those provided by the <code>-c</code> Nextflow option can be used to provide any configuration except for parameters;  see docs.</p> <p>For more details and further functionality, please refer to the usage documentation and the parameter documentation.</p>"},{"location":"#credits","title":"Credits","text":"<p>Viralgenie was originally written by <code>Joon-Klaps</code>.</p> <p>We thank the following people for their extensive assistance in the development of this pipeline:</p> <ul> <li><code>Philippe Lemey</code></li> <li><code>Liana Kafetzopoulou</code></li> <li><code>nf-core community</code></li> </ul>"},{"location":"#contributions-and-support","title":"Contributions and Support","text":"<p>If you would like to contribute to this pipeline, please see the contributing guidelines.</p>"},{"location":"#citations","title":"Citations","text":"<p>An extensive list of references for the tools used by the pipeline can be found in the <code>CITATIONS.md</code> file.</p> <p>You can cite the <code>nf-core</code> publication as follows:</p> <p>The nf-core framework for community-curated bioinformatics pipelines.</p> <p>Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso &amp; Sven Nahnsen.</p> <p>Nat Biotechnol. 2020 Feb 13. doi: 10.1038/s41587-020-0439-x.</p>"},{"location":"CITATIONS/","title":"Joon-Klaps/viralgenie: Citations","text":""},{"location":"CITATIONS/#nf-core","title":"nf-core","text":"<p>Ewels PA, Peltzer A, Fillinger S, Patel H, Alneberg J, Wilm A, Garcia MU, Di Tommaso P, Nahnsen S. The nf-core framework for community-curated bioinformatics pipelines. Nat Biotechnol. 2020 Mar;38(3):276-278. doi: 10.1038/s41587-020-0439-x. PubMed PMID: 32055031.</p>"},{"location":"CITATIONS/#nextflow","title":"Nextflow","text":"<p>Di Tommaso P, Chatzou M, Floden EW, Barja PP, Palumbo E, Notredame C. Nextflow enables reproducible computational workflows. Nat Biotechnol. 2017 Apr 11;35(4):316-319. doi: 10.1038/nbt.3820. PubMed PMID: 28398311.</p>"},{"location":"CITATIONS/#pipeline-tools","title":"Pipeline tools","text":"<ul> <li> <p>FastQC</p> <p>Andrews, S. (2010). FastQC: A Quality Control Tool for High Throughput Sequence Data [Online].</p> </li> <li> <p>MultiQC</p> <p>Ewels P, Magnusson M, Lundin S, K\u00e4ller M. MultiQC: summarize analysis results for multiple tools and samples in a single report. Bioinformatics. 2016 Oct 1;32(19):3047-8. doi: 10.1093/bioinformatics/btw354. Epub 2016 Jun 16. PubMed PMID: 27312411; PubMed Central PMCID: PMC5039924.</p> </li> </ul>"},{"location":"CITATIONS/#software-packagingcontainerisation-tools","title":"Software packaging/containerisation tools","text":"<ul> <li> <p>Anaconda</p> <p>Anaconda Software Distribution. Computer software. Vers. 2-2.4.0. Anaconda, Nov. 2016. Web.</p> </li> <li> <p>Bioconda</p> <p>Gr\u00fcning B, Dale R, Sj\u00f6din A, Chapman BA, Rowe J, Tomkins-Tinch CH, Valieris R, K\u00f6ster J; Bioconda Team. Bioconda: sustainable and comprehensive software distribution for the life sciences. Nat Methods. 2018 Jul;15(7):475-476. doi: 10.1038/s41592-018-0046-7. PubMed PMID: 29967506.</p> </li> <li> <p>BioContainers</p> <p>da Veiga Leprevost F, Gr\u00fcning B, Aflitos SA, R\u00f6st HL, Uszkoreit J, Barsnes H, Vaudel M, Moreno P, Gatto L, Weber J, Bai M, Jimenez RC, Sachsenberg T, Pfeuffer J, Alvarez RV, Griss J, Nesvizhskii AI, Perez-Riverol Y. BioContainers: an open-source and community-driven framework for software standardization. Bioinformatics. 2017 Aug 15;33(16):2580-2582. doi: 10.1093/bioinformatics/btx192. PubMed PMID: 28379341; PubMed Central PMCID: PMC5870671.</p> </li> <li> <p>Docker</p> <p>Merkel, D. (2014). Docker: lightweight linux containers for consistent development and deployment. Linux Journal, 2014(239), 2. doi: 10.5555/2600239.2600241.</p> </li> <li> <p>Singularity</p> <p>Kurtzer GM, Sochat V, Bauer MW. Singularity: Scientific containers for mobility of compute. PLoS One. 2017 May 11;12(5):e0177459. doi: 10.1371/journal.pone.0177459. eCollection 2017. PubMed PMID: 28494014; PubMed Central PMCID: PMC5426675.</p> </li> </ul>"},{"location":"CONTRIBUTING/","title":"Joon-Klaps/viralgenie: Contributing Guidelines","text":"<p>Hi there!</p> <p>\ud83e\udd29 Many thanks for taking an interest in improving Viralgenie. \ud83e\udd29</p> <p>We try to manage the required tasks for Viralgenie using GitHub issues, you probably came to this page when creating one. Please use the pre-filled template to save time.</p> <p>However, don't be put off by this template - other more general issues and suggestions are welcome! Contributions to the code are even more welcome ;</p> <p>Info</p> <p>If you need help using or modifying viralgenie then the best place to ask is on the nf-core Slack Joon-Klaps.</p>"},{"location":"CONTRIBUTING/#contribution-workflow","title":"Contribution workflow","text":"<p>If you'd like to write some code for Joon-Klaps/viralgenie, the standard workflow is as follows:</p> <ol> <li>Check that there isn't already an issue about your idea in the Joon-Klaps/viralgenie issues to avoid duplicating work. If there isn't one already, please create one so that others know you're working on this</li> <li>Fork the Joon-Klaps/viralgenie repository to your GitHub account</li> <li>Make the necessary changes / additions within your forked repository following Pipeline conventions</li> <li>Use <code>nf-core schema build</code> and add any new parameters to the pipeline JSON schema (requires nf-core tools &gt;= 1.10).</li> <li>Submit a Pull Request against the <code>dev</code> branch and wait for the code to be reviewed and merged</li> </ol> <p>If you're not used to this workflow with git, you can start with some docs from GitHub or even their excellent <code>git</code> resources.</p>"},{"location":"CONTRIBUTING/#tests","title":"Tests","text":"<p>You can optionally test your changes by running the pipeline locally. Then it is recommended to use the <code>debug</code> profile to receive warnings about process selectors and other debug info. Example: <code>nextflow run . -profile debug,test,docker --outdir &lt;OUTDIR&gt;</code>.</p> <p>When you create a pull request with changes, GitHub Actions will run automatic tests. Typically, pull-requests are only fully reviewed when these tests are passing, though of course we can help out before then.</p> <p>There are typically two types of tests that run:</p>"},{"location":"CONTRIBUTING/#lint-tests","title":"Lint tests","text":"<p><code>nf-core</code> has a set of guidelines which viralgenie adheres to. To enforce these and ensure that viralgenie stays in sync, we have developed a helper tool which runs checks on the pipeline code. This is in the nf-core/tools repository and once installed can be run locally with the <code>nf-core lint &lt;pipeline-directory&gt;</code> command.</p> <p>If any failures or warnings are encountered, please follow the listed URL for more documentation.</p>"},{"location":"CONTRIBUTING/#pipeline-tests","title":"Pipeline tests","text":"<p>Viralgenie is set up with a minimal set of test-data. <code>GitHub Actions</code> then runs the pipeline on this data to ensure that it exits successfully. If there are any failures then the automated tests fail. These tests are run both with the latest available version of <code>Nextflow</code> and also the minimum required version that is stated in the pipeline code.</p>"},{"location":"CONTRIBUTING/#patch","title":"Patch","text":"<p>Only in the unlikely and regretful event of a release happening with a bug.</p> <ul> <li>On your own fork, make a new branch <code>patch</code> based on <code>upstream/master</code>.</li> <li>Fix the bug, and bump version (X.Y.Z+1).</li> <li>A PR should be made on <code>master</code> from patch to directly this particular bug.</li> </ul>"},{"location":"CONTRIBUTING/#getting-help","title":"Getting help","text":"<p>For further information/help, please consult the Joon-Klaps/viralgenie documentation and don't hesitate to get in touch on Slack Joon-Klaps channel (join the nf-core Slack here).</p>"},{"location":"CONTRIBUTING/#pipeline-contribution-conventions","title":"Pipeline contribution conventions","text":"<p>To make the Joon-Klaps/viralgenie code and processing logic more understandable for new contributors and to ensure quality, we semi-standardise the way the code and other contributions are written.</p>"},{"location":"CONTRIBUTING/#adding-a-new-step","title":"Adding a new step","text":"<p>If you wish to contribute a new step, please use the following coding standards:</p> <ol> <li>Define the corresponding input channel into your new process from the expected previous process channel</li> <li>Write the process block (see below).</li> <li>Define the output channel if needed (see below).</li> <li>Add any new parameters to <code>nextflow.config</code> with a default (see below).</li> <li>Add any new parameters to <code>nextflow_schema.json</code> with help text (via the <code>nf-core schema build</code> tool).</li> <li>Add sanity checks and validation for all relevant parameters.</li> <li>Perform local tests to validate that the new code works as expected.</li> <li>If applicable, add a new test command in <code>.github/workflow/ci.yml</code>.</li> <li>Update MultiQC config <code>assets/multiqc_config.yml</code> so relevant suffixes, file name clean up and module plots are in the appropriate order. If applicable, add a MultiQC module.</li> <li>Add a description of the output files and if relevant any appropriate images from the MultiQC report to <code>docs/output.md</code>.</li> </ol>"},{"location":"CONTRIBUTING/#default-values","title":"Default values","text":"<p>Parameters should be initialised / defined with default values in <code>nextflow.config</code> under the <code>params</code> scope.</p> <p>Once there, use <code>nf-core schema build</code> to add to <code>nextflow_schema.json</code>.</p>"},{"location":"CONTRIBUTING/#default-processes-resource-requirements","title":"Default processes resource requirements","text":"<p>Sensible defaults for process resource requirements (CPUs / memory / time) for a process should be defined in <code>conf/base.config</code>. These should generally be specified generic with <code>withLabel:</code> selectors so they can be shared across multiple processes/steps of the pipeline. A nf-core standard set of labels that should be followed where possible can be seen in the nf-core pipeline template, which has the default process as a single core-process, and then different levels of multi-core configurations for increasingly large memory requirements defined with standardised labels.</p> <p>The process resources can be passed on to the tool dynamically within the process with the <code>${task.cpu}</code> and <code>${task.memory}</code> variables in the <code>script:</code> block.</p>"},{"location":"CONTRIBUTING/#naming-schemes","title":"Naming schemes","text":"<p>Please use the following naming schemes, to make it easy to understand what is going where.</p> <ul> <li>initial process channel: <code>ch_output_from_&lt;process&gt;</code></li> <li>intermediate and terminal channels: <code>ch_&lt;previousprocess&gt;_for_&lt;nextprocess&gt;</code></li> </ul>"},{"location":"CONTRIBUTING/#nextflow-version-bumping","title":"Nextflow version bumping","text":"<p>If you are using a new feature from core Nextflow, you may bump the minimum required version of nextflow in the pipeline with: <code>nf-core bump-version --nextflow . [min-nf-version]</code></p>"},{"location":"CONTRIBUTING/#images-and-figures","title":"Images and figures","text":"<p>For overview images and other documents we follow the nf-core style guidelines and examples.</p>"},{"location":"CONTRIBUTING/#github-codespaces","title":"GitHub Codespaces","text":"<p>This repo includes a devcontainer configuration which will create a GitHub Codespaces for Nextflow development! This is an online developer environment that runs in your browser, complete with VSCode and a terminal.</p> <p>To get started:</p> <ul> <li>Open the repo in Codespaces</li> <li>Tools installed<ul> <li>nf-core</li> <li>Nextflow</li> </ul> </li> </ul> <p>Devcontainer specs:</p> <ul> <li>DevContainer config</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>Viralgenie uses Nextflow, and a package/container management system (Docker, singularity or conda) so both need to be installed on the system where you launch your analysis.</p>"},{"location":"installation/#software-managers-docker-singularity-and-conda","title":"Software managers: Docker, singularity, and conda","text":"<p>Viralgenie can be run using either Docker, singularity or conda. The choice of container system is up to the user, but it is important to note that Docker and Singularity are the most reproducible. Nextflow supports more containers in addition to Docker and Singularity, such as Podman, Shifter, and Charliecloud. You can read the full list of supported containers and how to set them up here.</p> DockerSingularityConda | Mamba <p>Docker is a containerisation system that allows you to package your code, tools and data into a single image that can be run on most operating systems. It is the most widely used containerisation system in bioinformatics.</p> <p>To install Docker, follow the instructions on the Docker website.</p> <p>Warning</p> <p>Docker requires root access to run. If you do not have root access like, i.e. a user on a HPC or on a cloud - use Singularity instead.</p> <p>Singularity is a containerisation system that allows you to package your code, tools and data into a single image that can be run on most operating systems. It is the most widely used containerisation system in bioinformatics.</p> <p>To install Singularity, follow the instructions on the Singularity website.</p> <p>Conda is a package manager that allows you to install software packages and dependencies in isolated environments. It is a good choice if you are facing issues while installing Docker or Singularity.</p> <ul> <li>To install Conda, follow the instructions on the Conda website.</li> <li>To install Mamba, a faster alternative to Conda, follow the instructions on the Mamba miniforge website.</li> </ul> <p>Warning</p> <p>Conda environments are not as reproducible as Docker or Singularity containers. If you encounter issues with Conda, please try running the pipeline with Docker or Singularity first to see if the issue persists.</p>"},{"location":"installation/#nextflow","title":"Nextflow","text":"<p>Nextflow runs on most POSIX systems (Linux, macOS, etc) and requires java 11 or later. It can be installed in several ways, including using the Nextflow installer or Bioconda.</p> Nextflow installerBioconda <p>Tip</p> <p>Unsure how to install Nextflow with these commands? Check out the Nextflow installation documentation for more information.</p> <pre><code># Make sure that Java v11+ is installed:\njava -version\n\n# Install Nextflow\ncurl -fsSL get.nextflow.io | bash\n\n# Try a simple demo\n./nextflow run hello\n</code></pre> <p>Tip</p> <p>Add Nextflow binary to your user's <code>PATH</code>: <pre><code>mv nextflow ~/bin/\n</code></pre> Or to install it system-wide: <pre><code>sudo mv nextflow /usr/local/bin/\n</code></pre></p> <p>First, set up Bioconda according to the Bioconda documentation, notably setting up channels: <pre><code>conda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\n</code></pre></p> <p>A best practice with conda is to create an environment and install the tools in it. Therefore you will prevent version conflicts and keep everything clean. To do so use the following command:</p> <pre><code>conda create --name nextflow-env nextflow\nconda activate nextflow-env\n</code></pre> <p>To deactivate the conda environment, run the following command:</p> <pre><code>conda deactivate\n</code></pre> <p>If you're already in the conda environment you want to use, you can just install Nextflow directly:</p> <pre><code>conda install nextflow\n</code></pre>"},{"location":"installation/#viralgenie","title":"Viralgenie","text":"<p>If you have both nextflow and a software manager installed, you are all set! You can test the pipeline using the following command:</p> <pre><code>nextflow run Joon-Klaps/viralgenie \\\n    -profile test,&lt;docker/singularity/.../institute&gt; \\\n</code></pre> <p>Note</p> <p>With the argument <code>-profile &lt;docker/singularity/.../institute&gt;</code>, you can specify the container system you want to use. The <code>test</code> profile is used to run the pipeline with a small dataset to check if everything is working correctly.</p>"},{"location":"output/","title":"Joon-Klaps/viralgenie: Output","text":""},{"location":"output/#introduction","title":"Introduction","text":"<p>This document describes the output produced by the pipeline. Most of the plots are taken from the MultiQC report, which summarises results at the end of the pipeline.</p> <p>The directories listed below will be created in the results directory after the pipeline has finished. All paths are relative to the top-level results directory.</p>"},{"location":"output/#pipeline-overview","title":"Pipeline overview","text":"<p>The pipeline is built using Nextflow and processes data using the following steps:</p> <ul> <li>FastQC - Raw read QC</li> <li>MultiQC - Aggregate report describing results and QC from the whole pipeline</li> <li>Pipeline information - Report metrics generated during the workflow execution</li> </ul>"},{"location":"output/#fastqc","title":"FastQC","text":"Output files <ul> <li><code>fastqc/</code><ul> <li><code>*_fastqc.html</code>: FastQC report containing quality metrics.</li> <li><code>*_fastqc.zip</code>: Zip archive containing the FastQC report, tab-delimited data file and plot images.</li> </ul> </li> </ul> <p>FastQC gives general quality metrics about your sequenced reads. It provides information about the quality score distribution across your reads, per base sequence content (%A/T/G/C), adapter contamination and overrepresented sequences. For further reading and documentation see the FastQC help pages.</p> <p></p> <p></p> <p></p> <p>:::note The FastQC plots displayed in the MultiQC report shows untrimmed reads. They may contain adapter sequence and potentially regions with low quality. :::</p>"},{"location":"output/#multiqc","title":"MultiQC","text":"Output files <ul> <li><code>multiqc/</code><ul> <li><code>multiqc_report.html</code>: a standalone HTML file that can be viewed in your web browser.</li> <li><code>multiqc_data/</code>: directory containing parsed statistics from the different tools used in the pipeline.</li> <li><code>multiqc_plots/</code>: directory containing static images from the report in various formats.</li> </ul> </li> </ul> <p>MultiQC is a visualization tool that generates a single HTML report summarising all samples in your project. Most of the pipeline QC results are visualised in the report and further statistics are available in the report data directory.</p> <p>Results generated by MultiQC collate pipeline QC from supported tools e.g. FastQC. The pipeline has special steps which also allow the software versions to be reported in the MultiQC output for future traceability. For more information about how to use MultiQC reports, see http://multiqc.info.</p>"},{"location":"output/#pipeline-information","title":"Pipeline information","text":"Output files <ul> <li><code>pipeline_info/</code><ul> <li>Reports generated by Nextflow: <code>execution_report.html</code>, <code>execution_timeline.html</code>, <code>execution_trace.txt</code> and <code>pipeline_dag.dot</code>/<code>pipeline_dag.svg</code>.</li> <li>Reports generated by the pipeline: <code>pipeline_report.html</code>, <code>pipeline_report.txt</code> and <code>software_versions.yml</code>. The <code>pipeline_report*</code> files will only be present if the <code>--email</code> / <code>--email_on_fail</code> parameter's are used when running the pipeline.</li> <li>Reformatted samplesheet files used as input to the pipeline: <code>samplesheet.valid.csv</code>.</li> <li>Parameters used by the pipeline run: <code>params.json</code>.</li> </ul> </li> </ul> <p>Nextflow provides excellent functionality for generating various reports relevant to the running and execution of the pipeline. This will allow you to troubleshoot errors with the running of the pipeline, and also provide you with other information such as launch commands, run times and resource usage.</p>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#running-the-pipeline","title":"Running the pipeline","text":"<p>The typical command for running the pipeline is as follows:</p> <pre><code>nextflow run Joon-Klaps/viralgenie --input ./samplesheet.csv --outdir &lt;OUTDIR&gt; -profile docker\n</code></pre> <p>This will launch the pipeline with the <code>docker</code> configuration profile. See below for more information about profiles.</p> <p>Note that the pipeline will create the following files in your working directory:</p> <pre><code>work                # Directory containing the nextflow working files\n&lt;OUTDIR&gt;            # Finished results in specified location (defined with --outdir)\n.nextflow_log       # Log file from Nextflow\n# Other nextflow hidden files, eg. history of pipeline runs and old logs.\n</code></pre> <p>If you wish to repeatedly use the same parameters for multiple runs, rather than specifying each flag in the command, you can specify these in a params file.</p> <p>Pipeline settings can be provided in a <code>yaml</code> or <code>json</code> file via <code>-params-file &lt;file&gt;</code>.</p> <p>Warning</p> <p>Do not use <code>-c &lt;file&gt;</code> to specify parameters as this will result in errors. Custom config files specified with <code>-c</code> must only be used for tuning process resource specifications, other infrastructural tweaks (such as output directories), or module arguments (args).</p> <p>The above pipeline run specified with a params file in yaml format:</p> <pre><code>nextflow run Joon-Klaps/viralgenie -profile docker -params-file params.yaml\n</code></pre> <code>params.yaml</code> will contain: <pre><code>{\n    input: \"./samplesheet.csv\"\n    outdir: \"./results/\"\n}\n</code></pre> <p>You can also generate such <code>YAML</code>/<code>JSON</code> files via <code>nf-core launch</code>  if <code>nf-core</code> is installed. <pre><code>nf-core launch Joon-Klaps/viralgenie\n</code></pre></p> <p>Tip</p> <p>Use <code>nf-core launch</code> if it is the first time running the pipeline to explore all its features and options in an accesible way.</p>"},{"location":"usage/#samplesheets","title":"Samplesheets","text":""},{"location":"usage/#samplesheets-input","title":"Samplesheets: input","text":"<p>You will need to create a samplesheet with information about the samples you would like to analyse before running the pipeline. Use this parameter to specify its location. It has to be a comma-separated file with 2-3 columns, and a header row as shown in the examples below.</p> <pre><code>--input '[path to samplesheet file]'\n</code></pre> <p>The pipeline will auto-detect whether a sample is single- or paired-end using the information provided in the samplesheet (i.e. if the <code>fastq_2</code> column is empty, the sample is assumed to be single-end).</p> <p>An example samplesheet file consisting of both single- and paired-end data may look something like the one below. This is for 5 samples.</p> input-samplesheet.csv<pre><code>sample,fastq_1,fastq_2\nsample1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\nsample2,AEG588A5_S5_L003_R1_001.fastq.gz,\nsample3,AEG588A3_S3_L002_R1_001.fastq.gz,AEG588A3_S3_L002_R2_001.fastq.gz\n</code></pre> Column Description <code>sample</code> Custom sample name, needs to be unique <code>fastq_1</code> Full path (not relative paths) to FastQ file for Illumina short reads 1. File has to be gzipped and have the extension \".fastq.gz\" or \".fq.gz\". <code>fastq_2</code> Full path (not relative paths) to FastQ file for Illumina short reads 2. File has to be gzipped and have the extension \".fastq.gz\" or \".fq.gz\"."},{"location":"usage/#samplesheets-mapping-constrains","title":"Samplesheets: Mapping constrains","text":"<p>Viralgenie allows the user to specify the mapping constrains for the pipeline, meaning that the specified or all samples will be mapped against the specified references. This is done by providing a samplesheet to parameter <code>--mapping_constrains</code>.</p> <p>An example mapping constrain samplesheet file consisting of references that need to have all samples mapped towards them and a selection may look something like the one below. This is for 5 references, 2 of them being a multi-fasta file.</p> constrains-samplesheet.tsv<pre><code>id  species segment samples sequence    definition\nLassa-L-dataset LASV    L       LASV_L.multi.fasta  Collection of LASV sequences used for hybrid capture bait design, all publicly availble sequences of the L segment clustered at 99.5% similarity\nLassa-S-dataset LASV    S   sample1;sample3 LASV_S.multi.fasta  Collection of LASV sequences used for hybrid capture bait design, all publicly availble sequences of the S segment clustered at 99.5% similarity\nNC038709.1  HAZV    L   sample1;sample2 L-NC_038709.1.fasta Hazara virus isolate JC280 segment L, complete sequence.\nNC038710.1  HAZV    M       M-NC_038710.1.fasta Hazara virus isolate JC280 segment M, complete sequence.\nNC038711.1  HAZV    S       S-NC_038711.1.fasta Hazara virus isolate JC280 segment S, complete sequence.\n</code></pre> Column Description <code>id</code> Reference identifier, needs to be unique' <code>species</code> [Optional] Species name of the reference <code>segment</code> [Optional] Segment name of the reference <code>samples</code> [Optional] List of samples that need to be mapped towards the reference. If empty, map all samples. <code>sequence</code> Full path (not relative paths) to the reference sequence file. <code>definition</code> [Optional] Definition of the reference sequence file. <p>Tip</p> <p>The <code>samples</code> column is optional, if empty, all samples will be mapped towards the reference.</p> <p>Tip</p> <p>Multi-fasta files can be provided and all reads will mapped to all genomes but stats will not be reported separately in the final report.</p>"},{"location":"usage/#samplesheets-metadata-optional","title":"Samplesheets: metadata [optional]","text":"<p>A metadata file can also be provided to the pipeline. This file should contain information about the samples in the samplesheet, such as the condition, timepoint, or any other relevant information. This file should be a comma/tab-separated file with a header row and at least one column with the same name as the <code>sample</code> column in the samplesheet.</p> <p>The file will only be used in the final report and will not affect the pipeline run.</p> metadata.tsv<pre><code>sample  sample_accession    secondary_sample_accession  study_accession run_alias   library_layout\nsample1 SAMN14154201    SRS6189918  PRJNA607948 vero76_Illumina.fastq   PAIRED\nsample2 SAMN14154205    SRS6189924  PRJNA607948 veroSTAT-1KO_Illumina.fastq PAIRED\n</code></pre> <p>Provide the metadata file with the argument <code>--metadata</code>.</p>"},{"location":"usage/#updating-the-pipeline","title":"Updating the pipeline","text":"<pre><code>nextflow pull Joon-Klaps/viralgenie\n</code></pre> <p>When you run the above command, Nextflow automatically pulls the pipeline code from GitHub and stores it as a cached version. When running the pipeline after this, it will always use the cached version if available - even if the pipeline has been updated since. To make sure that you're running the latest version of the pipeline, make sure that you regularly update the cached version of the pipeline:</p> <pre><code>nextflow pull Joon-Klaps/viralgenie\n</code></pre>"},{"location":"usage/#reproducibility","title":"Reproducibility","text":"<p>It is a good idea to specify a pipeline version when running the pipeline on your data. This ensures that a specific version of the pipeline code and software are used when you run your pipeline. If you keep using the same tag, you'll be running the same version of the pipeline, even if there have been changes to the code since.</p> <p>First, go to the Joon-Klaps/viralgenie releases page and find the latest pipeline version - numeric only (eg. <code>1.3.1</code>). Then specify this when running the pipeline with <code>-r</code> (one hyphen) - eg. <code>-r 1.3.1</code>. Of course, you can switch to another version by changing the number after the <code>-r</code> flag.</p> <p>This version number will be logged in reports when you run the pipeline, so that you'll know what you used when you look back in the future. For example, at the bottom of the MultiQC reports.</p> <p>To further assist in reproducibility, you can use share and re-use parameter files to repeat pipeline runs with the same settings without having to write out a command with every single parameter.</p> <p>Tip</p> <p>If you wish to share such profile (such as upload as supplementary material for academic publications), make sure to NOT include cluster specific paths to files, nor institutional specific profiles.</p>"},{"location":"usage/#core-nextflow-arguments","title":"Core Nextflow arguments","text":"<p>Note</p> <p>These options are part of Nextflow and use a single hyphen (pipeline parameters use a double-hyphen).</p>"},{"location":"usage/#the-profile-parameter","title":"The <code>-profile</code> parameter","text":"<p>Use this parameter to choose a configuration profile. Profiles can give configuration presets for different compute environments.</p> <p>Several generic profiles are bundled with the pipeline which instruct the pipeline to use software packaged using different methods (Docker, Singularity, Podman, Shifter, Charliecloud, Apptainer, Conda) - see below.</p> <p>Info</p> <p>We highly recommend the use of Docker or Singularity containers for full pipeline reproducibility, however when this is not possible, Conda is also supported.</p> <p>The pipeline also dynamically loads configurations from https://github.com/nf-core/configs when it runs, making multiple config profiles for various institutional clusters available at run time. For more information and to see if your system is available in these configs please see the nf-core/configs documentation.</p> <p>Note that multiple profiles can be loaded, for example: <code>-profile test,docker</code> - the order of arguments is important! They are loaded in sequence, so later profiles can overwrite earlier profiles.</p> <p>If <code>-profile</code> is not specified, the pipeline will run locally and expect all software to be installed and available on the <code>PATH</code>. This is not recommended, since it can lead to different results on different machines dependent on the computer enviroment.</p> <ul> <li><code>test</code><ul> <li>A profile with a complete configuration for automated testing</li> <li>Includes links to test data so needs no other parameters</li> </ul> </li> <li><code>docker</code><ul> <li>A generic configuration profile to be used with Docker</li> </ul> </li> <li><code>singularity</code><ul> <li>A generic configuration profile to be used with Singularity</li> </ul> </li> <li><code>podman</code><ul> <li>A generic configuration profile to be used with Podman</li> </ul> </li> <li><code>shifter</code><ul> <li>A generic configuration profile to be used with Shifter</li> </ul> </li> <li><code>charliecloud</code><ul> <li>A generic configuration profile to be used with Charliecloud</li> </ul> </li> <li><code>apptainer</code><ul> <li>A generic configuration profile to be used with Apptainer</li> </ul> </li> <li><code>conda</code><ul> <li>A generic configuration profile to be used with Conda. Please only use Conda as a last resort i.e. when it's not possible to run the pipeline with Docker, Singularity, Podman, Shifter, Charliecloud, or Apptainer.</li> </ul> </li> </ul>"},{"location":"usage/#-resume","title":"<code>-resume</code>","text":"<p>Specify this when restarting a pipeline. Nextflow will use cached results from any pipeline steps where the inputs are the same, continuing from where it got to previously. For input to be considered the same, not only the names must be identical but the files' contents as well. For more info about this parameter, see this blog post.</p> <p>You can also supply a run name to resume a specific run: <code>-resume [run-name]</code>. Use the <code>nextflow log</code> command to show previous run names.</p>"},{"location":"usage/#-c","title":"<code>-c</code>","text":"<p>Specify the path to a specific config file (this is a core Nextflow command). See the nf-core website documentation for more information.</p>"},{"location":"usage/#custom-configuration","title":"Custom configuration","text":""},{"location":"usage/#resource-requests","title":"Resource requests","text":"<p>Whilst the default requirements set within the pipeline will hopefully work for most people and with most input data, you may find that you want to customize the compute resources that the pipeline requests. Each step in the pipeline has a default set of requirements for number of CPUs, memory and time. For most of the steps in the pipeline, if the job exits with any of the error codes specified here it will automatically be resubmitted with higher requests (2 x original, then 3 x original). If it still fails after the third attempt then the pipeline execution is stopped.</p> <p>To change the resource requests, please see the max resources and tuning workflow resources section of the nf-core website.</p>"},{"location":"usage/#custom-containers","title":"Custom Containers","text":"<p>In some cases you may wish to change which container or conda environment a step of the pipeline uses for a particular tool. By default Viralgenie uses containers and software from the biocontainers or bioconda projects. However in some cases the pipeline specified version maybe out of date.</p> <p>To use a different container from the default container or conda environment specified in a pipeline, please see the updating tool versions section of the nf-core website.</p>"},{"location":"usage/#custom-tool-arguments","title":"Custom Tool Arguments","text":"<p>A pipeline might not always support every possible argument or option of a particular tool used in pipeline. Fortunately, Viralgenie provides some freedom to users to insert additional parameters that the pipeline does not include by default.</p> <p>To learn how to provide additional arguments to a particular tool of the pipeline, please see the customising tool arguments section of the nf-core website.</p>"},{"location":"usage/#nf-coreconfigs","title":"nf-core/configs","text":"<p>In most cases, you will only need to create a custom config as a one-off but if you and others within your organisation are likely to be running Viralgenie regularly and need to use the same settings regularly it may be a good idea to request that your custom config file is uploaded to the <code>nf-core/configs</code> git repository. Before you do this please can you test that the config file works with your pipeline of choice using the <code>-c</code> parameter. You can then create a pull request to the <code>nf-core/configs</code> repository with the addition of your config file, associated documentation file (see examples in <code>nf-core/configs/docs</code>), and amending <code>nfcore_custom.config</code> to include your custom profile.</p> <p>See the main Nextflow documentation for more information about creating your own configuration files.</p> <p>If you have any questions or issues please send us a message on Slack on the <code>#configs</code> channel.</p>"},{"location":"usage/#azure-resource-requests","title":"Azure Resource Requests","text":"<p>To be used with the <code>azurebatch</code> profile by specifying the <code>-profile azurebatch</code>. We recommend providing a compute <code>params.vm_type</code> of <code>Standard_D16_v3</code> VMs by default but these options can be changed if required.</p> <p>Note that the choice of VM size depends on your quota and the overall workload during the analysis. For a thorough list, please refer the Azure Sizes for virtual machines in Azure.</p>"},{"location":"usage/#running-in-the-background","title":"Running in the background","text":"<p>Nextflow handles job submissions and supervises the running jobs. The Nextflow process must run until the pipeline is finished.</p> <p>The Nextflow <code>-bg</code> flag launches Nextflow in the background, detached from your terminal so that the workflow does not stop if you log out of your session. The logs are saved to a file.</p> <p>Alternatively, you can use <code>screen</code> / <code>tmux</code> or similar tool to create a detached session which you can log back into at a later time. Some HPC setups also allow you to run nextflow within a cluster job submitted your job scheduler (from where it submits more jobs).</p>"},{"location":"usage/#nextflow-memory-requirements","title":"Nextflow memory requirements","text":"<p>In some cases, the Nextflow Java virtual machines can start to request a large amount of memory. We recommend adding the following line to your environment to limit this (typically in <code>~/.bashrc</code> or <code>~./bash_profile</code>):</p> <pre><code>NXF_OPTS='-Xms1g -Xmx4g'\n</code></pre>"},{"location":"customisation/databases/","title":"Joon-Klaps/viralgenie: Databases","text":""},{"location":"customisation/databases/#introduction","title":"Introduction","text":"<p>Viralgenie uses a multitude of databases in order to analyse reads, contigs and consensus constructs. The default databases will be sufficient in most cases but there are always exceptions. This document will guide you towards the right documentation location for creating your custom databases.</p> <p>Tip</p> <p>Keep an eye out for nf-core createtaxdb as it can be used for the customization of the main databases but is still under development.</p>"},{"location":"customisation/databases/#reference-pool","title":"Reference pool","text":"<p>The reference pool dataset is used to identify potential references for scaffolding. It's a fasta file that will be used to make a blast database within the pipeline. The default database is the clustered Reference Viral DataBase (C-RVDB) a database that was build for enhancing virus detection using high-throughput/next-generation sequencing (HTS/NGS) technologies. An alternative reference pool is the Virosaurus database which is a manually curated database of viral genomes.</p> <p>Any nucleotide fasta file will do. Specify it with the parameter <code>--reference_pool</code>.</p>"},{"location":"customisation/databases/#kaiju","title":"Kaiju","text":"<p>The kaiju database will be used to classify the reads and intermediate contigs in taxonomic groups. The default database is the RVDB-prot pre-built database from Kaiju.</p> <p>A number of Kaiju pre-built indexes for reference datasets are maintained by the the developers of Kaiju and made available on the Kaiju website. To build a kaiju database, you need three components: a FASTA file with the protein sequences, the NCBI taxonomy dump files, and you need to define the uppercase characters of the standard 20 amino acids you wish to include.</p> <p>Warning</p> <p>The headers of the protein fasta file must be numeric NCBI taxon identifiers of the protein sequences.</p> <p>To download the NCBI taxonomy files, please run the following commands:</p> <pre><code>wget https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/new_taxdump/new_taxdump.zip\nunzip new_taxdump.zip\n</code></pre> <p>To build the database, run the following command (the contents of taxdump must be in the same location where you run the command):</p> <pre><code>kaiju-mkbwt -a ACDEFGHIKLMNPQRSTVWY -o proteins proteins.faa\nkaiju-mkfmi proteins\n</code></pre> <p>Tip</p> <p>You can speed up database construction by supplying the threads parameter (<code>-t</code>).</p> Expected files in database directory <ul> <li><code>kaiju</code><ul> <li><code>kaiju_db_*.fmi</code></li> <li><code>nodes.dmp</code></li> <li><code>names.dmp</code></li> </ul> </li> </ul> <p>For the Kaiju database construction documentation, see here.</p>"},{"location":"customisation/databases/#kraken2-databases","title":"Kraken2 databases","text":"<p>The Kraken2 database will be used to classify the reads and intermediate contigs in taxonomic groups.</p> <p>A number of database indexes have already been generated and maintained by @BenLangmead Lab, see here. These databases can directly be used to run the workflow with Kraken2 as well as Bracken.</p> <p>In case the databases above do not contain your desired libraries, you can build a custom Kraken2 database. This requires two components: a taxonomy (consisting of <code>names.dmp</code>, <code>nodes.dmp</code>, and <code>*accession2taxid</code>) files, and the FASTA files you wish to include.</p> <p>To pull the NCBI taxonomy, you can run the following:</p> <pre><code>kraken2-build --download-taxonomy --db &lt;YOUR_DB_NAME&gt;\n</code></pre> <p>You can then add your FASTA files with the following build command.</p> <pre><code>kraken2-build --add-to-library *.fna --db &lt;YOUR_DB_NAME&gt;\n</code></pre> <p>You can repeat this step multiple times to iteratively add more genomes prior building.</p> <p>Once all genomes are added to the library, you can build the database (and optionally clean it up):</p> <pre><code>kraken2-build --build --db &lt;YOUR_DB_NAME&gt;\nkraken2-build --clean --db &lt;YOUR_DB_NAME&gt;\n</code></pre> <p>You can then add the <code>&lt;YOUR_DB_NAME&gt;/</code> path to your nf-core/taxprofiler database input sheet.</p> Expected files in database directory <ul> <li><code>kraken2</code><ul> <li><code>opts.k2d</code></li> <li><code>hash.k2d</code></li> <li><code>taxo.k2d</code></li> </ul> </li> </ul> <p>You can follow the Kraken2 tutorial for a more detailed description.</p>"},{"location":"customisation/databases/#host-read-removal","title":"Host read removal","text":"<p>Viralgenie uses kraken2 to remove contaminated reads.</p> <p>Info</p> <p>The reason why we use Kraken2 for host removal over regular read mappers is nicely explained in the following papers:</p> <ul> <li>The human \u201ccontaminome\u201d: bacterial, viral, and computational contamination in whole genome sequences from 1000 families</li> <li>Reconstruction of the personal information from human genome reads in gut metagenome sequencing data</li> </ul> <p>The contamination database is likely the largest database. The default databases is made small explicitly made smaller to save storage for end users but is not optimal. I would recommend to create a database consisting of the libraries <code>human, archea, bacteria</code> which will be more then 200GB in size. Additionally, it's good practice to include DNA &amp; RNA of the host of origin if known (i.e. mice, ticks, mosquito, ... ). Add it as described above.</p> <p>Set it with the variable <code>--host_k2_db</code></p>"},{"location":"customisation/databases/#viral-diversity-with-kraken2","title":"Viral Diversity with Kraken2","text":"<p>The metagenomic diveristy estimated with kraken2 is based on the viral refseq database which can cut short if you expect your the species within your sample to have a large amount of diversity eg below 80% ANI (quasi-species). To resolve this it's better to create a database that contains a wider species diversity then only one genome per species. Databases that have this wider diversity is Virosaurus or the RVDB which can increase the accuracy of kraken2. Add it as described above.</p> <p>Set it with the variable <code>--kraken2_db</code></p>"},{"location":"customisation/databases/#annotation-sequences","title":"Annotation sequences","text":"<p>Identifying the species and the segment of the final genome constructs is done based on a tblastx search (with MMSEQS) to a annotated sequencing dataset. This dataset is by default the Virosaurus as it contains a good representation of the viral genomes and is annotated.</p> <p>This annotation database can be specified using <code>--annotation_db</code></p>"},{"location":"customisation/databases/#creating-a-custom-annotation-dataset","title":"Creating a custom annotation dataset","text":"<p>In case Virosaurus does not suffice your needs, a custom annotation dataset can be made. Creating a custom annotation dataset can easily be done as long as the annotation data is in the fasta header using this format: <code>(key)=(value)</code> or <code>(key):(value)</code>. For example, the following fasta headers are both valid:</p> <pre><code>&gt;754189.6 species:\"Ungulate tetraparvovirus 3\"|segment:\"nan\"|host_common_name:\"Pig\"|genbank_accessions:\"NC_038883\"|taxon_id:\"754189\"\n&gt;NC_001731; usual name=Molluscum contagiosum virus; clinical level=SPECIES; clinical typing=unknown; species=Molluscum contagiosum virus; taxid=10279; acronym=MOCV; nucleic acid=DNA; circular=N; segment=N/A; host=Human,Vertebrate;\n</code></pre> <p>An easy to use public database to creata a custom consenus annotation dataset is BV-BRC. Sequences can be extracted using their CLI-tool and linked to their metadata</p> <p>Here we select all viral genomes that are not lab reassortments and are reference genomes and add metadata attributes to the output.</p> <pre><code># download annotation metadata +/- 5s\np3-all-genomes --eq superkingdom,Viruses --eq reference_genome,Reference --ne host_common_name,'Lab reassortment' --attr genome_id,species,segment,genome_name,genome_length,host_common_name,genbank_accessions,taxon_id   &gt; all-virus-anno.txt\n# download genome data, done seperatly as it takes much longer to query +/- 1 hour\np3-all-genomes --eq superkingdom,Viruses --eq reference_genome,Reference --ne host_common_name,'Lab reassortment' | p3-get-genome-contigs --attr sequence &gt; all-virus.fasta\n</code></pre> <p>Tip</p> <p>Any attribute can be downloaded and will be added to the final report if the formatting remains the same. For a complete list of attributes see <code>p3-all-genomes --fields</code> or read their manual</p> <p>Next, the metadata and the genomic data is combined into a single fasta file where the metada fields are stored in the fasta comment as <code>key1:\"value1\"|key2:\"value2\"|...</code> using the following python code.</p> <pre><code>import pandas as pd\nimport re\n\n# read in sequences with all columns as strings\nsequences = pd.read_csv(\"refseq-virus.fasta\", index_col=0, sep=\"\\t\", dtype=str)\ndata = pd.read_csv(\"refseq-virus-anno.txt\", index_col=0, sep=\"\\t\", dtype=str)\n\n# merge the df's\ndf = sequences.join(data)\n# remove 'genome' from the name\ndf.columns = df.columns.str.replace(\"genome.\", \"\")\n\n# create fasta header\ndef create_fasta_header(row):\n    annotations = \"|\".join(\n        [\n            f'{column}:\"{value}\"'\n            for column, value in row.items()\n            if column != \"contig.sequence\"\n        ]\n    )\n    return f\"{annotations}\\n\"\n\n\ndf[\"fasta_header\"] = df.apply(create_fasta_header, axis=1)\n\ndf[\"fasta_entry\"] = (\n    \"&gt;\" + df.index.astype(str) + \" \" + df[\"fasta_header\"] + df[\"contig.sequence\"]\n)\nwith open(\"bv-brc-refvirus-anno.fasta\", \"w\") as f:\n    for entry in df[\"fasta_entry\"]:\n        f.write(entry + \"\\n\")\n</code></pre> Expected files in database directory <ul> <li><code>bv-brc-refvirus-anno.fasta.gz</code></li> </ul>"}]}